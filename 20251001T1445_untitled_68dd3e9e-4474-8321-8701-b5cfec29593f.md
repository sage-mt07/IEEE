---
conversation_id: "68dd3e9e-4474-8321-8701-b5cfec29593f"
title: "複合キーエラー修正"
create_time: 1759329952.262989
update_time: 1759330190.99126
language_guess: "en"
num_messages: 29
part_index: 1
part_count: 1
---

### user — 2025-10-01T14:45:47.615000+00:00

Shell


$ pwsh -NoLogo -NoProfile -Command '.\physicalTests\long_run_bar_4h.ps1 -DurationMinutes 5'
[[[REDACTED_PHONE]]:55:32Z] reset environment
[reset] using compose: C:\dev\ex2\experimental\physicalTests\docker-compose.yaml (serviceId=ksql_service_72062338, prefix=phys_72062338_20251001135532835_)
 Container physicaltests-ksqldb-cli-1  Stopping
 Container physicaltests-runner-1  Stopping
 Container physicaltests-ksqldb-cli-1  Stopped
 Container physicaltests-ksqldb-cli-1  Removing
 Container physicaltests-ksqldb-cli-1  Removed
 Container physicaltests-runner-1  Stopped
 Container physicaltests-runner-1  Removing
 Container physicaltests-runner-1  Removed
 Container physicaltests-ksqldb-server-1  Stopping
 Container physicaltests-ksqldb-server-1  Stopped
 Container physicaltests-ksqldb-server-1  Removing
 Container physicaltests-ksqldb-server-1  Removed
 Container schema-registry  Stopping
 Container schema-registry  Stopped
 Container schema-registry  Removing
 Container schema-registry  Removed
 Container physicaltests-kafka-1  Stopping
 Container physicaltests-kafka-1  Stopped
 Container physicaltests-kafka-1  Removing
 Container physicaltests-kafka-1  Removed
 Container physicaltests-zookeeper-1  Stopping
 Container physicaltests-zookeeper-1  Stopped
 Container physicaltests-zookeeper-1  Removing
 Container physicaltests-zookeeper-1  Removed
 Network physicaltests_default  Removing
 Volume physicaltests_kafka_data  Removing
 Volume physicaltests_kafka_data  Removed
 Network physicaltests_default  Removed
[up] using compose: C:\dev\ex2\experimental\physicalTests\docker-compose.yaml
 Network physicaltests_default  Creating
 Network physicaltests_default  Created
 Volume "physicaltests_kafka_data"  Creating
 Volume "physicaltests_kafka_data"  Created
 Container physicaltests-zookeeper-1  Creating
 Container physicaltests-zookeeper-1  Created
 Container physicaltests-kafka-1  Creating
 Container physicaltests-kafka-1  Created
 Container schema-registry  Creating
 Container schema-registry  Created
 Container physicaltests-ksqldb-server-1  Creating
 Container physicaltests-ksqldb-server-1  Created
 Container physicaltests-runner-1  Creating
 Container physicaltests-ksqldb-cli-1  Creating
 Container physicaltests-ksqldb-cli-1  Created
 Container physicaltests-runner-1  Created
 Container physicaltests-zookeeper-1  Starting
 Container physicaltests-zookeeper-1  Started
 Container physicaltests-kafka-1  Starting
 Container physicaltests-kafka-1  Started
 Container schema-registry  Starting
 Container schema-registry  Started
 Container physicaltests-ksqldb-server-1  Starting
 Container physicaltests-ksqldb-server-1  Started
 Container physicaltests-ksqldb-server-1  Waiting
 Container physicaltests-kafka-1  Waiting
 Container physicaltests-ksqldb-server-1  Waiting
 Container physicaltests-kafka-1  Healthy
 Container physicaltests-ksqldb-server-1  Healthy
 Container physicaltests-ksqldb-cli-1  Starting
 Container physicaltests-ksqldb-server-1  Healthy
 Container physicaltests-runner-1  Starting
 Container physicaltests-ksqldb-cli-1  Started
 Container physicaltests-runner-1  Started
[up] Phase 1: Infrastructure readiness (Kafka/SR/ksql health)
[up] Phase 2: ksqlDB internal state (SHOW QUERIES)
[up] Phase 3: Settling 45s
[up] environment is ready
[reset] environment ready
[[[REDACTED_PHONE]]:57:16Z] create sources
[[[REDACTED_PHONE]]:57:17Z] create CSAS tables
Invoke-RestMethod: C:\dev\ex2\experimental\physicalTests\long_run_bar_4h.ps1:13
Line |
  13 |      return Invoke-RestMethod -Uri $uri -Method $method -ContentType ' …
     |             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     |  {   "@type": "generic_error",   "error_code": 40000,   "message": "Key format does not support schema.\nformat:
     | KAFKA\nschema: Persistence{columns=[\u0060BROKER\u0060 STRING KEY, \u0060SYMBOL\u0060 STRING KEY],
     | features=[]}\nreason: The \u0027KAFKA\u0027 format only supports a single field. Got: [\u0060BROKER\u0060 STRING
     | KEY, \u0060SYMBOL\u0060 STRING KEY]" }

### system — 2025-10-01T14:45:52.262989+00:00

_(empty)_

### system — 2025-10-01T14:45:52.262989+00:00

_(empty)_

### assistant — 2025-10-01T14:45:54.302270+00:00

やあ司令、天城です。  
エラーは「KAFKA（生バイト）キーが“複合キー”をサポートしない」のが原因です。`BROKER STRING KEY, SYMBOL STRING KEY` のようにキー列が2本あるのに、キーのシリアライズ形式が既定の `KAFKA`（=単一フィールドのみ可）になっているため 400 になっています。

## なにが起きた？
- CSAS/CTAS でターゲット（またはソース）に **複合キー** を定義
- しかし `KEY_FORMAT` を明示していない ⇒ 既定は `KAFKA`
- `KAFKA` は単一キー専用 ⇒ 「The 'KAFKA' format only supports a single field」

## 直し方（おすすめ順）

### プランA：複合キー対応のキー形式にする（推奨）
`KEY_FORMAT='JSON'` か `KEY_FORMAT='AVRO'`（または `PROTOBUF`）を **ターゲット定義側** に明示してください。  
（Schema Registry を使うなら `AVRO` / `PROTOBUF` が管理しやすいです）

**例：ソース定義（STREAM/TABLE）を作るとき：**
```sql
CREATE STREAM RATES (
  BROKER STRING KEY,
  SYMBOL STRING KEY,
  TS     BIGINT,
  BID    DECIMAL(18,6),
  ASK    DECIMAL(18,6)
) WITH (
  KAFKA_TOPIC='RATES',
  KEY_FORMAT='JSON',      -- ★ 複合キーをJSONで
  VALUE_FORMAT='AVRO'     -- 値はお好みで
);
```

**例：CSAS/CTAS 側で明示：**
```sql
CREATE TABLE BAR_1S_FINAL WITH (
  KEY_FORMAT='JSON',      -- ★ 忘れずに
  VALUE_FORMAT='AVRO',
  PARTITIONS=3
) AS
SELECT
  BROKER,
  SYMBOL,
  WINDOWSTART AS TS,
  MIN(price)  AS LOW,
  MAX(price)  AS HIGH
FROM RATES
WINDOW TUMBLING (SIZE 1 SECOND)
GROUP BY BROKER, SYMBOL, WINDOWSTART;
```

> メモ：上流のソースが既に `KAFKA` キーで作られている場合でも、**CSAS/CTAS の出力側**に `KEY_FORMAT` を指定すれば、出力トピックのキーは複合キー（JSON/AVRO/PROTOBUF）でシリアライズされます。ソース自体も複合キーにしたいなら、そちらの `CREATE STREAM/TABLE` にも `KEY_FORMAT` を追加して再作成してください。

### プランB：複合キーを「1本のキー」に畳み込む（互換性優先）
`KAFKA` のままにしたい／上流互換を崩せない場合は、`BROKER` と `SYMBOL` を連結して **単一キー** を作り、`PARTITION BY` で再キーイングします。

```sql
CREATE STREAM RATES_SINGLEKEY AS
SELECT
  CONCAT_WS('|', BROKER, SYMBOL) AS BKSYM_KEY,
  *
FROM RATES
PARTITION BY CONCAT_WS('|', BROKER, SYMBOL);

-- 以降の集計は BKSYM_KEY 単独キーで
CREATE TABLE BAR_1S_FINAL AS
SELECT
  BKSYM_KEY,
  WINDOWSTART AS TS,
  MIN(price)  AS LOW,
  MAX(price)  AS HIGH
FROM RATES_SINGLEKEY
WINDOW TUMBLING (SIZE 1 SECOND)
GROUP BY BKSYM_KEY, WINDOWSTART;
```

## どこを直せばいい？（今回ログの位置づけ）
- `long_run_bar_4h.ps1` の **「create CSAS tables」** フェーズで叩いている REST の ksql 文に、  
  `WITH (KEY_FORMAT='JSON' | 'AVRO' | 'PROTOBUF')` を足してください。  
  ソース作成（「create sources」）でも複合キーを宣言しているなら、そちらの `CREATE STREAM/TABLE` にも `KEY_FORMAT` を追加。

## チェックリスト
- [ ] `CREATE STREAM/TABLE ... ( ... STRING KEY, ... STRING KEY )` としている箇所に `KEY_FORMAT` があるか  
- [ ] 出力系（CSAS/CTAS）で `WITH (KEY_FORMAT='...')` を必ず明示  
- [ ] Schema Registry を使う場合：`AVRO`/`PROTOBUF` のキーが登録できているか（`-key` サブジェクト）  
- [ ] 既存に `KAFKA` キーのソースがあるなら、出力側だけでも `KEY_FORMAT` を明示して矛盾を解消

必要なら、該当の DDL 断片を貼ってくれれば、**最小差分のパッチ（diff）** にします。  

— 送信: [[REDACTED_PHONE]]:29 (JST)

### system — 2025-10-01T14:45:56.160298+00:00

_(empty)_

### system — 2025-10-01T14:45:56.275844+00:00

_(empty)_

### assistant — 2025-10-01T14:45:56.293889+00:00

_(empty)_

### assistant — 2025-10-01T14:45:57.577520+00:00

_(empty)_

### assistant — 2025-10-01T14:46:39.372680+00:00

_(empty)_

### user — 2025-10-01T14:47:29.506484+00:00

_(empty)_

### user — 2025-10-01T14:47:31.742000+00:00

_(empty)_

### system — 2025-10-01T14:47:37.963217+00:00

_(empty)_

### assistant — 2025-10-01T14:47:37.992064+00:00

{"pointers": ["url:file-4GFLWPwDb9MosP4ZghhSpX"]}

### system — 2025-10-01T14:47:38.327707+00:00

_(empty)_

### assistant — 2025-10-01T14:47:39.020260+00:00

{"pointers": ["files_uploaded_in_conversation:0"]}

### tool — 2025-10-01T14:47:39.131538+00:00

No valid pointers provided.

### assistant — 2025-10-01T14:47:39.902290+00:00

{"queries": ["long_run_bar_4h.ps1"], "source_filter": ["files_uploaded_in_conversation"]}

### tool — 2025-10-01T14:47:40.004287+00:00

files_uploaded_in_conversation:0 is not a valid pointer.

### tool — 2025-10-01T14:47:40.809825+00:00

_(empty)_

### assistant — 2025-10-01T14:47:41.784993+00:00

受け取った `long_run_bar_4h.ps1` はファイル検索経由では開けませんでしたが、PowerShell スクリプトを直接読んでエラー箇所を特定する必要があります。  

いまのエラーメッセージは **`Invoke-RestMethod` で実行している ksqlDB の DDL（CREATE STREAM/TABLE/CSAS）** が原因です。  
`BROKER STRING KEY, SYMBOL STRING KEY` と複合キーを定義したにもかかわらず、`KEY_FORMAT` を明示せずに作っているため、デフォルトの `KAFKA` フォーマットが適用され、ksqlDB 側から拒否されています。

---

## 修正方針

1. **ps1 内で投げている KSQL 文に `KEY_FORMAT` を追加する**  
   例：
   ```sql
   CREATE STREAM RATES (
     BROKER STRING KEY,
     SYMBOL STRING KEY,
     TS BIGINT,
     BID DECIMAL(18,6),
     ASK DECIMAL(18,6)
   ) WITH (
     KAFKA_TOPIC='RATES',
     KEY_FORMAT='JSON',       -- ← これを追加
     VALUE_FORMAT='AVRO'
   );
   ```

2. **もし CSAS/CTAS 側だけでも複合キーにしたい場合は、出力テーブル定義に追加する**  
   ```sql
   CREATE TABLE BAR_1S_FINAL WITH (
     KEY_FORMAT='JSON',
     VALUE_FORMAT='AVRO'
   ) AS
   SELECT ...
   ```

3. **互換性の都合で `KAFKA` のままにしたい場合**は、`BROKER` と `SYMBOL` を連結して単一キーに変換する必要があります。  
   ```sql
   CREATE STREAM RATES_SINGLEKEY AS
   SELECT CONCAT_WS('|', BROKER, SYMBOL) AS BKSYM_KEY, *
   FROM RATES
   PARTITION BY CONCAT_WS('|', BROKER, SYMBOL);
   ```

---

## おすすめ

テスト環境なら **プランA（KEY_FORMAT='JSON' or 'AVRO'）** がシンプルでおすすめです。  
Schema Registry をすでに動かしているので、`AVRO` を選んでおくとキー・値ともにスキーマ管理できて安心です。  

---

司令、この `.ps1` の中でどの `CREATE STREAM` / `CSAS` を発行しているかを見せてもらえれば、**具体的な修正 diff** を作れます。  
貼っていただけますか？

### tool — 2025-10-01T14:47:41.785244+00:00

_(empty)_

### tool — 2025-10-01T14:47:41.862061+00:00

No search results matched your queries.

Note that a source being available does not imply that the user has synced data from that source yet; in case they haven't synced / connected a source yet, no documents will be returned from that source. If you get no results after a filtered search, try making another search with the source filter empty / undefined instead.

### user — 2025-10-01T14:47:54.909000+00:00

param(
  [int]$DurationMinutes = 240,
  [string]$ComposeFile = "$(Split-Path $PSCommandPath)\docker-compose.yaml",
  [string]$KsqlUrl = "http://127.0.0.1:18088"
)

$ErrorActionPreference = 'Stop'

function Invoke-Ksql([string]$path, [string]$method = 'GET', [object]$body = $null) {
  $uri = "$KsqlUrl$path"
  if ($null -ne $body) {
    $json = ($body | ConvertTo-Json -Depth 8)
    return Invoke-RestMethod -Uri $uri -Method $method -ContentType 'application/json' -Body $json
  } else {
    return Invoke-RestMethod -Uri $uri -Method $method
  }
}

function Write-Log([string]$msg) {
  $ts = (Get-Date).ToString('u')
  Write-Host "[$ts] $msg"
}

# 0) Reset environment per validation doc
Write-Log "reset environment"
& "$(Split-Path $PSCommandPath)\reset.ps1" -ComposeFile $ComposeFile

# 1) Create sources (DEDUPRATES, MSCHED) — idempotent
Write-Log "create sources"
Invoke-Ksql '/ksql' 'POST' @{ ksql = @'
CREATE STREAM IF NOT EXISTS DEDUPRATES (
  BROKER STRING KEY,
  SYMBOL STRING,
  TS BIGINT,
  BID DECIMAL(18,4)
) WITH (KAFKA_TOPIC='deduprates', VALUE_FORMAT='AVRO', PARTITIONS=1);

CREATE TABLE IF NOT EXISTS MSCHED (
  BROKER STRING PRIMARY KEY,
  SYMBOL STRING,
  OPEN_TS BIGINT,
  CLOSE_TS BIGINT
) WITH (KAFKA_TOPIC='msched', VALUE_FORMAT='AVRO', PARTITIONS=1);
'@ } | Out-Null

# 2) Create CSAS views (bar_1d_live / bar_1wk_live) — simplified representatives
Write-Log "create CSAS tables"
Invoke-Ksql '/ksql' 'POST' @{ ksql = @'
CREATE TABLE IF NOT EXISTS bar_1d_live WITH (KAFKA_TOPIC='bar_1d_live', KEY_FORMAT='AVRO', VALUE_FORMAT='AVRO', PARTITIONS=1) AS
SELECT BROKER, SYMBOL, WINDOWSTART AS WS, WINDOWEND AS WE, COUNT(*) AS CNT
FROM DEDUPRATES WINDOW TUMBLING (SIZE 1 DAY)
GROUP BY BROKER, SYMBOL EMIT CHANGES;

CREATE TABLE IF NOT EXISTS bar_1wk_live WITH (KAFKA_TOPIC='bar_1wk_live', KEY_FORMAT='AVRO', VALUE_FORMAT='AVRO', PARTITIONS=1) AS
SELECT BROKER, SYMBOL, WINDOWSTART AS WS, WINDOWEND AS WE, COUNT(*) AS CNT
FROM DEDUPRATES WINDOW TUMBLING (SIZE 7 DAYS)
GROUP BY BROKER, SYMBOL EMIT CHANGES;
'@ } | Out-Null

# 3) Ingestion loop (10–60s). Observation loop (Push/Pull) each 1–10m.
$stopAt = (Get-Date).AddMinutes($DurationMinutes)
$nextInsert = Get-Date
$nextObserve = (Get-Date).AddMinutes(1)

$utc = (Get-Date -AsUTC).ToString('yyyyMMdd_HHmmssZ')
$reportDir = Join-Path (Resolve-Path ..\) "Reportsx\physical\$utc"
New-Item -ItemType Directory -Force -Path $reportDir | Out-Null
$pushLog = Join-Path $reportDir 'push_bar_1d_live.ndjson'
$pullLog = Join-Path $reportDir 'pull_bar_1wk_live.ndjson'

Write-Log "start long-run: $DurationMinutes min; reporting to $reportDir"

while ((Get-Date) -lt $stopAt) {
  $now = Get-Date
  if ($now -ge $nextInsert) {
    $ts = [int64]([DateTimeOffset](Get-Date -AsUTC)).ToUnixTimeMilliseconds()
    $body = @{ ksqs = @() }
    $sql = "INSERT INTO DEDUPRATES (BROKER, SYMBOL, TS, BID) VALUES ('B','S', $ts, [[REDACTED_PHONE]]);"
    Invoke-Ksql '/ksql' 'POST' @{ ksql = $sql } | Out-Null
    Write-Log "inserted deduprate ts=$ts"
    $nextInsert = $now.AddSeconds((Get-Random -Minimum 10 -Maximum 60))
  }

  if ($now -ge $nextObserve) {
    # Push (query-stream) limited
    try {
      $pushBody = @{ sql = "SELECT * FROM bar_1d_live EMIT CHANGES LIMIT 2;" }
      $pushResp = Invoke-Ksql '/query-stream' 'POST' $pushBody | ConvertTo-Json -Depth 12
      $wrap = @{ ts = (Get-Date).ToUniversalTime().ToString('o'); kind = 'push'; raw = $pushResp }
      Add-Content -LiteralPath $pushLog -Value ($wrap | ConvertTo-Json -Depth 12)
    } catch { Write-Warning $_ }

    # Pull (query)
    try {
      $pullBody = @{ sql = "SELECT * FROM bar_1wk_live WHERE BROKER='B' AND SYMBOL='S' LIMIT 10;" }
      $pullResp = Invoke-Ksql '/query' 'POST' $pullBody | ConvertTo-Json -Depth 12
      $wrap = @{ ts = (Get-Date).ToUniversalTime().ToString('o'); kind = 'pull'; raw = $pullResp }
      Add-Content -LiteralPath $pullLog -Value ($wrap | ConvertTo-Json -Depth 12)
    } catch { Write-Warning $_ }

    Write-Log "observed push/pull; logs appended"
    $nextObserve = $now.AddMinutes((Get-Random -Minimum 1 -Maximum 10))
  }

  Start-Sleep -Seconds 1
}

Write-Log "long-run finished; collecting SHOW TABLES"
try {
  $tables = Invoke-Ksql '/ksql' 'POST' @{ ksql = 'SHOW TABLES;' }
  $tables | ConvertTo-Json -Depth 8 | Set-Content -LiteralPath (Join-Path $reportDir 'show_tables.json')
} catch { Write-Warning $_ }

Write-Log "done. See $reportDir"

### assistant — 2025-10-01T14:48:02.950905+00:00

司令、ログと `.ps1` ありがとう。結論から：**今回の 400 エラーは「複合キーなのに KEY_FORMAT が KAFKA 既定になってた」ことが原因**です。いま貼ってくれた最新版は **CSAS 側に `KEY_FORMAT='AVRO'` を入れてあるので、その点は OK**。再現を完全に潰すなら、**ソース側にも `KEY_FORMAT` を明示**しておくのが安全です（将来の既定値やテンプレ更新に揺れないようにするため）。

加えて、スクリプトに小さな気になる点が1つ（未使用変数）あったので合わせて最小差分を出します。

# 最小差分（パッチ）

```diff
@@
 CREATE STREAM IF NOT EXISTS DEDUPRATES (
   BROKER STRING KEY,
   SYMBOL STRING,
   TS BIGINT,
   BID DECIMAL(18,4)
-) WITH (KAFKA_TOPIC='deduprates', VALUE_FORMAT='AVRO', PARTITIONS=1);
+) WITH (
+  KAFKA_TOPIC='deduprates',
+  KEY_FORMAT='AVRO',      -- ★ 単一キーでも明示（将来の既定変動に備える）
+  VALUE_FORMAT='AVRO',
+  PARTITIONS=1
+);
 
 CREATE TABLE IF NOT EXISTS MSCHED (
   BROKER STRING PRIMARY KEY,
   SYMBOL STRING,
   OPEN_TS BIGINT,
   CLOSE_TS BIGINT
-) WITH (KAFKA_TOPIC='msched', VALUE_FORMAT='AVRO', PARTITIONS=1);
+) WITH (
+  KAFKA_TOPIC='msched',
+  KEY_FORMAT='AVRO',      -- ★ TABLE の PK 形式も明示
+  VALUE_FORMAT='AVRO',
+  PARTITIONS=1
+);
@@
-  if ($now -ge $nextInsert) {
+  if ($now -ge $nextInsert) {
     $ts = [int64]([DateTimeOffset](Get-Date -AsUTC)).ToUnixTimeMilliseconds()
-    $body = @{ ksqs = @() }
+    # $body は未使用なので削除（ノイズ防止）
     $sql = "INSERT INTO DEDUPRATES (BROKER, SYMBOL, TS, BID) VALUES ('B','S', $ts, [[REDACTED_PHONE]]);"
     Invoke-Ksql '/ksql' 'POST' @{ ksql = $sql } | Out-Null
```

# これで何が効く？
- **CSAS/CTAS 出力**はすでに `KEY_FORMAT='AVRO'` を指定済み → 複合キー（`BROKER`,`SYMBOL`）の schema 付きキーが使える。
- **ソース/辞書側**にも `KEY_FORMAT` を付けておくことで、将来の既定変更や `FORMAT` 周りの微妙な差分で “勝手に KAFKA 扱い” になる事故を予防。
- 未使用 `$body` を削除して、ログと可読性をクリーンに。

# 念のためのワンポイント確認
- ksqlDB のバージョンが十分新しく（**複合キーをサポート**）、`KEY_FORMAT` を**キーに適用**できることを前提にしています。もし古いバージョンや互換モードで **複合キー自体が不可**な場合は、フォールバックとして  
  **`CONCAT_WS('|', BROKER, SYMBOL)` で単一キーに畳む**方式へ切り替えましょう（必要ならすぐ差分出します）。

このパッチを入れてもう一度実行してみてください。まだ 400 が出るようなら、`/ksql` に投げている **レスポンス全文**を一緒に貼ってくれれば、該当 DDL をピンポイントで直します。

— 送信: [[REDACTED_PHONE]]:44 (JST)

### system — 2025-10-01T14:48:04.194741+00:00

_(empty)_

### system — 2025-10-01T14:48:04.206575+00:00

_(empty)_

### assistant — 2025-10-01T14:48:05.177022+00:00

_(empty)_

### assistant — 2025-10-01T14:49:09.008732+00:00

_(empty)_
