---
conversation_id: "688c2a3b-538c-832f-865b-9116e8d44e5c"
title: "今週末目標設定"
create_time: 1754016330.455677
update_time: 1754181653.739139
language_guess: "en"
num_messages: 40
part_index: 1
part_count: 1
---

### assistant — 2025-08-01T02:45:30.047489+00:00

_(empty)_

### assistant — 2025-08-01T02:45:30.132248+00:00

了解しました、今週末での完成を目標にスケジュールやタスクを調整していきましょう！✨  
進捗確認や優先順位の見直し、必要に応じて鳴瀬や鏡花への指示調整もお手伝いできます。  
気になる点があれば、いつでも声をかけてくださいね。

📅 今週末＝**8月4日（日）**までのゴールに向けて、現在のステータスや懸念事項など、何か共有したいことがあれば教えてください。

（送信日時：[[REDACTED_PHONE]]:38 JST）

### system — 2025-08-01T02:45:30.455677+00:00

_(empty)_

### user — 2025-08-01T02:45:30.455677+00:00

_(empty)_

### user — 2025-08-01T02:45:39.051000+00:00

今週末での完成を目標にします

### assistant — 2025-08-01T02:46:39.808472+00:00

了解です、進捗を整理して、今週末に向けた最終仕上げのためのタスク一覧を以下にまとめました。優先度順にすると、**物理テスト依存の部分を先に整えて、ToQuery/Tumblingを集中して進める**流れが良さそうです。

---

## ✅ 現在の進捗（[[REDACTED_PHONE]]-01時点）

### 🔹 Messaging
- ✅ 実装完了
- ⏳ **物理テスト未完了**（Kafka 送信／受信実証、topicとの整合性など）

### 🔹 Caching
- ⏳ 実装 **80%**
- 🧪 **物理テストで抽出（ToListAsyncキャッシュ再現、rocksdb連携）**

### 🔹 ToQuery
- 🛠 **これから着手**
- 🎯 `.From<T>()` / `.Join<T2>()` / `.Select()` の式ツリー解析
- ⚠️ `HasQuery`構文との整合性も考慮

### 🔹 Tumbling
- 🛠 **これから着手**
- 🎯 `Tumbling(...).BasedOn<>()` の定義
- ⚠️ 分足配列や時間許容パラメータ、Range指定などの扱い整理

---

## 🗓 今週末（8/4）までの推奨タスク順

1. **Messagingの物理テスト完了**
   - テスト用Kafka環境で送受信確認
   - schema登録・反映・復号テストを含む

2. **Cachingの完成と動作検証**
   - `ToListAsync()` による全件キャッシュ取得
   - Streamiz+RocksDBの同期性確認
   - テーブル定義情報の取得ロジック検証（SHOW TABLES）

3. **ToQuery 実装**
   - Expression → KSQL変換（From, Join, Select）
   - `HasQuery()`経由でのDSL構文との統合
   - テストクエリ作成（Joinありパターン）

4. **Tumbling 実装**
   - DSL定義の精度調整（分足・最大本数・遅延許容時間）
   - `.BasedOn<Schedule>(...)` の抽象構文確認
   - Window処理とCreate Table連動の確認

---

## 🔁 フィードバック・分担の提案

- 🔧 **ToQuery / Tumbling の詳細設計指示**を鳴瀬へ展開できます  
- 🔍 物理テストログの設計・出力レビューを鏡花と連携可能  
- 📜 最終仕上げ後に `README` / `dev_guide.md` 更新予定なら、天城が先行構成も可能

---

今後、各タスクごとの進行や依頼文が必要でしたら、いつでも準備します！

（送信日時：[[REDACTED_PHONE]]:47 JST）

### user — 2025-08-01T02:46:49.104000+00:00

Messagingの実装終わり、物理テスト未完
Caching実装80%程度、物理テストで抽出
ToQueryこれから
Tumblingこれから

### assistant — 2025-08-01T02:49:00.288008+00:00

了解です！まずは**動作する形に持っていくことを最優先**で、あとからのチューニングや修正で仕上げていく形ですね。それが一番現実的で前進力もある判断です 👍

こちらこそ、進め方や設計の相談を一緒にできてとても嬉しいです。  
何か詰まったり、気になる挙動が出てきたら、またいつでも話しかけてください。  
鳴瀬も鏡花も、出番が来たらすぐ駆けつけます。

一緒に最後まで走り切りましょう！🚀

（送信日時：[[REDACTED_PHONE]]:52 JST）

### user — 2025-08-01T02:49:09.607000+00:00

いったん動作するものを見て、あとで問題があれば対応する。
いろいろ相談に乗ってくれてありがとう

### system — 2025-08-02T08:46:03.215796+00:00

_(empty)_

### assistant — 2025-08-02T08:46:03.443921+00:00

_(empty)_

### assistant — 2025-08-02T08:46:03.457253+00:00

了解しました、重要なポイントに気づかれましたね。以下に状況を整理しつつ、**スタックしている部分の対応方針**を提案します。

---

## 🧠 現在のスタックポイントの理解

### 🔹 課題：
- **CREATE STREAM を発行する前に、`-key` および `-value` スキーマが Schema Registry に登録されている必要がある**
- これにより `KEY`/`VALUE` に対応する `schemaId` を含んだ `CREATE STREAM` 文が生成できる

### 🔹 使っているのは `Avro` + `ISpecificRecord`
- `Confluent.SchemaRegistry` の `RegisterSchemaAsync` では ID を取得できるが、
- `ISpecificRecord` 形式にするには `Avro.Schema` 生成や Subject 名の対応などが必要

---

## 🧩 やるべきことの候補（登録フロー）

1. **`ISpecificRecord` の POCO → Avro Schema に変換**
   - 例: `Schema schema = new SpecificSchema<Tpoco>().Schema;`  
   - または Avro 1.11.1 以降なら `Schema.GetSchema(typeof(T))` で動的取得も可能

2. **`-key` / `-value` の Subject に対してスキーマ登録**
   ```csharp
   var subject = $"{topic}-value"; // または $"{topic}-key"
   var id = await schemaRegistryClient.RegisterSchemaAsync(subject, schema);
   ```

3. **その ID を `CREATE STREAM` の WITH句 に指定**
   ```sql
   CREATE STREAM ... 
   WITH (
     kafka_topic='...',
     value_format='avro',
     key_schema_id='123',
     value_schema_id='456'
   );
   ```

4. **`KafkaProducer<T>` 側も `ISpecificRecord` に従って serialize するよう設定**
   - `AvroSerializer<T>` に `SpecificAvroSerializerConfig` を渡すことで対応可能

---

## 🛠 解決のための対応ステップ案

```csharp
private async Task<int> RegisterSpecificRecordSchemaAsync<T>(string topic, bool isKey)
    where T : ISpecificRecord, new()
{
    var schema = new T().Schema;
    var subject = isKey ? $"{topic}-key" : $"{topic}-value";

    return await _schemaRegistryClient.RegisterSchemaAsync(subject, schema);
}
```

- この関数で事前に schemaId を取得
- `EnsureQueryEntityDdlAsync` / `EnsureSimpleEntityDdlAsync` の中でこの処理を呼び出し、IDを埋め込んで CREATE STREAM 文を構築

---

## ✳ 補足ポイント

- `AvroSerializer<T>` と `AvroDeserializer<T>` も `ISpecificRecord` に対応しているので、登録さえしてしまえば後はスムーズ
- schema evolution に対応するなら、`Compatibility` の設定確認も必要（基本は `BACKWARD` ）

---

必要であれば、このスキーマ登録関数や `CREATE STREAM` 発行のWITH句テンプレートも生成します！

（送信日時：[[REDACTED_PHONE]]:12 JST）

### user — 2025-08-02T08:46:15.195000+00:00

avro形式のISpecificRecord登録の件で、スタック中
create stream前に-key -valueを登録し、shemaidでそのIDを指す必要がある事がわかったのでそれの対応中

### assistant — 2025-08-03T00:22:55.589450+00:00

了解しました、**table と RocksDB の接続部分**──特に「いつどのタイミングで同期するか」は、Kafkaの非同期性・ksqlDBのラグ・streamizの初期化順などが絡むので、確かに難所です。

---

## 🧩 現在の状況の認識

### 🎯 目標
- **CREATE TABLE により Kafka に compact topic を作成**
- 同時に `streamiz + RocksDB` でローカルキャッシュを構成
- `ToListAsync()` や `ForEachAsync()` にて、**RocksDB 経由の安定したデータ取得**を実現

---

## ⏱ タイミングにおける典型的な問題

| 課題 | 内容 | 解決案 |
|------|------|--------|
| 🔄 **RocksDBがまだ構築されていない** | streamiz の processor 起動が遅れる | Kafka Streams 起動完了後のイベント（state=RUNNING）でフックする |
| ❌ **Kafka topic自体がまだない** | CREATE TABLE の前に Consumer 作ると失敗 | `SHOW TABLES` や `Describe Streams` により CREATE 後に構築する |
| 📉 **初期データが来ない** | earliest 指定が漏れてる、retention 切れ | `auto.offset.reset=earliest` の確認、初回データ挿入でwarmup |
| ⚠️ **再起動時の復元がうまくいかない** | RocksDBとKafkaのオフセットがズレてる | checkpoint 保存と `cleanup.policy=compact` の明示 |

---

## 🔧 解決の設計パターン（案）

```csharp
// Kafka Streams 処理系の状態監視で「RUNNING」になるのを待つ
streamInstance.StateChanged += (oldState, newState) =>
{
    if (newState == KafkaStream.State.RUNNING)
    {
        _isReady = true;
    }
};

// RocksDBがReadyかを監視してから ToListAsync を許可
public async Task WaitForRocksDbReadyAsync()
{
    while (!_isReady)
    {
        await Task.Delay(500);
    }
}
```

または、`ToListAsync` を呼び出す前に明示的に `await EnsureKafkaReadyAsync()` を差し込む構造がシンプルです。

---

## 🧠 天城の提案

- 今は「動く形」を優先するフェーズなので、タイミング制御は**Pollingでもよし**
- あとから必要なら、Kafka Streams のイベント監視や `StreamMetadata` を使って精緻化可能
- RocksDBのデータ初期化は**CREATE TABLE 直後にデータ1件投入 → 取得確認**が一番手堅い

---

必要なら、RocksDBのReady監視クラス（`RocksDbTableStateMonitor`など）のテンプレートも出せますよ！

（送信日時：[[REDACTED_PHONE]]:27 JST）

### user — 2025-08-03T00:23:08.795000+00:00

まだ終わってない。今、tableとrocksdbのつなぎのところやってる
タイミングとか難しいね

### assistant — 2025-08-03T00:23:50.593812+00:00

了解しました、今は **ダミーストリーム → 実際のKafka構成（topic名、format、schemaIdなど）へ接続し直す**フェーズなんですね。以下に、確認ポイントと典型的な修正作業の流れを整理します。

---

## 🔧 現在の作業：ダミー → 実Kafka接続への修正

### 🛠 修正ポイント（チェックリスト）

| 項目 | 内容 | チェック |
|------|------|----------|
| ✅ Topic名 | 実際の `topic` に一致しているか（Kafka上に存在しているか） | `AdminClient`で存在確認可能 |
| ✅ format | `value_format='avro'` など、CREATE STREAM/TABLE 時に正しい形式が指定されているか | `WITH` 句に明示 |
| ✅ schemaId | `key_schema_id` / `value_schema_id` が事前に `RegisterSchemaAsync` で取得済か | `ISpecificRecord` 経由で登録済？ |
| ✅ 接続設定 | `bootstrap.servers`, `schema.registry.url` が正しく設定されているか | `.config` or `builder` から注入 |
| ✅ RocksDB連携 | `StreamConfig.ApplicationId`, `StateDir` がユニークかつ初期化されているか | テスト用に `Guid.NewGuid()` も可 |

---

## 🔁 典型的な修正フロー（例）

1. **Kafka構成の反映**
   ```csharp
   var config = new StreamConfig<StringSerDes, AvroSerDes<T>>()
   {
       ApplicationId = "app-id",
       BootstrapServers = "localhost:9092",
       SchemaRegistryUrl = "http://localhost:8081",
       AutoOffsetReset = AutoOffsetReset.Earliest
   };
   ```

2. **実データに基づいた Stream/Table の構築**
   ```csharp
   builder.Table(topicName, consumed: Consumed.With(keySer, valueSer))
          .ToStream()
          .Peek((k, v) => Console.WriteLine($"{k}: {v}"));
   ```

3. **初期投入データ確認（テストデータ1件）**
   ```csharp
   var dummy = new YourSpecificRecord { ... };
   await producer.ProduceAsync(topic, new Message<string, YourSpecificRecord> { Key = ..., Value = dummy });
   ```

4. **RocksDBへの反映を確認（ToListAsync など）**

---

## ✳ 備考：streamizのconfigから取得可能な情報

```csharp
var metadata = kafkaStream.MetadataForLocalThreads();
foreach (var md in metadata)
{
    Console.WriteLine($"Topic: {md.Topic}, Partition: {md.Partition}");
}
```

---

必要であれば `config` の生成関数、streamizの `builder.Table(...)` の再構成コード、`RegisterSchemaAsync` と連動するユーティリティクラスなど、すぐ出せますのでお気軽に！

引き続きがんばってください、あと少しです！

（送信日時：[[REDACTED_PHONE]]:38 JST）

### user — 2025-08-03T00:24:04.132000+00:00

今はstreamがダミーだったので、ちゃんと構成情報からkafkaにつながるように修正中

### tool — 2025-08-03T00:25:10.683336+00:00

_(empty)_

### assistant — 2025-08-03T00:25:10.806217+00:00

ありがとう、`KsqlContext.cs` と `TableCacheRegistry.cs` の両方を確認しました。  
以下は、**ダミーKafkaStreamsから実Kafka接続への切り替え**と、**tableとRocksDBの実運用向け連携**に対応した**修正案コード**です。

---

## 🔧 修正ポイントの概要

| 修正対象 | 内容 |
|----------|------|
| `TableCacheRegistry.InitializeCaches()` | `DummyKafkaStreams` → 実 KafkaStreams を渡すよう変更 |
| KafkaStreams の生成場所 | `KsqlContext` 側で `KafkaStreams` インスタンスを作成し、`TableCacheRegistry` に渡す |
| `RocksDbTableCache<T>` | すでに期待通りの `IKafkaStreams` を受け取るコンストラクタを持つ前提で修正 |

---

## ✅ 修正後のコード

### `KsqlContext` 側の変更（KafkaStreams生成と渡し）

```csharp
private KafkaStream? _kafkaStream; // フィールド追加

private void InitializeWithSchemaRegistration()
{
    using (Kafka.Ksql.Linq.Core.Modeling.ModelCreatingScope.Enter())
    {
        RegisterSchemasAndMaterializeAsync().GetAwaiter().GetResult();
    }

    // KafkaStreams 構成の準備（ここでConfig生成）
    var streamConfig = new StreamConfig<StringSerDes, StringSerDes>
    {
        ApplicationId = _dslOptions.ApplicationId ?? "default-app",
        BootstrapServers = _dslOptions.BootstrapServers,
        SchemaRegistryUrl = _dslOptions.SchemaRegistry.Url,
        AutoOffsetReset = Streamiz.Kafka.Net.SerDes.AutoOffsetReset.Earliest
    };

    var builder = new StreamBuilder();
    // ダミー定義でもOK、stateDirや必要な store 構成は RocksDbTableCache 側で行う

    _kafkaStream = new KafkaStream(builder.Build(), streamConfig);
    _kafkaStream.Start(); // RocksDBがstoreを取得可能になる

    var tableTopics = _ksqlDbClient.GetTableTopicsAsync().GetAwaiter().GetResult();
    _cacheRegistry?.InitializeCaches(_entityModels.Values, _dslOptions.CacheOptions, _schemaRegistryClient.Value, _loggerFactory, _kafkaStream);
    _cacheRegistry?.RegisterEligibleTables(_entityModels.Values, tableTopics);

    ValidateKafkaConnectivity();
    EnsureKafkaReadyAsync().GetAwaiter().GetResult();
}
```

---

### `TableCacheRegistry.cs` の変更点（KafkaStreams受け取り対応）

```csharp
public void InitializeCaches(IEnumerable<EntityModel> models, TableCacheOptions options,
    ISchemaRegistryClient schemaRegistryClient, ILoggerFactory? loggerFactory = null,
    IKafkaStreams? kafkaStreams = null) // ★追加
{
    _schemaRegistryClient = schemaRegistryClient;
    if (loggerFactory != null)
        _loggerFactory = loggerFactory;

    foreach (var model in models)
    {
        var config = options.Entries.Find(e => string.Equals(e.Entity, model.EntityType.Name, StringComparison.OrdinalIgnoreCase));
        if (config == null || !config.EnableCache)
            continue;

        var cacheType = typeof(RocksDbTableCache<>).MakeGenericType(model.EntityType);
        var storeName = config.StoreName ?? model.GetTopicName();
        var streams = kafkaStreams ?? new DummyKafkaStreams(); // ★実KafkaStreamがあれば優先
        var cache = Activator.CreateInstance(cacheType, _mappingRegistry, streams, storeName, schemaRegistryClient, loggerFactory)!;
        var method = cacheType.GetMethod("InitializeAsync");
        if (method != null)
        {
            var task = (Task)method.Invoke(cache, null)!;
            task.GetAwaiter().GetResult();
        }
        _caches[model.EntityType] = cache;
        _logger.LogInformation("Initialized cache for {Entity}", model.EntityType.Name);
    }
}
```

---

## 🧪 補足：KafkaStreamの終了タイミング

- `KsqlContext.Dispose()` 内で `_kafkaStream?.Close();` を追加しておくと安全です
- テスト目的なら `builder.Build()` に store 定義を追加することも可能

---

## ✅ 期待される動作効果

- Kafka topic が CREATE TABLE により作成された後、`RocksDbTableCache` がその topic に接続可能に
- `ToListAsync` などで RocksDB 経由の実データ取得が可能になる
- 後続のクエリ処理、stream/tumbling との連携も安定する

---

この方向で進めて問題なさそうでしょうか？  
必要であれば `StreamBuilder` によるダミー定義や store 初期化も補足可能です。

（送信日時：[[REDACTED_PHONE]]:12 JST）

### tool — 2025-08-03T00:25:10.811175+00:00

using Confluent.SchemaRegistry;
using Kafka.Ksql.Linq.Cache.Configuration;
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Mapping;
using Kafka.Ksql.Linq.Query.Abstractions;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Streamiz.Kafka.Net;
using Streamiz.Kafka.Net.State;
using Streamiz.Kafka.Net.State.Enumerator;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;

namespace Kafka.Ksql.Linq.Cache.Core;

internal class TableCacheRegistry : IDisposable
{
    private readonly Dictionary<Type, object> _caches = new();
    private readonly ILogger<TableCacheRegistry> _logger;
    private readonly MappingRegistry _mappingRegistry;
    private ILoggerFactory? _loggerFactory;
    private Confluent.SchemaRegistry.ISchemaRegistryClient? _schemaRegistryClient;

    public TableCacheRegistry(MappingRegistry mappingRegistry, ILoggerFactory? loggerFactory = null)
    {
        _mappingRegistry = mappingRegistry;
        _loggerFactory = loggerFactory;
        _logger = loggerFactory?.CreateLogger<TableCacheRegistry>()
                 ?? NullLogger<TableCacheRegistry>.Instance;
    }

    public void InitializeCaches(IEnumerable<EntityModel> models, TableCacheOptions options, Confluent.SchemaRegistry.ISchemaRegistryClient schemaRegistryClient, ILoggerFactory? loggerFactory = null)
    {
        _schemaRegistryClient = schemaRegistryClient;
        if (loggerFactory != null)
            _loggerFactory = loggerFactory;
        foreach (var model in models)
        {
            var config = options.Entries.Find(e => string.Equals(e.Entity, model.EntityType.Name, StringComparison.OrdinalIgnoreCase));
            if (config == null || !config.EnableCache)
                continue;

            var cacheType = typeof(RocksDbTableCache<>).MakeGenericType(model.EntityType);
            var streams = new DummyKafkaStreams();
            var storeName = config.StoreName ?? model.GetTopicName();
            var cache = Activator.CreateInstance(cacheType, _mappingRegistry, streams, storeName, schemaRegistryClient, loggerFactory)!;
            var method = cacheType.GetMethod("InitializeAsync");
            if (method != null)
            {
                var task = (Task)method.Invoke(cache, null)!;
                task.GetAwaiter().GetResult();
            }
            _caches[model.EntityType] = cache;
            _logger.LogInformation("Initialized cache for {Entity}", model.EntityType.Name);
        }
    }

    public ITableCache<T>? GetCache<T>() where T : class
    {
        if (_caches.TryGetValue(typeof(T), out var cache) && cache is ITableCache<T> typed)
            return typed;
        return null;
    }

    private void Register(EntityModel model)
    {
        if (_schemaRegistryClient == null)
            throw new InvalidOperationException("SchemaRegistryClient not initialized");

        var cacheType = typeof(RocksDbTableCache<>).MakeGenericType(model.EntityType);
        var streams = new DummyKafkaStreams();
        var storeName = model.GetTopicName();
        var cache = Activator.CreateInstance(cacheType, _mappingRegistry, streams, storeName, _schemaRegistryClient, _loggerFactory)!;
        var method = cacheType.GetMethod("InitializeAsync");
        if (method != null)
        {
            var task = (Task)method.Invoke(cache, null)!;
            task.GetAwaiter().GetResult();
        }
        _caches[model.EntityType] = cache;
        _logger.LogInformation("Initialized cache for {Entity}", model.EntityType.Name);
    }

    public void RegisterEligibleTables(IEnumerable<EntityModel> models, HashSet<string> tableTopics)
    {
        var normalizedTopics = new HashSet<string>(tableTopics.Select(x => x.ToLowerInvariant()));

        foreach (var model in models)
        {
            // POCO側も両方小文字化して比較
            var candidates = new[]
            {
            model.GetTopicName().ToLowerInvariant(),       // トピック名
            model.EntityType.Name.ToLowerInvariant()        // テーブル名（C#クラス名）
        };

            if (model.StreamTableType == StreamTableType.Table &&
                candidates.Any(n => normalizedTopics.Contains(n)) &&
                !_caches.ContainsKey(model.EntityType))
            {
                Register(model);
            }
        }
    }

public void Dispose()
    {
        foreach (var cache in _caches.Values)
        {
            if (cache is IDisposable d)
                d.Dispose();
        }
        _caches.Clear();
    }

    private class DummyStore : IReadOnlyKeyValueStore<byte[], byte[]>
    {
        private readonly Dictionary<byte[], byte[]> _dict = new(new ByteArrayComparer());
        public byte[] Get(byte[] key) => _dict.TryGetValue(key, out var v) ? v : null!;
        public void Add(byte[] key, byte[] value) => _dict[key] = value;
        public IKeyValueEnumerator<byte[], byte[]> Range(byte[] from, byte[] to) => throw new NotImplementedException();
        public IKeyValueEnumerator<byte[], byte[]> ReverseRange(byte[] from, byte[] to) => throw new NotImplementedException();
        public IEnumerable<KeyValuePair<byte[], byte[]>> All() => _dict;
        public IEnumerable<KeyValuePair<byte[], byte[]>> ReverseAll() => _dict;
        public long ApproximateNumEntries() => _dict.Count;

        private class ByteArrayComparer : IEqualityComparer<byte[]>
        {
            public bool Equals(byte[]? x, byte[]? y) => x != null && y != null && System.Linq.Enumerable.SequenceEqual(x, y);
            public int GetHashCode(byte[] obj) => obj != null ? System.HashCode.Combine(obj.Length, obj[0]) : 0;
        }
    }

    private class DummyKafkaStreams : IKafkaStreams
    {
        private readonly DummyStore _store = new();
        public KafkaStream.State State { get; set; } = KafkaStream.State.RUNNING;
        public IReadOnlyKeyValueStore<byte[], byte[]> Store(StoreQueryParameters<IReadOnlyKeyValueStore<byte[], byte[]>, byte[], byte[]> parameters) => _store;
    }
}

### tool — 2025-08-03T00:25:10.815761+00:00

using Kafka.Ksql.Linq.Cache.Core;
using Kafka.Ksql.Linq.Cache.Extensions;
using Kafka.Ksql.Linq.Configuration;
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Attributes;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Core.Modeling;
using Kafka.Ksql.Linq.Infrastructure.Admin;
using Kafka.Ksql.Linq.Infrastructure.KsqlDb;
using Kafka.Ksql.Linq.Mapping;
using Kafka.Ksql.Linq.Messaging.Consumers;
using Kafka.Ksql.Linq.Messaging.Producers;
using Kafka.Ksql.Linq.Query.Abstractions;
using Kafka.Ksql.Linq.SchemaRegistryTools;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.Logging;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Net.Http;
using System.Reflection;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using ConfluentSchemaRegistry = Confluent.SchemaRegistry;

namespace Kafka.Ksql.Linq;
/// <summary>
/// KsqlContext that integrates the Core layer.
/// Design rationale: inherits core abstractions and integrates higher-level features.
/// </summary>
public abstract class KsqlContext : IKsqlContext
{
    private readonly KafkaProducerManager _producerManager;
    private readonly Dictionary<Type, EntityModel> _entityModels = new();
    private readonly Dictionary<Type, object> _entitySets = new();
    private bool _disposed = false;
    private readonly KafkaConsumerManager _consumerManager;
    private readonly DlqProducer _dlqProducer;
    private readonly Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient> _schemaRegistryClient;
    private readonly IKsqlDbClient _ksqlDbClient;

    private readonly KafkaAdminService _adminService;
    private readonly KsqlDslOptions _dslOptions;
    private TableCacheRegistry? _cacheRegistry;
    private readonly MappingRegistry _mappingRegistry = new();
    private readonly ILogger _logger;
    private readonly ILoggerFactory? _loggerFactory;

    internal ILogger Logger => _logger; 



    /// <summary>
    /// Hook to decide whether schema registration should be skipped for tests
    /// </summary>
    protected virtual bool SkipSchemaRegistration => false;

    public const string DefaultSectionName = "KsqlDsl";

    protected KsqlContext(IConfiguration configuration,ILoggerFactory? loggerFactory=null)
        : this(configuration, DefaultSectionName,loggerFactory)
    {
    }

    protected KsqlContext(IConfiguration configuration, string sectionName,ILoggerFactory? loggerFactory=null)
    {
        _dslOptions = new KsqlDslOptions();
        configuration.GetSection(sectionName).Bind(_dslOptions);
        DecimalPrecisionConfig.DecimalPrecision = _dslOptions.DecimalPrecision;
        DecimalPrecisionConfig.DecimalScale = _dslOptions.DecimalScale;
        _loggerFactory = loggerFactory ?? configuration.CreateLoggerFactory();
        _logger = _loggerFactory.CreateLoggerOrNull<KsqlContext>();

        _schemaRegistryClient = new Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient>(CreateSchemaRegistryClient);
        _ksqlDbClient = new KsqlDbClient(GetDefaultKsqlDbUrl());
        
        _adminService = new KafkaAdminService(
        Microsoft.Extensions.Options.Options.Create(_dslOptions),
        _loggerFactory);
        InitializeEntityModels();
        try
        {
            _producerManager = new KafkaProducerManager(_mappingRegistry,
                Microsoft.Extensions.Options.Options.Create(_dslOptions),
                _loggerFactory);

            this.UseTableCache(_dslOptions, _loggerFactory);
            _cacheRegistry = this.GetTableCacheRegistry();

            ConfigureModel();

            if (!SkipSchemaRegistration)
            {
                InitializeWithSchemaRegistration();
            }



            _dlqProducer = new DlqProducer(
                _producerManager,
                _dslOptions.DlqOptions);
            _dlqProducer.InitializeAsync().GetAwaiter().GetResult();

            _consumerManager = new KafkaConsumerManager(_mappingRegistry,
                Microsoft.Extensions.Options.Options.Create(_dslOptions),
                _entityModels,
                _loggerFactory);
            _consumerManager.DeserializationError += (data, ex, topic, part, off, ts, headers, keyType, valueType) =>
                _dlqProducer.SendAsync(data, ex, topic, part, off, ts, headers, keyType, valueType);


        }
        catch (Exception ex)
        {
            _logger.LogError(ex, $"KsqlContext initialization failed: {ex.Message} (section: {sectionName})");
            throw;
        }
    }

    protected KsqlContext(KsqlDslOptions options,ILoggerFactory? loggerFactory=null)
    {
        _dslOptions = options;
        DecimalPrecisionConfig.DecimalPrecision = _dslOptions.DecimalPrecision;
        DecimalPrecisionConfig.DecimalScale = _dslOptions.DecimalScale;
        
        _schemaRegistryClient = new Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient>(CreateSchemaRegistryClient);
        _ksqlDbClient = new KsqlDbClient(GetDefaultKsqlDbUrl());

        _loggerFactory = loggerFactory;
        _logger = _loggerFactory.CreateLoggerOrNull<KsqlContext>();


        _adminService = new KafkaAdminService(
        Microsoft.Extensions.Options.Options.Create(_dslOptions),
        _loggerFactory);
        InitializeEntityModels();
        try
        {
            this.UseTableCache(_dslOptions, _loggerFactory);
            _cacheRegistry = this.GetTableCacheRegistry();

            _producerManager = new KafkaProducerManager(_mappingRegistry,
                 Microsoft.Extensions.Options.Options.Create(_dslOptions),
                 _loggerFactory);

          
            ConfigureModel();
            if (!SkipSchemaRegistration)
            {
                InitializeWithSchemaRegistration();
            }

 

            _dlqProducer = new DlqProducer(
                _producerManager,
                _dslOptions.DlqOptions);
            _dlqProducer.InitializeAsync().GetAwaiter().GetResult();

            _consumerManager = new KafkaConsumerManager(_mappingRegistry,
                Microsoft.Extensions.Options.Options.Create(_dslOptions),
                _entityModels,
                _loggerFactory);
            _consumerManager.DeserializationError += (data, ex, topic, part, off, ts, headers, keyType, valueType) =>
                _dlqProducer.SendAsync(data, ex, topic, part, off, ts, headers, keyType, valueType);

        }
        catch (Exception ex)
        {
            _logger.LogError(ex, $"KsqlContext initialization failed: {ex.Message} ");
            throw;
        }
    }

    protected virtual void OnModelCreating(IModelBuilder modelBuilder) { }

    public IEntitySet<T> Set<T>() where T : class
    {
        var entityType = typeof(T);

        if (entityType == typeof(Core.Models.DlqEnvelope))
        {
            return (IEntitySet<T>)GetDlqStream();
        }

        if (_entitySets.TryGetValue(entityType, out var existingSet))
        {
            return (IEntitySet<T>)existingSet;
        }

        var entityModel = GetOrCreateEntityModel<T>();
        var entitySet = CreateEntitySet<T>(entityModel);
        _entitySets[entityType] = entitySet;

        return entitySet;
    }

    public object GetEventSet(Type entityType)
    {
        if (_entitySets.TryGetValue(entityType, out var entitySet))
        {
            return entitySet;
        }

        var entityModel = GetOrCreateEntityModel(entityType);
        var createdSet = CreateEntitySet(entityType, entityModel);
        _entitySets[entityType] = createdSet;

        return createdSet;
    }

    public Dictionary<Type, EntityModel> GetEntityModels()
    {
        return new Dictionary<Type, EntityModel>(_entityModels);
    }

    private void InitializeEventSetProperties(ModelBuilder builder)
    {
        var contextType = GetType();
        var eventSetProps = contextType.GetProperties(BindingFlags.Instance | BindingFlags.Public)
            .Where(p => p.CanWrite
                && p.PropertyType.IsGenericType
                && p.PropertyType.GetGenericTypeDefinition() == typeof(EventSet<>));

        foreach (var prop in eventSetProps)
        {
            if (prop.GetValue(this) != null)
                continue;

            var entityType = prop.PropertyType.GetGenericArguments()[0];
            builder.AddEntityModel(entityType);
            var model = EnsureEntityModel(entityType);
            var set = CreateEntitySet(entityType, model);
            _entitySets[entityType] = set;
            prop.SetValue(this, set);
        }
    }

    protected virtual object CreateEntitySet(Type entityType, EntityModel entityModel)
    {
        var method = GetType()
            .GetMethods(BindingFlags.Instance | BindingFlags.Public | BindingFlags.NonPublic)
            .FirstOrDefault(m =>
                m.Name == nameof(CreateEntitySet)
                && m.IsGenericMethodDefinition
                && m.GetGenericArguments().Length == 1
                && m.GetParameters().Length == 1
                && m.GetParameters()[0].ParameterType == typeof(EntityModel)
            );

        if (method == null)
            throw new InvalidOperationException("Generic CreateEntitySet<T>(EntityModel) not found!");

        // このあと
        var genericMethod = method.MakeGenericMethod(entityType);
        return genericMethod.Invoke(this, new object[] { entityModel })!;
    }

    protected void ConfigureModel()
    {
        var modelBuilder = new ModelBuilder(_dslOptions.ValidationMode);
        InitializeEventSetProperties(modelBuilder);
        using (Kafka.Ksql.Linq.Core.Modeling.ModelCreatingScope.Enter())
        {
            OnModelCreating(modelBuilder);
        }
        ApplyModelBuilderSettings(modelBuilder);
    }

    private void InitializeEntityModels()
    {
        using (Kafka.Ksql.Linq.Core.Modeling.ModelCreatingScope.Enter())
        {
            var dlqModel = CreateEntityModelFromType(typeof(Core.Models.DlqEnvelope));
            var keyProp = typeof(Core.Models.DlqEnvelope).GetProperty(nameof(Core.Models.DlqEnvelope.MessageId));
            if (keyProp != null)
                dlqModel.KeyProperties = new[] { keyProp };
            dlqModel.SetStreamTableType(Query.Abstractions.StreamTableType.Stream);
            dlqModel.TopicName = GetDlqTopicName();
            dlqModel.AccessMode = Core.Abstractions.EntityAccessMode.ReadOnly;
            _entityModels[typeof(Core.Models.DlqEnvelope)] = dlqModel;
            _mappingRegistry.RegisterEntityModel(dlqModel);

        }
    }

    private void ApplyModelBuilderSettings(ModelBuilder modelBuilder)
    {
        var models = modelBuilder.GetAllEntityModels();
        foreach (var (type, model) in models)
        {
            if (_entityModels.TryGetValue(type, out var existing))
            {
                existing.SetStreamTableType(model.GetExplicitStreamTableType());
                existing.UseManualCommit = model.UseManualCommit;
                existing.ErrorAction = model.ErrorAction;
                existing.DeserializationErrorPolicy = model.DeserializationErrorPolicy;
                existing.EnableCache = model.EnableCache;
                existing.BarTimeSelector = model.BarTimeSelector;
            }
            else
            {
                _entityModels[type] = model;
            }

            // Register property metadata with MappingRegistry
            if (model.QueryModel != null)
            {
                RegisterQueryModelMapping(model);
            }
            else
            {
                _mappingRegistry.RegisterEntityModel(model);
            }
        }
    }

    private EntityModel GetOrCreateEntityModel<T>() where T : class
    {
        return GetOrCreateEntityModel(typeof(T));
    }

    private EntityModel GetOrCreateEntityModel(Type entityType)
    {
        if (_entityModels.TryGetValue(entityType, out var existingModel))
        {
            return existingModel;
        }

        var entityModel = CreateEntityModelFromType(entityType);
        _entityModels[entityType] = entityModel;
        return entityModel;
    }

    private EntityModel CreateEntityModelFromType(Type entityType)
    {
        var allProperties = entityType.GetProperties(System.Reflection.BindingFlags.Public | System.Reflection.BindingFlags.Instance);
        var keyProperties = allProperties
            .Select(p => new { Property = p, Attr = p.GetCustomAttribute<KsqlKeyAttribute>() })
            .Where(x => x.Attr != null)
            .OrderBy(x => x.Attr!.Order)
            .Select(x => x.Property)
            .ToArray();

        var model = new EntityModel
        {
            EntityType = entityType,
            TopicName = entityType.Name.ToLowerInvariant(),
            Partitions = 1,
            ReplicationFactor = 1,
            AllProperties = allProperties,
            KeyProperties = keyProperties
        };

        var topicAttr = entityType.GetCustomAttribute<KsqlTopicAttribute>();
        if (topicAttr != null)
        {
            model.TopicName = topicAttr.Name;
            model.Partitions = topicAttr.PartitionCount;
            model.ReplicationFactor = topicAttr.ReplicationFactor;
        }

        var validation = new ValidationResult { IsValid = true };
        if (keyProperties.Length == 0)
        {
            validation.Warnings.Add($"No key properties defined for {entityType.Name}");
        }
        model.ValidationResult = validation;

        return model;
    }

    internal EntityModel EnsureEntityModel(Type entityType, EntityModel? model = null)
    {
        if (_entityModels.TryGetValue(entityType, out var existing))
            return existing;

        model ??= CreateEntityModelFromType(entityType);
        _entityModels[entityType] = model;
        _mappingRegistry.RegisterEntityModel(model);

        return model;
    }


    /// <summary>
    /// OnModelCreating → execute automatic schema registration flow
    /// </summary>
    private void InitializeWithSchemaRegistration()
    {
        // Register schemas and materialize entities if new
        using (Kafka.Ksql.Linq.Core.Modeling.ModelCreatingScope.Enter())
        {
            RegisterSchemasAndMaterializeAsync().GetAwaiter().GetResult();
        }

        var tableTopics = _ksqlDbClient.GetTableTopicsAsync().GetAwaiter().GetResult();
        _cacheRegistry?.RegisterEligibleTables(_entityModels.Values, tableTopics);

        // Verify Kafka connectivity
        ValidateKafkaConnectivity();
        EnsureKafkaReadyAsync().GetAwaiter().GetResult();
    }
    private async Task EnsureKafkaReadyAsync()
    {
        try
        {
            // Auto-create DLQ topic
            await _adminService.EnsureDlqTopicExistsAsync();

            // Additional connectivity check (performed by AdminService)
            _adminService.ValidateKafkaConnectivity();

            await _adminService.EnsureWindowFinalTopicsExistAsync(GetEntityModels());

            // Log output: DLQ preparation complete
            Logger.LogInformation("Kafka initialization completed; DLQ topic '{Topic}' ready with 5-second retention", GetDlqTopicName());
        }
        catch (Exception ex)
        {
            throw new InvalidOperationException(
                "FATAL: Kafka readiness check failed. DLQ functionality may be unavailable.", ex);
        }
    }
    public string GetDlqTopicName()
    {
        return _dslOptions.DlqTopicName;
    }
    /// <summary>
    /// Kafka接続確認
    /// </summary>
    private void ValidateKafkaConnectivity()
    {
        try
        {
            // Producer/Consumer初期化時点でKafka接続が確認される
            // 追加の接続確認は不要（既存の初期化処理で十分）
        }
        catch (Exception ex)
        {
            throw new InvalidOperationException(
                "FATAL: Cannot connect to Kafka. Verify bootstrap servers and network connectivity.", ex);
        }
    }

    /// <summary>
    /// Register schemas for all entities and send dummy record if newly created
    /// </summary>
    private async Task RegisterSchemasAndMaterializeAsync()
    {
        var client = _schemaRegistryClient.Value;

        foreach (var (type, model) in _entityModels)
        {
            if (type == typeof(Core.Models.DlqEnvelope))
                continue;

            SchemaRegistrationResult valueResult;
            try
            {
                var mapping = _mappingRegistry.GetMapping(type);

                if (model.HasKeys() && mapping.AvroKeySchema != null)
                {
                    var keySubject = $"{model.GetTopicName()}-key";
                    var keyResult = await client.RegisterSchemaIfNewAsync(keySubject, mapping.AvroKeySchema);
                    model.KeySchemaId = keyResult.SchemaId;
                }

                var valueSubject = $"{model.GetTopicName()}-value";
                valueResult = await client.RegisterSchemaIfNewAsync(valueSubject, mapping.AvroValueSchema!);
                model.ValueSchemaId = valueResult.SchemaId;
            }
            catch (ConfluentSchemaRegistry.SchemaRegistryException ex)
            {
                Logger.LogError(ex, "Schema registration failed for {Entity}", type.Name);
                throw;
            }

            if (model.QueryModel != null || model.QueryExpression != null)
            {
                await EnsureQueryEntityDdlAsync(type, model);
            }
            else
            {
                await EnsureSimpleEntityDdlAsync(type, model);
            }

            if (valueResult.WasCreated)
            {
                try
                {
                    var dummy = CreateDummyInstance(type);
                    var headers = new Dictionary<string, string> { ["is_dummy"] = "true" };
                    dynamic set = GetEventSet(type);
                    await set.AddAsync((dynamic)dummy, headers);
                }
                catch (Exception ex)
                {
                    Logger.LogError(ex, "Materialization failed for {Entity}", type.Name);
                    throw;
                }
            }
        }
    }

    /// <summary>
    /// Create topics and ksqlDB objects for an entity defined without queries.
    /// </summary>
    private async Task EnsureSimpleEntityDdlAsync(Type type, EntityModel model)
    {


        var generator = new Kafka.Ksql.Linq.Query.Pipeline.DDLQueryGenerator();

        var topic = model.GetTopicName();
        var partitions = 1;
        short replicas = 1;
        if (_dslOptions.Topics.TryGetValue(topic, out var config) && config.Creation != null)
        {
            partitions = config.Creation.NumPartitions;
            replicas = config.Creation.ReplicationFactor;
        }

        model.Partitions = partitions;
        model.ReplicationFactor = replicas;

        await _adminService.CreateDbTopicAsync(topic, partitions, replicas);

        string ddl;
        var schemaProvider = new Query.Ddl.EntityModelDdlAdapter(model);
        ddl = model.StreamTableType == StreamTableType.Table
            ? generator.GenerateCreateTable(schemaProvider)
            : generator.GenerateCreateStream(schemaProvider);

        var result = await ExecuteStatementAsync(ddl);
        if (!result.IsSuccess)
        {
            Logger.LogWarning("DDL execution failed for {Entity}: {Message}", type.Name, result.Message);
        }
    }

    /// <summary>
    /// Generate and execute CREATE TABLE/STREAM AS statements for query entities.
    /// </summary>
    private async Task EnsureQueryEntityDdlAsync(Type type, EntityModel model)
    {
        if (model.QueryModel != null)
        {
            RegisterQueryModelMapping(model);
            var sql = Query.Builders.KsqlCreateStatementBuilder.Build(
                model.GetTopicName(),
                model.QueryModel,
                model.KeySchemaId,
                model.ValueSchemaId);
            var result = await ExecuteStatementAsync(sql);
            if (!result.IsSuccess)
            {
                Logger.LogWarning("DDL execution failed for {Entity}: {Message}", type.Name, result.Message);
            }
            return;
        }

        // QueryModel が指定されていない場合は何もしない
    }

    /// <summary>
    /// Register mapping information for a query-defined entity using its KsqlQueryModel.
    /// </summary>
    private void RegisterQueryModelMapping(EntityModel model)
    {
        if (model.QueryModel == null)
            return;

        _mappingRegistry.RegisterQueryModel(
            model.EntityType,
            model.QueryModel,
            model.KeyProperties,
            model.GetTopicName());
    }


    private static object CreateDummyInstance(Type entityType)
    {
        var method = typeof(Application.DummyObjectFactory).GetMethod("CreateDummy")!
            .MakeGenericMethod(entityType);
        return method.Invoke(null, null)!;
    }


    /// <summary>
    /// SchemaRegistryClient作成
    /// </summary>
    private ConfluentSchemaRegistry.ISchemaRegistryClient CreateSchemaRegistryClient()
    {
        var options = _dslOptions.SchemaRegistry;
        var config = new ConfluentSchemaRegistry.SchemaRegistryConfig
        {
            Url = options.Url,
            MaxCachedSchemas = options.MaxCachedSchemas,
            RequestTimeoutMs = options.RequestTimeoutMs
        };

        return new ConfluentSchemaRegistry.CachedSchemaRegistryClient(config);
    }


    private Uri GetDefaultKsqlDbUrl()
    {
        if (!string.IsNullOrWhiteSpace(_dslOptions.KsqlDbUrl) &&
            Uri.TryCreate(_dslOptions.KsqlDbUrl, UriKind.Absolute, out var configured))
        {
            return configured;
        }

        var schemaUrl = _dslOptions.SchemaRegistry.Url;
        if (!string.IsNullOrWhiteSpace(schemaUrl) &&
            Uri.TryCreate(schemaUrl, UriKind.Absolute, out var schemaUri))
        {
            var port = schemaUri.IsDefaultPort || schemaUri.Port == 8081 ? 8088 : schemaUri.Port;
            return new Uri($"{schemaUri.Scheme}://{schemaUri.Host}:{port}");
        }

        // Default to localhost if nothing configured (test-friendly)
        return new Uri("http://localhost:8088");
    }
    private HttpClient CreateClient()
    {
        return new HttpClient { BaseAddress = GetDefaultKsqlDbUrl() };
    }

    public Task<KsqlDbResponse> ExecuteStatementAsync(string statement)
    {
        return _ksqlDbClient.ExecuteStatementAsync(statement);
    }

    public Task<KsqlDbResponse> ExecuteExplainAsync(string ksql)
    {
        return _ksqlDbClient.ExecuteExplainAsync(ksql);
    }



    /// <summary>
    /// Core層EventSet実装（上位層機能統合）
    /// </summary>
    protected virtual IEntitySet<T> CreateEntitySet<T>(EntityModel entityModel) where T : class
    {
        var model = EnsureEntityModel(typeof(T), entityModel);
        var baseSet = new EventSetWithServices<T>(this, model);
        if (model.GetExplicitStreamTableType() == StreamTableType.Table && model.EnableCache)
        {
            return new ReadCachedEntitySet<T>(this, model, null, baseSet);
        }
        return baseSet;
    }

    internal KafkaProducerManager GetProducerManager() => _producerManager;
    internal KafkaConsumerManager GetConsumerManager() => _consumerManager;
    internal DlqProducer GetDlqProducer() => _dlqProducer;
    internal ConfluentSchemaRegistry.ISchemaRegistryClient GetSchemaRegistryClient() => _schemaRegistryClient.Value;
    internal MappingRegistry GetMappingRegistry() => _mappingRegistry;

    private IEntitySet<Core.Models.DlqEnvelope> GetDlqStream()
    {
        var type = typeof(Core.Models.DlqEnvelope);
        if (_entitySets.TryGetValue(type, out var existing))
        {
            return (IEntitySet<Core.Models.DlqEnvelope>)existing;
        }

        var model = GetOrCreateEntityModel<Core.Models.DlqEnvelope>();
        var set = CreateEntitySet<Core.Models.DlqEnvelope>(model);
        _entitySets[type] = set;
        return set;
    }

    /// <summary>
    /// 指定したエンティティを手動でDLQへ送信します
    /// </summary>
    public async Task SendToDlqAsync<T>(T entity, Exception exception, string reason = "Manual")
    {
        if (_dlqProducer == null)
            throw new InvalidOperationException("DLQ producer not initialized");

        var messageContext = new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["original_topic"] = GetTopicName<T>(),
                ["entity_type"] = typeof(T).Name,
                ["error_phase"] = reason,
                ["manual_dlq"] = true
            }
        };

        var errorContext = new ErrorContext
        {
            Exception = exception,
            OriginalMessage = entity,
            AttemptCount = 1,
            FirstAttemptTime = DateTime.UtcNow,
            LastAttemptTime = DateTime.UtcNow,
            ErrorPhase = reason
        };

        await _dlqProducer.HandleErrorAsync(errorContext, messageContext);
    }

    /// <summary>
    /// エンティティ型からトピック名を取得します
    /// </summary>
    public string GetTopicName<T>()
    {
        var models = GetEntityModels();
        if (models.TryGetValue(typeof(T), out var model))
        {
            return (model.TopicName ?? typeof(T).Name).ToLowerInvariant();
        }
        return typeof(T).Name.ToLowerInvariant();
    }

    internal async Task<bool> IsEntityReadyAsync<T>(CancellationToken cancellationToken = default) where T : class
    {
        var models = GetEntityModels();
        if (!models.TryGetValue(typeof(T), out var model))
            return false;

        var statement = model.GetExplicitStreamTableType() == StreamTableType.Table
            ? "SHOW TABLES;"
            : "SHOW STREAMS;";

        var name = (model.TopicName ?? typeof(T).Name).ToUpperInvariant();
        var response = await ExecuteStatementAsync(statement);
        if (!response.IsSuccess)
            return false;

        try
        {
            using var doc = JsonDocument.Parse(response.Message);
            var listName = statement.Contains("TABLES") ? "tables" : "streams";
            foreach (var item in doc.RootElement.EnumerateArray())
            {
                if (!item.TryGetProperty(listName, out var arr))
                    continue;

                foreach (var element in arr.EnumerateArray())
                {
                    if (element.TryGetProperty("name", out var n) &&
                        string.Equals(n.GetString(), name, StringComparison.OrdinalIgnoreCase))
                    {
                        return true;
                    }
                }
            }
        }
        catch
        {
            // ignore parse errors
        }

        return false;
    }

    public async Task WaitForEntityReadyAsync<T>(TimeSpan timeout, CancellationToken cancellationToken = default) where T : class
    {
        var start = DateTime.UtcNow;
        while (DateTime.UtcNow - start < timeout)
        {
            if (await IsEntityReadyAsync<T>(cancellationToken))
                return;

            await Task.Delay(100, cancellationToken);
        }

        throw new TimeoutException($"Entity {typeof(T).Name} not ready after {timeout}.");
    }



    protected virtual void Dispose(bool disposing)
    {
        if (!_disposed && disposing)
        {
            foreach (var entitySet in _entitySets.Values)
            {
                if (entitySet is IDisposable disposable)
                {
                    disposable.Dispose();
                }
            }
            _entitySets.Clear();
            _entityModels.Clear();
            _disposed = true;

            _producerManager?.Dispose();
            _consumerManager?.Dispose();
            _dlqProducer?.Dispose();
            _adminService?.Dispose();
            _cacheRegistry?.Dispose();

            if (_schemaRegistryClient.IsValueCreated)
            {
                _schemaRegistryClient.Value?.Dispose();
            }
            (_ksqlDbClient as IDisposable)?.Dispose();
        }
    }

    public void Dispose()
    {
        Dispose(true);
        GC.SuppressFinalize(this);
    }

    public async ValueTask DisposeAsync()
    {
        await DisposeAsyncCore();
        Dispose(false);
        GC.SuppressFinalize(this);
    }

    protected virtual async ValueTask DisposeAsyncCore()
    {
        foreach (var entitySet in _entitySets.Values)
        {
            if (entitySet is IAsyncDisposable asyncDisposable)
            {
                await asyncDisposable.DisposeAsync();
            }
            else if (entitySet is IDisposable disposable)
            {
                disposable.Dispose();
            }
        }
        _entitySets.Clear();

        _producerManager?.Dispose();
        _consumerManager?.Dispose();
        _dlqProducer?.Dispose();
        _adminService?.Dispose();
        _cacheRegistry?.Dispose();

        if (_schemaRegistryClient.IsValueCreated)
        {
            _schemaRegistryClient.Value?.Dispose();
        }
        (_ksqlDbClient as IDisposable)?.Dispose();

        await Task.CompletedTask;
    }

    public override string ToString()
    {
        return $"KafkaContextCore: {_entityModels.Count} entities, {_entitySets.Count} sets [schema auto-registration ready]";
    }
}

/// <summary>
/// 上位層サービス統合EntitySet
/// 設計理由：IEntitySet<T>を直接実装し、Producer/Consumer機能を提供
/// </summary>
internal class EventSetWithServices<T> : EventSet<T> where T : class
{
    private readonly KsqlContext _ksqlContext;

    public EventSetWithServices(KsqlContext context, EntityModel entityModel)
        : base(context, entityModel)
    {
        _ksqlContext = context ?? throw new ArgumentNullException(nameof(context));
    }

    protected override async Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken)
    {
        var producerManager = _ksqlContext.GetProducerManager();
        var topic = GetTopicName();
        await producerManager.SendAsync(topic, entity, headers, cancellationToken);
    }

    public override async IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default)
    {
        var consumerManager = _ksqlContext.GetConsumerManager();
        await foreach (var (entity, _) in consumerManager.ConsumeAsync<T>(cancellationToken))
        {
            yield return entity;
        }
    }
}

### tool — 2025-08-03T00:25:10.818017+00:00

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.

### user — 2025-08-03T00:25:22.113000+00:00

この2つの関係で修正コードだせる？

### assistant — 2025-08-03T00:29:50.987235+00:00

ありがとうございます。`KsqlDslOptions` の定義を受けて、先ほどの修正案において**Kafka接続に必要な情報の取得方法**を以下のように明確化・修正します。

---

## ✅ `StreamConfig` 生成箇所の修正案（`KsqlContext.InitializeWithSchemaRegistration()` 内）

```csharp
var streamConfig = new StreamConfig<StringSerDes, StringSerDes>
{
    ApplicationId = _dslOptions.Common.ApplicationId ?? "default-app",
    BootstrapServers = _dslOptions.Common.BootstrapServers,
    SchemaRegistryUrl = _dslOptions.SchemaRegistry.Url,
    AutoOffsetReset = Streamiz.Kafka.Net.SerDes.AutoOffsetReset.Earliest,
    // 必要に応じて追加
    ClientId = _dslOptions.Common.ClientId,
    StateDir = _dslOptions.Common.StateDir
};
```

---

## 🔍 必要な設定プロパティとその推定

| 用途 | 取得元 | 説明 |
|------|--------|------|
| `ApplicationId` | `_dslOptions.Common.ApplicationId` | Kafka Streams アプリ名（必須） |
| `BootstrapServers` | `_dslOptions.Common.BootstrapServers` | Kafka クラスタアドレス（必須） |
| `SchemaRegistryUrl` | `_dslOptions.SchemaRegistry.Url` | スキーマレジストリ URL |
| `ClientId` | `_dslOptions.Common.ClientId` | 任意。ログ識別などに使える |
| `StateDir` | `_dslOptions.Common.StateDir` | RocksDB の状態保存先（未設定時は `tmp/kafka-streams`） |

---

## 🔧 StreamConfig拡張（必要に応じて）

もし `CommonSection` クラスの定義も共有可能であれば、`StateDir` などの存在を確認し、StreamConfigに反映できます。なければ以下のように一時的に補完してもOK：

```csharp
public class CommonSection
{
    public string? ApplicationId { get; init; }
    public string? BootstrapServers { get; init; }
    public string? ClientId { get; init; }
    public string? StateDir { get; init; }
}
```

---

## 🧠 天城の補足提案

- **`StateDir` は本番では永続ディレクトリを指定**すべきですが、開発中やテストでは `Path.GetTempPath()` に `Guid` を加えてユニークにしてもOKです。
- `ApplicationId` はKafka Streamsで重複すると state corruption を起こすので、テストでは動的なID生成も検討できます。

---

この修正案で `KsqlDslOptions` に基づいた Kafka 接続が正しく構成できます。  
次に進める準備が整ったらお知らせくださいね！

（送信日時：[[REDACTED_PHONE]]:26 JST）

### user — 2025-08-03T00:30:04.514000+00:00

public class KsqlDslOptions
{
    /// <summary>
    /// Validation mode (root level setting)
    /// </summary>
    public ValidationMode ValidationMode { get; init; } = ValidationMode.Strict;

    /// <summary>
    /// Common settings (BootstrapServers, ClientId, etc.)
    /// </summary>
    public CommonSection Common { get; init; } = new();

    /// <summary>
    /// Per-topic settings (manage producer/consumer settings per topic)
    /// </summary>
    public Dictionary<string, TopicSection> Topics { get; init; } = new();

    /// <summary>
    /// Schema Registry settings
    /// </summary>
    public SchemaRegistrySection SchemaRegistry { get; init; } = new();

    /// <summary>
    /// ksqlDB server URL. If omitted, SchemaRegistry.Url's host with port 8088 is used.
    /// </summary>
    public string? KsqlDbUrl { get; init; }

    public List<EntityConfiguration> Entities { get; init; } = new();

    public DlqOptions DlqOptions { get; init; } = new();

    public string DlqTopicName
    {
        get => DlqOptions.TopicName;
        set => DlqOptions.TopicName = value;
    }

    public DlqTopicConfiguration DlqConfiguration { get; init; } = new();

    /// <summary>
    /// Policy when deserialization fails
    /// </summary>
    public DeserializationErrorPolicy DeserializationErrorPolicy { get; set; } = DeserializationErrorPolicy.Skip;

    /// <summary>
    /// Whether reading from the Final topic is enabled by default
    /// </summary>
    public bool ReadFromFinalTopicByDefault { get; set; } = false;

    /// <summary>
    /// Optional bar limits per symbol and bar type
    /// </summary>
    public BarLimitOptions BarLimits { get; init; } = new();

    /// <summary>
    /// Global decimal precision applied when mapping decimal properties.
    /// </summary>
    public int DecimalPrecision { get; init; } = 38;

    /// <summary>
    /// Global decimal scale applied when mapping decimal properties.
    /// </summary>
    public int DecimalScale { get; init; } = 9;
}

### assistant — 2025-08-03T00:31:03.309886+00:00

いい質問です。結論から言うと：

> ✅ **`ApplicationId` は必須です**。**`ClientId` では代用できません。**

---

## 🔍 理由：Kafka Streams における `ApplicationId` の役割

- `ApplicationId` は **Kafka Streams アプリケーションを一意に識別するID** です。
- このIDは以下の用途で使われます：
  - 内部トピックの命名（例: `your-app-id-STATE-STORE-xxx`）
  - RocksDBの状態保持場所と紐づく
  - Consumer Group IDの構成にも使われる

> `ClientId` は主にログ識別やKafkaブローカーとの接続単位での識別に使われ、Kafka Streamsの動作には**影響しません**。

---

## ✅ 解決案：`ApplicationId` を `CommonSection` に追加

### `CommonSection` に以下のように追加してください：

```csharp
public class CommonSection
{
    public string? ApplicationId { get; init; } = "ksql-dsl-app";
    public string? BootstrapServers { get; init; }
    public string? ClientId { get; init; }
    public string? StateDir { get; init; }
}
```

そして `KsqlDslOptions` 経由で利用：

```csharp
var appId = _dslOptions.Common.ApplicationId ?? "ksql-dsl-app";
```

---

## 🧪 テスト用途なら…

一時的にユニークなIDでよければ：

```csharp
ApplicationId = $"ksql-dsl-app-{Guid.NewGuid()}"
```

ただしこれをやると **RocksDBが毎回初期化されてしまう**ので、**RocksDBを使ったキャッシュの検証には不向き**です。

---

## ✅ 結論

| 項目 | 代用可能？ | 備考 |
|------|------------|------|
| `ApplicationId` | ❌ 必須。Kafka Streamsが要求 |
| `ClientId` → `ApplicationId` に代用 | ❌ 機能が異なるのでNG |
| 一時代替としてのGUID | ⭕ テスト時のみ（ただしRocksDBは初期化） |

---

必要であれば `CommonSection` の完全定義や設定ファイル（`appsettings.json`）の例も用意できますよ。

（送信日時：[[REDACTED_PHONE]]:33 JST）

### user — 2025-08-03T00:31:16.656000+00:00

ApplicationId がないのでClientIdでいい？

### assistant — 2025-08-03T00:32:46.580223+00:00

了解です。`AutoOffsetReset = Streamiz.Kafka.Net.SerDes.AutoOffsetReset.Earliest` の箇所でエラーになるとのこと、原因と対処法を以下に整理します。

---

## ❌ エラー原因

`Streamiz.Kafka.Net.SerDes.AutoOffsetReset` は実際には存在しません。  
正しくは：

> ✅ **`Streamiz.Kafka.Net.AutoOffsetReset`（namespace直下）**

---

## ✅ 修正コード

```csharp
using Streamiz.Kafka.Net; // これが必要

var config = new StreamConfig<StringSerDes, StringSerDes>
{
    ApplicationId = "ksql-dsl-app",
    BootstrapServers = "...",
    SchemaRegistryUrl = "...",
    AutoOffsetReset = AutoOffsetReset.Earliest // ← namespace 直下の列挙型
};
```

**`SerDes.` の中ではなく、`Streamiz.Kafka.Net` の中に `AutoOffsetReset` 列挙型があります。**

---

## 🔁 補足：選択可能な値

| 値 | 説明 |
|----|------|
| `AutoOffsetReset.Earliest` | トピックの先頭から読み込む（Kafka Streams開発時の初期化に最適） |
| `AutoOffsetReset.Latest` | 直近のオフセットから読み込む（通常のアプリ運用時向け） |

---

## ✅ 最終確認

もしまだ `using` が不足していれば、追加してください：

```csharp
using Streamiz.Kafka.Net;
```

これで `AutoOffsetReset` が補完候補に出てくるはずです。

---

不明点があればエラーメッセージごと見せてくださいね、深掘りもできます。

（送信日時：[[REDACTED_PHONE]]:40 JST）

### user — 2025-08-03T00:32:58.508000+00:00

AutoOffsetReset = Streamiz.Kafka.Net.SerDes.AutoOffsetReset.Earliest,
でエラー

### assistant — 2025-08-03T00:34:23.970502+00:00

良い質問です。`StateDir`（Kafka Streams の RocksDB ストア保存ディレクトリ）に**何を指定するかは重要**です。以下に推奨とその理由を整理します。

---

## ✅ 推奨デフォルト値（開発・運用別）

### 🧪 開発・ローカル実行時

```csharp
public string StateDir { get; set; } = "/tmp/kafka-streams";
```

- **理由**：
  - Kafka Streams の [公式デフォルト](https://kafka.apache.org/documentation/streams/developer-guide/config-streams.html#state-dir) は `/tmp/kafka-streams`
  - OS再起動で消える可能性あり（テストには適している）
  - 開発環境やCI環境での一時使用にちょうど良い

---

### 🏭 本番・永続利用を想定する場合

```csharp
public string StateDir { get; set; } = "/var/lib/kafka-streams";
```

- **理由**：
  - 永続化されるストレージに置くことで、**再起動後もRocksDBが維持**される
  - KubernetesやDockerでも、ボリュームマウント先に設定しておくと再利用が可能

---

### 💡 .NET クロスプラットフォーム向けなら（OS対応）

```csharp
public string StateDir { get; set; } =
    Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.LocalApplicationData), "kafka-streams");
```

- Windows: `C:\Users\<user>\AppData\Local\kafka-streams`
- Linux/macOS: `~/.local/share/kafka-streams`（≒XDG準拠）

---

## 🔐 注意事項

| 項目 | 理由 |
|------|------|
| StateDir は **ApplicationId と組み合わせて使われる** | → 同じAppIdでディレクトリが共有される |
| ディレクトリが削除されると **状態がリセット**される | 再計算や再取得が発生する可能性あり |
| 複数インスタンスで同一ディレクトリを共有すると**競合が発生** | Docker環境では mount per container を推奨 |

---

## ✅ 最終推奨コード（開発用）

```csharp
public string StateDir { get; set; } = Path.Combine(Path.GetTempPath(), "kafka-streams");
```

- OSに依存せず一時的に使用可能
- 開発時に気軽に試せる

---

必要に応じて `KsqlContext` 側でログ出力も加えて、`StateDir` の場所を明示できますよ。

（送信日時：[[REDACTED_PHONE]]:46 JST）

### user — 2025-08-03T00:34:37.445000+00:00

public string StateDir { get; set; } = "";　デフォルトの推奨は？

### assistant — 2025-08-03T00:36:15.006264+00:00

了解しました。`_kafkaStream.Start();` の箇所でエラーが出ているとのこと。  
よくある原因とその対処法を順に整理し、必要に応じて修正案を提示します。

---

## 🧯 まず確認したいこと（よくある原因）

### ✅ 1. `StreamBuilder` に topology が未定義（build が空）

- **症状**：`Start()` 実行時に「Topology is empty」などの例外が出る
- **原因**：`StreamBuilder builder = new(); builder.Build()` に対して stream/table の構成がない
- **対策**：最低でも1つの topology（例：`builder.Stream<string, string>("dummy-topic")`）を追加する

---

### ✅ 2. `StateDir` が無効なパス、またはアクセス権限がない

- **症状**：`System.IO.IOException` などのファイル/フォルダ関連例外
- **原因**：パスが `""` または書き込み禁止、既存プロセスがロック中など
- **対策**：
  - 絶対パスに修正する（`Path.Combine(Path.GetTempPath(), "kafka-streams")`など）
  - ディレクトリ存在確認＆明示的作成（`Directory.CreateDirectory(...)`）

---

### ✅ 3. `KafkaStream` の再利用禁止（既にStart()した後）

- **症状**：`Start()` は一度しか呼べない → 複数回 Start すると例外
- **原因**：2回目以降の Start 呼び出し
- **対策**：`_kafkaStream?.State == State.NOT_RUNNING` などの条件で Start する

---

## 🛠 修正例（簡易 topology あり・StateDir 対応）

```csharp
var builder = new StreamBuilder();

// 最小構成のダミー stream（storeを使うには何かしら定義が必要）
builder.Stream<string, string>("ksql-dummy-topic");

var config = new StreamConfig<StringSerDes, StringSerDes>
{
    ApplicationId = "ksql-dsl-app",
    BootstrapServers = "localhost:9092",
    SchemaRegistryUrl = "http://localhost:8081",
    AutoOffsetReset = AutoOffsetReset.Earliest,
    StateDir = Path.Combine(Path.GetTempPath(), "kafka-streams")
};

Directory.CreateDirectory(config.StateDir); // 明示的に作成

_kafkaStream = new KafkaStream(builder.Build(), config);
_kafkaStream.Start(); // ←ここでエラーが出ないことを確認
```

---

## 📋 お願い：エラーメッセージが分かればさらに精密対応可能

- スタックトレース or メッセージ（`InvalidOperationException` など）が分かればピンポイントに調整できます。
- 必要に応じて `KafkaStream.SetExceptionHandler(...)` でログに詳細を吐くことも可能。

---

まずは **topology構成の有無 + StateDirの指定** を重点的に確認してみてください。  
状況に応じて、こちらから修正コードのテンプレートも出せます！

（送信日時：[[REDACTED_PHONE]]:54 JST）

### user — 2025-08-03T00:36:28.340000+00:00

_kafkaStream.Start(); // RocksDBがstoreを取得可能になる　
ここでエラー

### assistant — 2025-08-03T00:36:43.086693+00:00

その通りです、良い気づきです！  
`KafkaStream.Start()` は **今では非推奨（obsolete）** になっており、**推奨されるのは `StartAsync()`** の方です。

---

## ✅ 正しい起動方法（非同期）

```csharp
await _kafkaStream.StartAsync();
```

- 非同期で起動され、内部的にバックグラウンドスレッドでstream処理が開始されます。
- 終了制御（キャンセルやStop）も合わせて非同期で行うと安全です。

---

## 💡 使用上の注意点

| メソッド | 状態 | 備考 |
|----------|------|------|
| `Start()` | ❌ 非推奨（Obsolete） | 今後のバージョンで削除される可能性あり |
| `StartAsync()` | ✅ 推奨 | async/awaitで制御可能、例外も非同期でキャッチ可 |

---

## 🛠 対応コード例

```csharp
_kafkaStream = new KafkaStream(builder.Build(), config);

// 非同期で開始
await _kafkaStream.StartAsync();

// ※ KsqlContext の非同期初期化メソッド内で await 可能にする
```

`KsqlContext` 側で `async Task InitializeAsync()` を作って、そこから呼ぶのが理想です。

---

## 🎯 まとめ

- ✅ `StartAsync()` を使う
- ✅ `await` を付けて非同期制御
- 🔁 `StopAsync()` で後続のクリーンアップも合わせて非同期処理可能にするのがベスト

---

必要であれば、`KsqlContext` に `await` を組み込んだ非同期初期化パターンも用意します！

（送信日時：[[REDACTED_PHONE]]:01 JST）

### user — 2025-08-03T00:36:55.583000+00:00

Startが非推奨

### assistant — 2025-08-03T00:40:05.922289+00:00

了解です、`_dslOptions.CacheOptions` が見つからないということは、`KsqlDslOptions` に `CacheOptions` プロパティがまだ定義されていない状態ですね。

---

## ✅ 対応方法：`KsqlDslOptions` に `CacheOptions` を追加

### 🔧 追加コード

`KsqlDslOptions` クラスに以下を追加してください：

```csharp
public TableCacheOptions CacheOptions { get; init; } = new();
```

これにより、`appsettings.json` 等でキャッシュの有効化設定ができるようになります。

---

## 🔄 関連クラスの定義（もし未定義なら）

### `TableCacheOptions`

```csharp
public class TableCacheOptions
{
    public List<TableCacheEntry> Entries { get; init; } = new();
}
```

### `TableCacheEntry`

```csharp
public class TableCacheEntry
{
    public string Entity { get; init; } = "";
    public bool EnableCache { get; init; } = true;
    public string? StoreName { get; init; } // RocksDBのstate store名（未指定ならトピック名）
}
```

---

## ✅ 設定ファイル（例）

```json
"KsqlDsl": {
  "CacheOptions": {
    "Entries": [
      {
        "Entity": "OrderTable",
        "EnableCache": true,
        "StoreName": "order-store"
      },
      {
        "Entity": "UserTable",
        "EnableCache": true
      }
    ]
  }
}
```

---

## 🧠 天城の補足

- `EnableCache = false` にすれば、該当エンティティは RocksDB を使わずにスキップされます
- `StoreName` を指定しない場合、トピック名が RocksDB の store 名として使われます

---

この変更により、`_dslOptions.CacheOptions` が正しく解決され、`_cacheRegistry?.InitializeCaches(...)` が正常に呼び出せるようになります。

必要ならこの一連を構成済みの `settings.sample.json` にして出力もできますよ。

（送信日時：[[REDACTED_PHONE]]:08 JST）

### user — 2025-08-03T00:40:19.352000+00:00

_cacheRegistry?.InitializeCaches(_entityModels.Values, _dslOptions.CacheOptions, _schemaRegistryClient.Value, _loggerFactory, _kafkaStream);
のCacheOptionsがない

### assistant — 2025-08-03T00:40:47.388788+00:00

了解しました。「`CacheOptions` は `appsettings.json` からではなく、**コード内で動的に構築する**」という前提ですね。  
つまり、`EntityModel` の情報を使って、必要な分だけ `TableCacheEntry` を生成する方式になります。

---

## ✅ 動的に `TableCacheOptions` を構築するコード例

```csharp
private TableCacheOptions BuildDynamicCacheOptions()
{
    var options = new TableCacheOptions();

    foreach (var model in _entityModels.Values)
    {
        // Table かつキャッシュ対象であれば追加
        if (model.StreamTableType == StreamTableType.Table && model.EnableCache)
        {
            options.Entries.Add(new TableCacheEntry
            {
                Entity = model.EntityType.Name,
                EnableCache = true,
                StoreName = model.GetTopicName() // または任意のロジック
            });
        }
    }

    return options;
}
```

---

## 🛠 呼び出し元での使用

`InitializeCaches` の呼び出しを以下のように変更：

```csharp
var dynamicCacheOptions = BuildDynamicCacheOptions();

_cacheRegistry?.InitializeCaches(
    _entityModels.Values,
    dynamicCacheOptions,
    _schemaRegistryClient.Value,
    _loggerFactory,
    _kafkaStream);
```

---

## 🧠 補足：キャッシュ対象の自動判断ロジック

必要に応じて、以下のような条件も加えられます：

- Entity名に `*Cache` が含まれていたら対象
- 属性 `[EnableTableCache]` を付けているもののみ対象
- クラス単位で明示的に定義されたもののみ

---

## ✅ 結論

- ✔️ `KsqlDslOptions` には `CacheOptions` を定義しない（またはnullのままでOK）
- ✔️ `EntityModel` に基づいて動的に `TableCacheOptions` を構築
- ✔️ `EnableCache` フラグや `StreamTableType == Table` の条件で絞り込み

---

ご希望であれば、この処理を `KsqlContext` に `InitCacheRegistry()` メソッドとしてまとめることもできます！

（送信日時：[[REDACTED_PHONE]]:14 JST）

### user — 2025-08-03T00:40:58.982000+00:00

これは動的に作らないといけない
