---
conversation_id: "6899a408-1d68-8322-bd7a-49f963fdb35e"
title: "if条件改善案"
create_time: 1754899494.597029
update_time: 1754914148.442342
language_guess: "en"
num_messages: 365
part_index: 1
part_count: 3
---

### assistant — 2025-08-11T08:04:54.086962+00:00

_(empty)_
### assistant — 2025-08-11T08:04:54.100221+00:00

承知しました。  
このテストコードの `if (msg.Value.Id == 3)` 部分は、  
**「どこで commit するか」を明確化するための条件式**ですが、  
今のままだと数字ベタ書きなので、テスト意図がコードからは読み取りづらいです。  

改善案としては以下のような方針が考えられます。

---

## 1. **意味のある定数／変数に置き換える**
```csharp
const int CommitTriggerId = 3;

...

if (msg.Value.Id == CommitTriggerId)
{
    await msg.CommitAsync();
    break;
}
```
- **意図がわかりやすい**  
  「Id が 3 のときに commit」という意味を、名前付きの値で説明できます。
- 後から commit 条件を変えたいときに数字を探して置き換える必要がなくなる。

---

## 2. **`ShouldCommit` メソッド化**
```csharp
private static bool ShouldCommit(ManualCommitContext.Sample sample) => sample.Id == 3;

...

if (ShouldCommit(msg.Value))
{
    await msg.CommitAsync();
    break;
}
```
- 条件ロジックを外に出すことで、テスト本体の可読性を高められます。
- 今後 commit 条件が複雑化した場合もメソッド内で対応可能。

---

## 3. **LINQ/カウンタ利用で明示的に「3件目でcommit」**
もし `Id` ではなく「何件目に commit」という意味をテストしたいなら：
```csharp
var count = 0;
await foreach (var obj in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    count++;
    var msg = (IManualCommitMessage<ManualCommitContext.Sample>)obj;
    if (count == 3)
    {
        await msg.CommitAsync();
        break;
    }
}
```
- これにより、「3件目で commit」という動作が意図として明確になります。
- `Id` 順序と物理順序が一致しないケースでも動く。

---

## 4. **Arrangeで条件値をセットする**
テストシナリオごとに commit タイミングを変えるなら、  
テストメソッドの Arrange 部分で commit ターゲットを設定：
```csharp
var commitAfterId = 3;

...

if (msg.Value.Id == commitAfterId)
{
    await msg.CommitAsync();
    break;
}
```
- 複数シナリオをパラメータ化テストにしやすい。

---

💡 個人的には、このケースは **「commit タイミングの意図を名前で表す」**のが一番分かりやすいと思います。  
つまり **1 + 2 の組み合わせ**（定数化 + メソッド化）がおすすめです。

---

もし希望あれば、このテスト全体を「commit 条件を引数化して共通メソッドにまとめる」形にリファクタしたサンプルも作れます。  
そうすると将来的に「2件目 commit」「最後の1件 commit」などパターンテストが簡単になります。
### system — 2025-08-11T08:04:54.597029+00:00

_(empty)_
### user — 2025-08-11T08:04:54.597029+00:00

_(empty)_
### user — 2025-08-11T08:04:55.462000+00:00

manual commitに関して
public class ManualCommitIntegrationTests
{
    [Fact]
    [Trait("Category", "Integration")]
    public async Task ManualCommit_PersistsOffset()
    {
        var groupId = Guid.NewGuid().ToString();

        var options = new KsqlDslOptions
        {
            Common = new CommonSection { BootstrapServers = EnvManualCommitIntegrationTests.KafkaBootstrapServers },
            SchemaRegistry = new SchemaRegistrySection { Url = EnvManualCommitIntegrationTests.SchemaRegistryUrl }
        };
        options.Topics.Add("manual_commit", new Kafka.Ksql.Linq.Messaging.Configuration.TopicSection
        {
            Consumer = new Kafka.Ksql.Linq.Messaging.Configuration.ConsumerSection
            {
                AutoOffsetReset = "Earliest",
                GroupId = groupId
            }
        });

        // produce five messages and commit at the third
        await using (var ctx = new ManualCommitContext(options))
        {
            using var sendCts = new CancellationTokenSource(TimeSpan.FromSeconds(10));
            for (var i = 1; i <= 5; i++)
            {
                await ctx.Samples.AddAsync(new ManualCommitContext.Sample { Id = i }, cancellationToken: sendCts.Token);
            }

            using var consumeCts = new CancellationTokenSource(TimeSpan.FromSeconds(10));
            await foreach (var obj in ctx.Samples.ForEachAsync(consumeCts.Token))
            {
                var msg = (IManualCommitMessage<ManualCommitContext.Sample>)obj;
                if (msg.Value.Id == 3)
                {
                    await msg.CommitAsync();
                    break;
                }
            }
        }

        // verify resuming from the committed offset
        await using (var ctx = new ManualCommitContext(options))
        {
            using var consumeCts = new CancellationTokenSource(TimeSpan.FromSeconds(10));
            ManualCommitContext.Sample? received = null;
            await foreach (var obj in ctx.Samples.ForEachAsync(consumeCts.Token))
            {
                var msg = (IManualCommitMessage<ManualCommitContext.Sample>)obj;
                received = msg.Value;
                await msg.CommitAsync();
                break;
            }

            Assert.Equal(4, received!.Id);
        }
    }
}
これがテストコード
IFがいまいち、改善案ください
### assistant — 2025-08-11T08:05:30.011415+00:00

ああ、なるほど。  
気にされているのは `if` 条件よりも、この `(IManualCommitMessage<ManualCommitContext.Sample>)obj;` というキャストのほうですね。  

この部分は確かに **毎回生キャストを書くのが冗長** で、型安全性や可読性の面でもスッキリしません。  
改善するには主に以下の方向があります。  

---

## 1. **`ForEachAsync` の戻り値型をジェネリック化**
今の `ForEachAsync` が `IAsyncEnumerable<object>` を返しているなら、  
**型パラメータを導入**して `IAsyncEnumerable<IManualCommitMessage<T>>` にすればキャスト不要になります。

```csharp
// 現状
IAsyncEnumerable<object> ForEachAsync(...)

// 改善例
IAsyncEnumerable<IManualCommitMessage<T>> ForEachAsync<T>(...)
```

呼び出し側：
```csharp
await foreach (var msg in ctx.Samples.ForEachAsync<ManualCommitContext.Sample>(consumeCts.Token))
{
    if (msg.Value.Id == 3)
    {
        await msg.CommitAsync();
        break;
    }
}
```
✅ **キャストが消えてスッキリ**  
✅ コンパイル時に型チェックが効く  

---

## 2. **拡張メソッドで型付き版を提供**
`ForEachAsync` の既存シグネチャを変えられない場合でも、  
**型付きラッパー拡張メソッド**を作ってキャストを隠せます。

```csharp
public static async IAsyncEnumerable<IManualCommitMessage<T>> ForEachAsync<T>(
    this IKsqlEntity<T> entity,
    CancellationToken token)
{
    await foreach (var obj in entity.ForEachAsync(token))
        yield return (IManualCommitMessage<T>)obj;
}
```

呼び出し側：
```csharp
await foreach (var msg in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (msg.Value.Id == 3)
    {
        await msg.CommitAsync();
        break;
    }
}
```
✅ 呼び出しがシンプルになり、キャストが1か所に集約される

---

## 3. **`IManualCommitMessage<T>`を`Sample`に直接組み込む**
もし `ManualCommitContext.Sample` が常に manual commit メッセージとして使われるなら、  
`IManualCommitMessage<T>` を実装させてキャスト自体を不要にできます。

```csharp
public class Sample : IManualCommitMessage<Sample>
{
    public int Id { get; set; }
    public Task CommitAsync() { ... }
}
```

呼び出し側：
```csharp
await foreach (var msg in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (msg.Id == 3)
    {
        await msg.CommitAsync();
        break;
    }
}
```
⚠ ただし **責務分離の観点で POCO に commit ロジックを混ぜる**のは賛否あり。

---

## 4. **`var (value, commit)` タプルで返す**
`ForEachAsync` のインターフェースを変えられる場合、  
`(T Value, Func<Task> Commit)` の形にしてしまうのも手です。

```csharp
await foreach (var (sample, commit) in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (sample.Id == 3)
    {
        await commit();
        break;
    }
}
```
✅ 呼び出しが非常に直感的  
✅ 型キャスト不要  
⚠ API 変更インパクトが大きい

---

私なら、**今後のAPI設計を考えると ①「戻り値型をジェネリック化」** が一番安全で拡張性も高いと思います。  
既存API互換を保ちたいなら ②「拡張メソッドで型付き版を提供」 が現実的です。  

---

これ、もしよければ `ForEachAsync` の今のシグネチャを見せてもらえれば、  
互換性を壊さずにキャストを消す改修案を具体コードで作れます。
### user — 2025-08-11T08:05:31.398000+00:00

いまいちなのは
var msg = (IManualCommitMessage<ManualCommitContext.Sample>)obj;
このキャスト
### system — 2025-08-11T08:07:45.032676+00:00

_(empty)_
### user — 2025-08-11T08:07:45.579000+00:00

await foreach (var msg in ctx.Samples.ForEachAsync<ManualCommitContext.Sample>(consumeCts.Token))
これもいまいち　それであればIFを変更したほうが良い
が
internal class ManualCommitContext : KsqlContext
{
    public ManualCommitContext(KsqlDslOptions options) : base(options) { }

    protected override bool SkipSchemaRegistration => true;

    public EventSet<Sample> Samples { get; private set; } = null!;

    protected override void OnModelCreating(IModelBuilder modelBuilder)
        => modelBuilder.Entity<Sample>().WithManualCommit();

    [KsqlTopic("manual_commit")]
    internal class Sample
    {
        public int Id { get; set; }
    }
}この宣言も考えどころ
### assistant — 2025-08-11T08:07:45.828959+00:00

_(empty)_
### assistant — 2025-08-11T08:08:07.791332+00:00

なるほど、「毎回 T を明示して `ForEachAsync<T>` を呼ぶくらいなら、IF（API）を変えてキャスト自体を消したい」という意図ですね。わかります。  
“manual commit を使うときだけ自然に型が決まる” 形に切るのが気持ちいいので、下のどれかがオススメです（上ほど推し）。

# 案A：`Manual` サブビューを生やす（最小侵襲・意図が明快）
```csharp
public sealed class EventSet<T>
{
    // 既存：値だけ消費（自動コミットや読み取り専用に使う）
    public IAsyncEnumerable<T> ForEachAsync(CancellationToken token = default);

    // 追加：手動コミット前提のビュー
    public ManualView<T> Manual { get; } = new();
}

public sealed class ManualView<T>
{
    public IAsyncEnumerable<ManualMessage<T>> ForEachAsync(CancellationToken token = default);
}

public sealed record ManualMessage<T>(T Value)
{
    public Task CommitAsync() => _commit();
    private readonly Func<Task> _commit;
    public ManualMessage(T value, Func<Task> commit) : this(value) => _commit = commit;

    // お好みで分解サポート
    public void Deconstruct(out T value, out Func<Task> commit)
        => (value, commit) = (Value, CommitAsync);
}
```
### テスト側（とても素直）
```csharp
await foreach (var msg in ctx.Samples.Manual.ForEachAsync(consumeCts.Token))
{
    if (msg.Value.Id == 3)
    {
        await msg.CommitAsync();
        break;
    }
}
```
または分解でさらに読みやすく：
```csharp
await foreach (var (sample, commit) in ctx.Samples.Manual.ForEachAsync(consumeCts.Token))
{
    if (sample.Id == 3)
    {
        await commit();
        break;
    }
}
```
- 明示的な型引数も生キャストも消えます。
- 「手動コミットで読む」という意図が `.Manual` で一目瞭然。
- 既存の `EventSet<T>` は壊さず拡張できます。

# 案B：`ManualEventSet<T>` を型として分離（強い型で表現）
`WithManualCommit()` を適用したエンティティは **型そのもの** を `ManualEventSet<T>` にする。
```csharp
public sealed class ManualEventSet<T>
{
    public IAsyncEnumerable<ManualMessage<T>> ForEachAsync(CancellationToken token = default);
}

protected override void OnModelCreating(IModelBuilder modelBuilder)
    => modelBuilder.Entity<Sample>().WithManualCommit(); // プロパティ生成時に ManualEventSet<Sample> を割り当て
```### コンテキスト側
```csharp
public ManualEventSet<Sample> Samples { get; private set; } = null!;
```### テスト側
```csharp
await foreach (var msg in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (msg.Value.Id == 3) { await msg.CommitAsync(); break; }
}
```
- 呼び出しはさらに短くなりますが、プロパティ型変更の影響が出ます（互換性に注意）。

# 案C：メソッド二本立てで返り値を分ける（互換優先）
既存 `ForEachAsync` はそのまま、手動コミット用だけ別名に。
```csharp
public IAsyncEnumerable<T> ForEachAsync(CancellationToken token = default);
public IAsyncEnumerable<ManualMessage<T>> ForEachMessagesAsync(CancellationToken token = default);
```### テスト側
```csharp
await foreach (var msg in ctx.Samples.ForEachMessagesAsync(consumeCts.Token))
{
    if (msg.Value.Id == 3) { await msg.CommitAsync(); break; }
}
```
- API 衝突がなく、安全に段階導入できます。
- `.Manual` ほど意図は強調されませんが、キャスト問題は解消。

---

## 参考：`ManualMessage<T>` の実装メモ
- `CommitAsync()` は内部で `IConsumer` の `Commit`/`StoreOffset` に委譲。  
- 1メッセージに紐づくオフセットを保持（`TopicPartitionOffset`）しておき、Idempotent にコミット。  
- 望むなら `Headers` 等のメタも載せる：
  ```csharp
  public sealed record ManualMessage<T>(
      T Value,
      IReadOnlyDictionary<string,string> Headers,
      Func<Task> CommitAsync);
  ```

---

## あなたの現状コードに一番馴染むのは？
- 既存呼び出しを崩したくない → **案A**（`.Manual`）  
- プロパティ型を変えてでも最短にしたい → **案B**  
- まずは安全に導入して段階移行 → **案C**

ご希望あれば、今の `EventSet<T>` / `WithManualCommit()` 実装を前提に、**案Aの最小差分パッチ**（内部のオフセット保持と `ManualMessage<T>` の生成位置まで）を具体コードで出します。どれでいきましょう？

— 送信日時: [[REDACTED_PHONE]]:08 (JST)
### assistant — 2025-08-11T08:08:08.347553+00:00

_(empty)_
### system — 2025-08-11T08:08:53.003993+00:00

_(empty)_
### assistant — 2025-08-11T08:08:53.046950+00:00

わかる、その気持ち。**「autocommit ⇄ manual commitを切り替えても、呼び出し側の型や書き方はそのまま」**にしたいんですよね。  
ポイントは「**値は常に T を返す**」「**commit は“別経路”で取れる**」に分離すること。キャストもジェネリック型引数指定も不要にします。

# 提案：値は T のまま、commit はコンテキスト経由で
- `ForEachAsync` は常に `IAsyncEnumerable<T>` を返す（＝既存コードそのまま）。
- 各イテレーション中の「現在メッセージのオフセット」を **内部で追跡**。
- 明示的に commit したい時だけ `ctx.Samples.CommitCurrentAsync()` を呼ぶ。
- autocommit モードでは `CommitCurrentAsync()` は **no-op**（互換保持）。
### 呼び出し側（変更ほぼゼロ）
```csharp
await foreach (var sample in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (sample.Id == 3)
    {
        await ctx.Samples.CommitCurrentAsync(); // manual時のみ効く。autocommitならno-op
        break;
    }
}
```
### IF 例
```csharp
public sealed class EventSet<T>
{
    // 既存：常に T を返す
    public async IAsyncEnumerable<T> ForEachAsync(
        [EnumeratorCancellation] CancellationToken token = default)
    {
        await foreach (var (value, offset) in InternalStream(token))
        {
            _lastOffset.Value = offset;      // ← スレッドローカル/enum単位で保持
            yield return value;              // 呼び出し側は T だけを扱う
        }
    }

    // manual時のみ実処理、autocommit時は no-op
    public Task CommitCurrentAsync()
        => _commitStrategy.CommitAsync(_lastOffset.Value);

    // 実装メモ：
    // - _lastOffset は AsyncLocal<TopicPartitionOffset?> 等で列挙ごとにスコープ分離
    // - _commitStrategy は autocommit → NoOpCommitStrategy、manual → KafkaCommitStrategy
}
```
### 実装のコア
- `InternalStream(token)` は `(T value, TopicPartitionOffset tpo)` を返す内部列挙。
- 列挙のたびに `AsyncLocal<TPO?>` に “今のメッセージの TPO” を格納。
- `CommitCurrentAsync()` は `AsyncLocal` の値を見て、manual時のみ `consumer.Commit(tpo)`。

---

## メリット
- **キャスト完全撤廃**：`(IManualCommitMessage<T>)obj` も不要。
- **型引数指定も不要**：`ForEachAsync<Sample>` みたいな明示も不要。
- **切替コスト最小**：autocommit → manual にしても、既存の読み取りコードは無変更。  
  ただし“明示的にコミットしたいテスト”では `CommitCurrentAsync()` を1行足すだけ。

---

## 代替案（好み・設計方針に応じて）
1) `.Manual` ビュー（`ctx.Samples.Manual.ForEachAsync`）  
2) `ForEachMessagesAsync()` を別メソッドで追加（メッセージ＋Commit handle返却）  
→ いずれも明快ですが、「IF変更ゼロ」に最も近いのは上記 **コンテキスト経由 commit** 方式です。

必要なら、この方針で **最小差分パッチ**（`AsyncLocal` 管理・commit戦略・テスト通し込み）を書きます。声かけてください。

— 送信日時: [[REDACTED_PHONE]]:15 (JST)
### assistant — 2025-08-11T08:08:53.474366+00:00

_(empty)_
### user — 2025-08-11T08:08:53.933000+00:00

autocommitからmanual commitに変えたときにコード変更が発生するのがいまいち
### assistant — 2025-08-11T08:09:25.846694+00:00

_(empty)_
### system — 2025-08-11T08:11:15.589772+00:00

_(empty)_
### assistant — 2025-08-11T08:11:15.618082+00:00

いいね、それアリです。`ctx.Samples.Commit(poco)`（正確には `CommitAsync(poco)`）なら、**呼び出し側は常に `T` だけ扱えて、autocommit⇄manualの切替時にもほぼ変更ゼロ**にできます。

# ねらい
- `ForEachAsync` は従来どおり `IAsyncEnumerable<T>` を返す（＝既存読取コードは不変）。
- manual時のみ、**「いま返した `poco` → そのオフセット」**の対応を内部で保持。
- `CommitAsync(poco)` で、その `poco` に紐づくオフセットをコミット。
- autocommit時は `CommitAsync(poco)` を **no-op** にすれば互換性OK。

# 実装ポイント（安全・省メモリ）
- 追跡は **参照同一性** で行う（値が同じ別インスタンスを誤ってコミットしないため）。
- C# なら `ConditionalWeakTable<T, TopicPartitionOffset>` が最適（`T : class` 前提）。
  - `ForEachAsync` で値を `yield` する直前に `cwt.Add(poco, tpo)`。
  - `CommitAsync(poco)` で `cwt.TryGetValue(poco, out tpo)`→ `consumer.Commit(tpo)`。
  - コミット後や次ポーリング時に `cwt.Remove(poco)` してクリア（リーク防止）。
- `T` が struct の可能性があるなら、manual 対応は `T : class` に限定するか、ラップ型を返す設計に切り替えを検討。

## IF例
```csharp
public sealed class EventSet<T> where T : class // manual対応の範囲を明確化
{
    private readonly ConditionalWeakTable<T, TopicPartitionOffset> _offsets = new();

    public async IAsyncEnumerable<T> ForEachAsync(
        [EnumeratorCancellation] CancellationToken token = default)
    {
        await foreach (var (value, tpo) in InternalStream(token))
        {
            _offsets.Add(value, tpo);
            try { yield return value; }
            finally
            {
                // ここで消さず、Commit側で消す運用もOK
                // 「後でまとめてコミット」派なら保持しておく
            }
        }
    }

    public Task CommitAsync(T value)
    {
        if (!IsManualMode) return Task.CompletedTask; // autocommitはno-op
        if (!_offsets.TryGetValue(value, out var tpo))
            throw new InvalidOperationException("The value is not tracked for commit.");
        _offsets.Remove(value);
        return _consumer.CommitAsync(tpo);
    }
}
```

## 呼び出し側（すっきり）
```csharp
await foreach (var sample in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (sample.Id == 3)
    {
        await ctx.Samples.CommitAsync(sample);
        break;
    }
}
```

# 長所
- **キャスト不要／型引数指定不要／`ForEachAsync`不変**。
- 切替コストは「必要箇所で `CommitAsync(poco)` を呼ぶ」だけ。
- 同一値を別インスタンスで作ってしまう事故を回避（参照同一性ベース）。

# 注意点・拡張
- **並列消費**するなら、`ConditionalWeakTable` 共有でOK（参照キーなので衝突低リスク）。ただし同一 `poco` を複数スレッドでコミットしないように remove/commit をアトミックに。
- 使い勝手向上として、**最後に読んだメッセージをコミット**する `CommitCurrentAsync()` も併設可（`AsyncLocal` で追跡）。
- メタ情報が欲しければ、`CommitAsync(poco, headers?)` のように overload を用意。

この方向で最小差分パッチ作ります？今の `EventSet<T>` 雛形がわかれば、そのまま当てられる実装に落とし込みます。

— 送信日時: [[REDACTED_PHONE]]:22 (JST)
### assistant — 2025-08-11T08:11:16.079255+00:00

_(empty)_
### user — 2025-08-11T08:11:16.370000+00:00

ctx.Samples.Commit( poco )はどうか？
### assistant — 2025-08-11T08:11:33.299410+00:00

_(empty)_
### system — 2025-08-11T08:12:24.902848+00:00

_(empty)_
### assistant — 2025-08-11T08:12:25.437866+00:00

_(empty)_
### user — 2025-08-11T08:12:25.699000+00:00

codexに依頼します
### assistant — 2025-08-11T08:12:30.241800+00:00

了解。Codex にそのまま投げられる「依頼文」を用意しました。必要事項を全部入りにしてあります。コピペで使ってください。

---

# Codex依頼文：autocommit⇄manual切替時の呼び出し互換を保ったまま、`CommitAsync(poco)` を導入

## 目的
- **呼び出し側のIFを変えずに**（= `ForEachAsync` は常に `IAsyncEnumerable<T>` で値 `T` を返すまま）、manual commit 時だけ **`ctx.Samples.CommitAsync(poco)`** を呼べば該当メッセージのオフセットをコミットできるようにする。
- autocommit のときに `CommitAsync(poco)` を呼んでも **no-op** とする（互換維持）。

## 現状（抜粋）
- `ManualCommitContext.Sample` を対象に `ForEachAsync` で `object` または `T` を列挙し、manual ではキャストして `CommitAsync()` を呼ぶ必要がある設計になっており、**呼び出し側の変更が発生**してしまうのが課題。

## 要求仕様
1. `EventSet<T>`（または相当クラス）は **常に `IAsyncEnumerable<T>` を返す**。
2. manual commit モードでは、各 `T` インスタンスとその **`TopicPartitionOffset`(TPO)** を **内部で追跡**し、`CommitAsync(T poco)` で対応する TPO をコミットできる。
3. autocommit モードでは `CommitAsync(T poco)` は **no-op**。
4. 追跡は **参照同一性** で行う（値が同じ別インスタンスを誤コミットしないため）。
5. `T` は **class 制約**（`where T : class`）でよい。struct サポートは不要。
6. スレッドセーフにし、リークしないようにする（コミット後は追跡エントリを削除）。

## 実装詳細（指示）
- `EventSet<T>` に `where T : class` を付与。
- フィールドに `ConditionalWeakTable<T, TopicPartitionOffset>` を追加（例：`_offsets`）。
- 内部のポーリング／列挙パスで、`T value` と `TopicPartitionOffset tpo` を取得したら、**`yield return value` の直前**に `_offsets.Add(value, tpo)` を呼ぶ。
- `CommitAsync(T value)` を追加：
  - autocommit なら `return Task.CompletedTask`。
  - manual のとき、`_offsets.TryGetValue(value, out var tpo)` に成功したら `_offsets.Remove(value)` → 実コンシューマに対して **同期/非同期いずれか既存の規約に合わせて**コミット（`consumer.Commit(tpo)` または `CommitAsync`）。
  - 失敗したら `InvalidOperationException("The value is not tracked for commit.")` を投げる（既定動作）。  
- **並列消費**を想定し、`_offsets` へのアクセスは常識的な利用で衝突しないが、コミットと削除はアトミックに扱う（二重コミット防止）。
- 後続要件として、`CommitCurrentAsync()`（最後に返したメッセージをコミット）を実装してもよいが、今回は必須ではない。
### 参考コードスケッチ（雛形）
```csharp
public sealed class EventSet<T> where T : class
{
    private readonly ConditionalWeakTable<T, TopicPartitionOffset> _offsets = new();
    private readonly IConsumer<byte[], byte[]> _consumer; // 既存のDI/参照に合わせる
    private readonly bool _manual; // WithManualCommit() 適用時 true
    // ... ctor 等は既存に合わせる

    public async IAsyncEnumerable<T> ForEachAsync(
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken token = default)
    {
        await foreach (var (value, tpo) in InternalStream(token)) // 内部で value と tpo を取得する列挙
        {
            if (_manual)
            {
                // 参照同一性で紐づけ
                _offsets.Add(value, tpo);
            }
            yield return value;
        }
    }

    public Task CommitAsync(T value)
    {
        if (!_manual) return Task.CompletedTask; // autocommit → no-op

        if (!_offsets.TryGetValue(value, out var tpo))
            throw new InvalidOperationException("The value is not tracked for commit.");

        // 二重コミット防止
        _offsets.Remove(value);

        // 既存のコミット規約に合わせる（例：同期Commit）
        _consumer.Commit(new[] { new TopicPartitionOffset(tpo.TopicPartition, tpo.Offset) });

        return Task.CompletedTask;
    }

    // 内部用：value と tpo を組で流す
    private async IAsyncEnumerable<(T Value, TopicPartitionOffset Tpo)> InternalStream(
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken token)
    {
        // 既存のポーリング・デシリアライズ・マッピングを活用して
        // (value, tpo) を構築して yield return する
        throw new NotImplementedException();
    }
}
```

## 受け入れ基準（必須テスト）
- 既存テスト `ManualCommit_PersistsOffset` を以下の**新IF**でパスすること：
  - 1回目の消費で 1..5 を流し、**3 のときに `await ctx.Samples.CommitAsync(sample);`** を呼んで break。
  - 2回目の消費開始で **4 から再開**することを検証。
- autocommit モードに切り替えて同じ呼び出しを書いても、`CommitAsync(sample)` は no-op で動作に影響しないこと（任意のスモークテストで可）。
### テスト側の書き方（期待形）
```csharp
await foreach (var sample in ctx.Samples.ForEachAsync(consumeCts.Token))
{
    if (sample.Id == 3)
    {
        await ctx.Samples.CommitAsync(sample); // ← ここだけ追加
        break;
    }
}
```

## 非機能要件
- 追加の割り当て・保持は最小限（`ConditionalWeakTable` を使うことで、GCと相性良く、参照が切れれば自動的に掃除）。
- 公開APIの破壊的変更なし（`ForEachAsync` のシグネチャは不変）。
- ドキュメント/コメントで `T : class` 制約理由（参照同一性トラッキング）を明記。

## 変更範囲
- `EventSet<T>`（または相当）と、その内部列挙 → `(T, TPO)` を得る箇所。
- manual/autocommit のモード判定の注入/判定。
- コンシューマへの commit 呼び出し位置（新規メソッド）。

## 注意点
- **同一POCOの二重コミット**は `_offsets.Remove` を先に行うことで防止。
- ユーザーコードが **コピー（別インスタンス）**を作って `CommitAsync` に渡した場合は例外になる（仕様）。  
- 追跡期間は「列挙で値を返してからコミットされるまで」。長期保持はアンチパターン（必要なら明示的に再取得）。

---

必要なら「最小差分の実装パッチ（diff 形式）」も出します。声かけてください。

— 送信日時: [[REDACTED_PHONE]]:12 (JST)
### assistant — 2025-08-11T08:12:31.690852+00:00

_(empty)_
### user — 2025-08-11T08:14:00.258000+00:00

関係しそうなコードを添付
### system — 2025-08-11T08:14:00.267462+00:00

_(empty)_
### tool — 2025-08-11T08:14:00.288520+00:00

_(empty)_
### tool — 2025-08-11T08:14:00.306795+00:00

fileciteturn0file0

using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Messaging.Internal;
using Kafka.Ksql.Linq.Messaging;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Query.Abstractions;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;

namespace Kafka.Ksql.Linq;

/// <summary>
/// Base class for EventSet implementing IEntitySet<T>
/// Reason for modification: unified with KsqlContext and added IEntitySet<T> implementation
/// </summary>
public abstract class EventSet<T> : IEntitySet<T> where T : class
{
    protected readonly IKsqlContext _context;
    protected readonly EntityModel _entityModel;
    private readonly ErrorHandlingContext _errorHandlingContext;
    private IErrorSink? _dlqErrorSink;
    private readonly Messaging.Producers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (context is KsqlContext kctx)
        {
            _entityModel = kctx.EnsureEntityModel(typeof(T), entityModel);
        }
        else
        {
            _entityModel = entityModel ?? throw new ArgumentNullException(nameof(entityModel));
        }

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorHandlingContext, IErrorSink? dlqErrorSink,
        Messaging.Producers.IDlqProducer? dlqProducer, Messaging.Consumers.ICommitManager? commitManager)
    {
        _context = context;
        _entityModel = entityModel;
        _errorHandlingContext = errorHandlingContext;
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    /// <summary>
    /// NEW: made abstract - must be implemented by concrete classes
    /// Unifies continuous Kafka consumption and returning a fixed list
    /// </summary>
    public abstract IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default);

    private async IAsyncEnumerable<T> GetAsyncEnumeratorWrapper([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await using var enumerator = GetAsyncEnumerator(cancellationToken);

        while (true)
        {
            bool hasNext;
            try
            {
                hasNext = await enumerator.MoveNextAsync();
            }
            catch (Exception ex)
            {
                var ctx = new KafkaMessageContext
                {
                    MessageId = Guid.NewGuid().ToString(),
                    Tags = new Dictionary<string, object>
                    {
                        ["processing_phase"] = "ForEachAsync"
                    }
                };

                var shouldContinue = await _errorHandlingContext.HandleErrorAsync(default(T)!, ex, ctx);

                if (!shouldContinue)
                {
                    continue;
                }

                throw;
            }

            if (!hasNext)
                yield break;

            yield return enumerator.Current;
        }
    }


    public virtual async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.EntityType == typeof(Messaging.DlqEnvelope))
            throw new InvalidOperationException("DLQは無限列挙/履歴列であり、バッチ取得・件数指定取得は現状未対応です");

        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        var results = new List<T>();

        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            results.Add(item);
        }

        return results;
    }
    /// <summary>
    /// ABSTRACT: Producer functionality - implemented in derived classes
    /// </summary>
    protected abstract Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken);

    /// <summary>
    /// IEntitySet<T> implementation: producer operations
    /// </summary>
    public virtual async Task AddAsync(T entity, Dictionary<string, string>? headers = null, CancellationToken cancellationToken = default)
    {
        if (entity == null)
            throw new ArgumentNullException(nameof(entity));

        await SendEntityAsync(entity, headers, cancellationToken);
    }

    public virtual Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"RemoveAsync is not supported for {GetType().Name}.");
    }

    /// <summary>
    /// Retrieves messages from the underlying consumer.
    /// Separated for ease of testing.
    /// </summary>
    /// <param name="context">Active KsqlContext</param>
    /// <param name="cancellationToken">Cancellation token</param>
    /// <returns>Message stream with headers</returns>
    protected virtual IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> ConsumeAsync(
        KsqlContext context,
        CancellationToken cancellationToken)
    {
        return context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
    }
    /// <summary>
    /// REDESIGNED: ForEachAsync supporting continuous Kafka consumption
    /// Design change: ToListAsync() is disallowed; now based on GetAsyncEnumerator
    /// </summary>
    public virtual Task ForEachAsync(Func<T, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        return ForEachAsync((item, headers, _) => action(item), timeout, cancellationToken);
    }

    [Obsolete("Use ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task>)")]
    public virtual Task ForEachAsync(Func<T, Dictionary<string,string>, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
        => ForEachAsync((e, h, _) => action(e, h), timeout, cancellationToken);

    public virtual async Task ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        var context = GetContext() as KsqlContext
            ?? throw new InvalidOperationException("KsqlContext is required");

        using var linkedCts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);

        if (timeout != default && timeout != TimeSpan.Zero)
        {
            linkedCts.CancelAfter(timeout);
        }
        await foreach (var (entity, headers, meta) in ConsumeAsync(context, linkedCts.Token))
        {
            if (headers.TryGetValue("is_dummy", out var dummyHeader) && bool.TryParse(dummyHeader, out var isDummy) && isDummy)
            {
                continue;
            }

            var maxAttempts = _errorHandlingContext.ErrorAction == ErrorAction.Retry
                ? _errorHandlingContext.RetryCount + 1
                : 1;

            for (var attempt = 1; attempt <= maxAttempts; attempt++)
            {
                try
                {
                    await action(entity, headers, meta);
                    break;
                }
                catch (Exception ex)
                {
                    _errorHandlingContext.CurrentAttempt = attempt;

                    if (attempt < maxAttempts && _errorHandlingContext.ErrorAction == ErrorAction.Retry)
                    {
                        await Task.Delay(_errorHandlingContext.RetryInterval, linkedCts.Token);
                        continue;
                    }

                    var dlq = context.DlqOptions;
                    if (_dlqProducer != null && dlq.EnableForHandlerError && DlqGuard.ShouldSend(dlq, context.DlqLimiter, ex.GetType()))
                    {
                        var env = DlqEnvelopeFactory.From(
                            meta, ex,
                            dlq.ApplicationId, dlq.ConsumerGroup, dlq.Host,
                            dlq.ErrorMessageMaxLength, dlq.StackTraceMaxLength, dlq.NormalizeStackTraceWhitespace);
                        await _dlqProducer.ProduceAsync(env, linkedCts.Token).ConfigureAwait(false);
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IEntitySet<T> implementation: retrieve metadata
    /// </summary>
    public string GetTopicName() => (_entityModel.TopicName ?? _entityModel.EntityType.Name).ToLowerInvariant();

    public EntityModel GetEntityModel() => _entityModel;

    public IKsqlContext GetContext() => _context;

    /// <summary>
    /// Create message context for error handling
    /// </summary>
    private KafkaMessageContext CreateMessageContext(T item)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["entity_type"] = typeof(T).Name,
                ["topic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }

    /// <summary>
    /// Configure the error handling policy
    /// </summary>
    internal virtual EventSet<T> WithErrorPolicy(ErrorHandlingPolicy policy)
    {
        if (policy == null)
            throw new ArgumentNullException(nameof(policy));

        _errorHandlingContext.ErrorAction = policy.Action;
        _errorHandlingContext.RetryCount = policy.RetryCount;
        _errorHandlingContext.RetryInterval = policy.RetryInterval;
        _errorHandlingContext.CustomHandler = policy.CustomHandler;

        return this;
    }

    public override string ToString()
    {
        return $"EventSet<{typeof(T).Name}> - Topic: {GetTopicName()}";
    }



    /// <summary>
    /// Specifies the number of retries.
    /// Used when ErrorAction.Retry is selected.
    /// </summary>
    /// <param name="maxRetries">Maximum retry count</param>
    /// <param name="retryInterval">Retry interval (optional)</param>
    /// <returns>EventSet with retry configuration applied</returns>
    public EventSet<T> WithRetry(int maxRetries, TimeSpan? retryInterval = null)
    {
        if (maxRetries < 0)
            throw new ArgumentException("Retry count must be zero or greater", nameof(maxRetries));

        var newContext = new ErrorHandlingContext
        {
            ErrorAction = _errorHandlingContext.ErrorAction,
            RetryCount = maxRetries,
            RetryInterval = retryInterval ?? TimeSpan.FromSeconds(1)
        };

        return CreateNewInstance(_context, _entityModel, newContext, _dlqErrorSink);
    }

    /// <summary>
    /// Passes the POCO to the business logic.
    /// After receiving from Kafka, each element is transformed using the supplied function.
    /// Exceptions and retries are handled based on the OnError and WithRetry settings.
    /// </summary>
    /// <typeparam name="TResult">Result type</typeparam>
    /// <param name="mapper">Mapping function</param>
    /// <returns>The mapped EventSet</returns>
    public async Task<EventSet<TResult>> Map<TResult>(Func<T, Task<TResult>> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = await ToListAsync();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            await ProcessItemWithErrorHandling(
                item,
                mapper,
                results,
                itemErrorContext);
        }

        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    /// <summary>
    /// Synchronous version of the Map function
    /// </summary>
    public EventSet<TResult> Map<TResult>(Func<T, TResult> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = ToListAsync().GetAwaiter().GetResult();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            ProcessItemWithErrorHandlingSync(
                item,
                mapper,
                results,
                itemErrorContext);
        }
        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    // Abstract method: create a new instance in derived classes
    protected virtual EventSet<T> CreateNewInstance(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorContext, IErrorSink? dlqErrorSink)
    {
        // Default implementation: concrete classes must override
        throw new NotImplementedException("Derived classes must implement CreateNewInstance");
    }

    private EntityModel CreateEntityModelForType<TResult>() where TResult : class
    {
        return new EntityModel
        {
            EntityType = typeof(TResult),
            TopicName = $"{typeof(TResult).Name.ToLowerInvariant()}_mapped",
            AllProperties = typeof(TResult).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(),
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    /// <summary>
    /// Item-level processing with error handling (async version)
    /// </summary>
    private async Task ProcessItemWithErrorHandling<TResult>(
        T item,
        Func<T, Task<TResult>> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = await mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    await Task.Delay(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = await errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext));

                if (!shouldContinue)
                {
                    return; // Skip this item and move to the next
                }
            }
        }
    }

    /// <summary>
    /// Item-level processing with error handling (sync version)
    /// </summary>
    private void ProcessItemWithErrorHandlingSync<TResult>(
        T item,
        Func<T, TResult> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    Thread.Sleep(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext)).GetAwaiter().GetResult();

                if (!shouldContinue)
                {
                    return; // Skip this item and proceed to the next
                }
            }
        }
    }

    /// <summary>
    /// Create a message context
    /// </summary>
    private KafkaMessageContext CreateContext(T item, ErrorHandlingContext errorContext)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["original_topic"] = GetTopicName(),
                ["original_partition"] = 0, // Replace with actual value
                ["original_offset"] = 0, // Replace with actual value
                ["retry_count"] = errorContext.CurrentAttempt,
                ["error_phase"] = "Processing"
            }
        };
    }

}
internal class MappedEventSet<T> : EventSet<T> where T : class
{
    private readonly List<T> _mapped;
    private readonly EntityModel _originalEntityModel;

    public MappedEventSet(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
        : base(context, CreateMappedEntityModel<T>(originalEntityModel), errorSink)
    {
        _mapped = mappedItems ?? throw new ArgumentNullException(nameof(mappedItems));
        _originalEntityModel = originalEntityModel;
    }

    /// <summary>
    /// NEW: GetAsyncEnumerator implementation for fixed lists
    /// Returns each _mapped[i] sequentially via yield return
    /// </summary>
    public override async IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default)
    {
        foreach (var item in _mapped)
        {
            if (cancellationToken.IsCancellationRequested)
                yield break;

            yield return item;

            // Inserted to treat the loop asynchronously (avoid CPU intensive work)
            await Task.Yield();
        }
    }

    /// <summary>
    /// OPTIMIZATION: ToListAsync - already a fixed list so return immediately
    /// </summary>
    public override async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        // Already a fixed list; return a copy
        await Task.CompletedTask;
        return new List<T>(_mapped);
    }

    /// <summary>
    /// Data after Map cannot be sent via Producer
    /// </summary>
    protected override Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken)
    {
        throw new NotSupportedException(
            $"MappedEventSet<{typeof(T).Name}> does not support AddAsync operations. " +
            "Mapped data is read-only and derived from transformation operations.");
    }

    public override Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"MappedEventSet<{typeof(T).Name}> does not support RemoveAsync operations.");
    }

    /// <summary>
    /// Helper method to create a MappedEventSet
    /// </summary>
    public static MappedEventSet<T> Create(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, errorSink);
    }

    /// <summary>
    /// Create a MappedEventSet with DLQ support
    /// </summary>
    public static MappedEventSet<T> CreateWithDlq(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink dlqErrorSink)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, dlqErrorSink);
    }

    /// <summary>
    /// Create an EntityModel for mapped data
    /// </summary>
    private static EntityModel CreateMappedEntityModel<TMapped>(EntityModel originalModel) where TMapped : class
    {
        return new EntityModel
        {
            EntityType = typeof(TMapped),
            TopicName = $"{originalModel.GetTopicName()}_mapped",
            AllProperties = typeof(TMapped).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(), // No key after mapping
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    public override string ToString()
    {
        return $"MappedEventSet<{typeof(T).Name}> - Items: {_mapped.Count}";
    }
}
### tool — 2025-08-11T08:14:00.331102+00:00

fileciteturn0file1

using Confluent.Kafka;
using Confluent.Kafka.SyncOverAsync;
using Confluent.SchemaRegistry.Serdes;
using Kafka.Ksql.Linq.Configuration;
using Kafka.Ksql.Linq.Configuration.Abstractions;
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Mapping;
using Kafka.Ksql.Linq.Messaging.Configuration;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Messaging.Producers;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;
using ConfluentSchemaRegistry = Confluent.SchemaRegistry;

namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal sealed class KafkaConsumerManager : IDisposable
{
    private readonly KsqlDslOptions _options;
    private readonly ILogger? _logger;
    private readonly Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient> _schemaRegistryClient;
    private readonly Dictionary<Type, EntityModel> _entityModels;
    private readonly MappingRegistry _mappingRegistry;
    private readonly DlqOptions _dlq;
    private readonly IRateLimiter _limiter;
    private readonly IDlqProducer _dlqProducer;
    private bool _disposed;

#pragma warning disable CS0067 // Event is never used
    public event Func<byte[]?, Exception, string, int, long, DateTime, Headers?, string, string, Task>? DeserializationError;
#pragma warning restore CS0067

    public KafkaConsumerManager(
        MappingRegistry mapping,
        IOptions<KsqlDslOptions> options,
        Dictionary<Type, EntityModel> entityModels,
        IDlqProducer dlqProducer,
        ILoggerFactory? loggerFactory = null,
        IRateLimiter? limiter = null)
    {
        _mappingRegistry = mapping;
        _options = options?.Value ?? throw new ArgumentNullException(nameof(options));
        _entityModels = entityModels ?? new();
        _dlqProducer = dlqProducer;
        _logger = loggerFactory?.CreateLogger<KafkaConsumerManager>();
        _schemaRegistryClient = new Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient>(CreateSchemaRegistryClient);
        _dlq = _options.DlqOptions;
        _limiter = limiter ?? new SimpleRateLimiter(_dlq.MaxPerSecond);
    }



    public async IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)> ConsumeAsync<TPOCO>(
        [EnumeratorCancellation] CancellationToken cancellationToken = default) where TPOCO : class
    {
        var model = GetEntityModel<TPOCO>();
        var topic = model.GetTopicName();
        var mapping = _mappingRegistry.GetMapping(typeof(TPOCO));
        var config = BuildConsumerConfig(topic, null);

        var method = typeof(KafkaConsumerManager)
            .GetMethod(nameof(ConsumeInternal), System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance)!
            .MakeGenericMethod(mapping.AvroKeyType!, mapping.AvroValueType!, typeof(TPOCO));

        var enumerable = (IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)>)method
            .Invoke(this, new object?[] { topic, config, mapping, cancellationToken })!;

        await foreach (var item in enumerable.WithCancellation(cancellationToken))
            yield return item;
    }

    private async IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)> ConsumeInternal<TKey, TValue, TPOCO>(
        string topicName,
        ConsumerConfig config,
        KeyValueTypeMapping mapping,
        [EnumeratorCancellation] CancellationToken cancellationToken)
        where TKey : class where TValue : class where TPOCO : class
    {
        using var consumer = CreateConsumer<TKey, TValue>(config);
        consumer.Subscribe(topicName);

        while (!cancellationToken.IsCancellationRequested)
        {
            ConsumeResult<TKey, TValue>? result;
            try
            {
                result = consumer.Consume(cancellationToken);
            }
            catch (OperationCanceledException)
            {
                break;
            }
            if (result == null || result.IsPartitionEOF)
                continue;

            TPOCO entity;
            Dictionary<string, string> headers;
            MessageMeta meta;
            try
            {
                entity = (TPOCO)mapping.CombineFromAvroKeyValue(result.Message.Key, result.Message.Value!, typeof(TPOCO));
                headers = new Dictionary<string, string>();
                if (result.Message.Headers != null)
                {
                    foreach (var h in result.Message.Headers)
                        headers[h.Key] = System.Text.Encoding.UTF8.GetString(h.GetValueBytes());
                }

                meta = new MessageMeta(
                    Topic: result.Topic,
                    Partition: result.Partition,
                    Offset: result.Offset,
                    TimestampUtc: result.Message.Timestamp.UtcDateTime,
                    SchemaIdKey: TryGetSchemaId(result.Message.Key as byte[]),
                    SchemaIdValue: TryGetSchemaId(result.Message.Value as byte[]),
                    KeyIsNull: result.Message.Key is null,
                    HeaderAllowList: ExtractAllowedHeaders(result.Message.Headers, _dlq.HeaderAllowList, _dlq.HeaderValueMaxLength)
                );
            }
            catch (Exception ex)
            {
                await HandleMappingException(result, ex, _dlqProducer, consumer, _dlq, _limiter, cancellationToken).ConfigureAwait(false);
                continue;
            }

            yield return (entity, headers, meta);
            await Task.CompletedTask;
        }
    }

    internal static async Task HandleMappingException<TKey, TValue>(
        ConsumeResult<TKey, TValue> result,
        Exception ex,
        IDlqProducer dlqProducer,
        IConsumer<TKey, TValue> consumer,
        DlqOptions options,
        IRateLimiter limiter,
        CancellationToken cancellationToken)
        where TKey : class where TValue : class
    {
        if (options.EnableForDeserializationError && DlqGuard.ShouldSend(options, limiter, ex.GetType()))
        {
            var allowHeaders = ExtractAllowedHeaders(result.Message.Headers, options.HeaderAllowList, options.HeaderValueMaxLength);
            var env = DlqEnvelopeFactory.From(result, ex,
                options.ApplicationId, options.ConsumerGroup, options.Host, allowHeaders,
                options.ErrorMessageMaxLength, options.StackTraceMaxLength, options.NormalizeStackTraceWhitespace);
            await dlqProducer.ProduceAsync(env, cancellationToken).ConfigureAwait(false);
        }
        consumer.Commit(result);
    }

    private IConsumer<TKey, TValue> CreateConsumer<TKey, TValue>(ConsumerConfig config)
        where TKey : class where TValue : class
    {
        return new ConsumerBuilder<TKey, TValue>(config)
            .SetKeyDeserializer(new AvroDeserializer<TKey>(_schemaRegistryClient.Value).AsSyncOverAsync())
            .SetValueDeserializer(new AvroDeserializer<TValue>(_schemaRegistryClient.Value).AsSyncOverAsync())
            .Build();
    }

    private ConfluentSchemaRegistry.ISchemaRegistryClient CreateSchemaRegistryClient()
    {
        var cfg = new ConfluentSchemaRegistry.SchemaRegistryConfig { Url = _options.SchemaRegistry.Url };
        return new ConfluentSchemaRegistry.CachedSchemaRegistryClient(cfg);
    }

    private EntityModel GetEntityModel<T>() where T : class
    {
        if (_entityModels.TryGetValue(typeof(T), out var model))
            return model;
        throw new InvalidOperationException($"Entity model not found for {typeof(T).Name}");
    }

    private ConsumerConfig BuildConsumerConfig(string topicName, KafkaSubscriptionOptions? subscriptionOptions)
    {
        var topicConfig = _options.Topics.TryGetValue(topicName, out var cfg) ? cfg : new TopicSection();
        var groupId = subscriptionOptions?.GroupId ?? topicConfig.Consumer.GroupId;
        if (string.IsNullOrWhiteSpace(groupId))
            groupId = "default-group";

        var consumerConfig = new ConsumerConfig
        {
            BootstrapServers = _options.Common.BootstrapServers,
            ClientId = _options.Common.ClientId,
            GroupId = groupId,
            AutoOffsetReset = Enum.Parse<AutoOffsetReset>(topicConfig.Consumer.AutoOffsetReset),
            EnableAutoCommit = topicConfig.Consumer.EnableAutoCommit,
            AutoCommitIntervalMs = topicConfig.Consumer.AutoCommitIntervalMs,
            SessionTimeoutMs = topicConfig.Consumer.SessionTimeoutMs,
            HeartbeatIntervalMs = topicConfig.Consumer.HeartbeatIntervalMs,
            MaxPollIntervalMs = topicConfig.Consumer.MaxPollIntervalMs,
            FetchMinBytes = topicConfig.Consumer.FetchMinBytes,
            FetchMaxBytes = topicConfig.Consumer.FetchMaxBytes,
            IsolationLevel = Enum.Parse<IsolationLevel>(topicConfig.Consumer.IsolationLevel)
        };

        if (subscriptionOptions != null)
        {
            if (subscriptionOptions.AutoCommit.HasValue)
                consumerConfig.EnableAutoCommit = subscriptionOptions.AutoCommit.Value;
            if (subscriptionOptions.SessionTimeout.HasValue)
                consumerConfig.SessionTimeoutMs = (int)subscriptionOptions.SessionTimeout.Value.TotalMilliseconds;
            if (subscriptionOptions.HeartbeatInterval.HasValue)
                consumerConfig.HeartbeatIntervalMs = (int)subscriptionOptions.HeartbeatInterval.Value.TotalMilliseconds;
            if (subscriptionOptions.MaxPollInterval.HasValue)
                consumerConfig.MaxPollIntervalMs = (int)subscriptionOptions.MaxPollInterval.Value.TotalMilliseconds;
        }

        if (_options.Common.SecurityProtocol != SecurityProtocol.Plaintext)
        {
            consumerConfig.SecurityProtocol = _options.Common.SecurityProtocol;
            if (_options.Common.SaslMechanism.HasValue)
            {
                consumerConfig.SaslMechanism = _options.Common.SaslMechanism.Value;
                consumerConfig.SaslUsername = _options.Common.SaslUsername;
                consumerConfig.SaslPassword = _options.Common.SaslPassword;
            }

            if (!string.IsNullOrEmpty(_options.Common.SslCaLocation))
            {
                consumerConfig.SslCaLocation = _options.Common.SslCaLocation;
                consumerConfig.SslCertificateLocation = _options.Common.SslCertificateLocation;
                consumerConfig.SslKeyLocation = _options.Common.SslKeyLocation;
                consumerConfig.SslKeyPassword = _options.Common.SslKeyPassword;
            }
        }

        foreach (var kvp in topicConfig.Consumer.AdditionalProperties)
            consumerConfig.Set(kvp.Key, kvp.Value);

        return consumerConfig;
    }

    public void ResetOffsetsToBeginning<TPOCO>() where TPOCO : class
    {
        var model = GetEntityModel<TPOCO>();
        var topic = model.GetTopicName();
        var mapping = _mappingRegistry.GetMapping(typeof(TPOCO));
        var config = BuildConsumerConfig(topic, null);
        config.EnableAutoCommit = false;
        var method = typeof(KafkaConsumerManager)
            .GetMethod(nameof(ResetOffsetsToBeginningInternal), System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance)!
            .MakeGenericMethod(mapping.AvroKeyType!, mapping.AvroValueType!);
        method.Invoke(this, new object?[] { topic, config, model.Partitions });
    }

    private void ResetOffsetsToBeginningInternal<TKey, TValue>(string topic, ConsumerConfig config, int partitions)
        where TKey : class where TValue : class
    {
        using var consumer = CreateConsumer<TKey, TValue>(config);
        var tps = new List<TopicPartitionOffset>(partitions);
        for (var i = 0; i < partitions; i++)
            tps.Add(new TopicPartitionOffset(topic, new Partition(i), new Offset(0)));
        consumer.Assign(tps);
        consumer.Commit(tps);
    }

    public void Dispose()
    {
        if (_disposed) return;
        if (_schemaRegistryClient.IsValueCreated)
            _schemaRegistryClient.Value.Dispose();
        _disposed = true;
    }

    private static int? TryGetSchemaId(byte[]? payload)
    {
        if (payload is { Length: >= 5 } && payload[0] == 0)
            return System.Buffers.Binary.BinaryPrimitives.ReadInt32BigEndian(payload.AsSpan(1, 4));
        return null;
    }

    private static System.Collections.Generic.IReadOnlyDictionary<string, string> ExtractAllowedHeaders(
        Headers? headers, System.Collections.Generic.IEnumerable<string> allowList, int maxLen = 1024)
    {
        var dict = new System.Collections.Generic.Dictionary<string, string>(System.StringComparer.OrdinalIgnoreCase);
        if (headers is null) return dict;

        var set = allowList is System.Collections.Generic.HashSet<string> hs ? hs :
                  new System.Collections.Generic.HashSet<string>(allowList ?? System.Array.Empty<string>(), System.StringComparer.OrdinalIgnoreCase);

        foreach (var h in headers)
        {
            if (!set.Contains(h.Key)) continue;
            var bytes = h.GetValueBytes() ?? System.Array.Empty<byte>();
            string val;
            try
            {
                val = System.Text.Encoding.UTF8.GetString(bytes);
                if (!System.Text.Encoding.UTF8.GetBytes(val).AsSpan().SequenceEqual(bytes))
                    val = "base64:" + System.Convert.ToBase64String(bytes);
            }
            catch
            {
                val = "base64:" + System.Convert.ToBase64String(bytes);
            }
            if (val.Length > maxLen) val = val[..maxLen];
            dict[h.Key] = val;
        }
        return dict;
    }
}
### tool — 2025-08-11T08:14:00.372561+00:00

fileciteturn0file2

namespace Kafka.Ksql.Linq.Messaging.Consumers;

public interface ICommitManager
{
    void Commit(object entity);
}
### tool — 2025-08-11T08:14:00.415571+00:00

fileciteturn0file3

namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal class NoopCommitManager : ICommitManager
{
    public void Commit(object entity) { }
}
### tool — 2025-08-11T08:14:00.428421+00:00

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.
### assistant — 2025-08-11T08:14:00.450367+00:00

_(empty)_
### assistant — 2025-08-11T08:29:17.376851+00:00

# Let's read the uploaded files to see content
import os, textwrap, json, sys, pathlib, re, itertools, math, random
### user — 2025-08-11T08:29:17.985000+00:00

回答お願いします
### system — 2025-08-11T08:29:18.225523+00:00

_(empty)_
### assistant — 2025-08-11T08:29:18.382018+00:00

_(empty)_
### assistant — 2025-08-11T08:29:23.272131+00:00

from pathlib import Path
base = Path('/mnt/data')
files = list(base.glob('*.cs'))
files
### tool — 2025-08-11T08:29:24.862062+00:00

[PosixPath('/mnt/data/EventSet.cs'),
 PosixPath('/mnt/data/KafkaConsumerManager.cs'),
 PosixPath('/mnt/data/ICommitManager.cs'),
 PosixPath('/mnt/data/NoopCommitManager.cs')]
### assistant — 2025-08-11T08:29:25.835580+00:00

for p in files:
    print("====", p.name, "====")
    print(p.read_text()[:2000])
### tool — 2025-08-11T08:29:26.878279+00:00

==== EventSet.cs ====
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Messaging.Internal;
using Kafka.Ksql.Linq.Messaging;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Query.Abstractions;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;

namespace Kafka.Ksql.Linq;

/// <summary>
/// Base class for EventSet implementing IEntitySet<T>
/// Reason for modification: unified with KsqlContext and added IEntitySet<T> implementation
/// </summary>
public abstract class EventSet<T> : IEntitySet<T> where T : class
{
    protected readonly IKsqlContext _context;
    protected readonly EntityModel _entityModel;
    private readonly ErrorHandlingContext _errorHandlingContext;
    private IErrorSink? _dlqErrorSink;
    private readonly Messaging.Producers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (context is KsqlContext kctx)
        {
            _entityModel = kctx.EnsureEntityModel(typeof(T), entityModel);
        }
        else
        {
            _entityModel = entityModel ?? throw new ArgumentNullException(nameof(entityModel));
        }

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHandlingContex
==== KafkaConsumerManager.cs ====
using Confluent.Kafka;
using Confluent.Kafka.SyncOverAsync;
using Confluent.SchemaRegistry.Serdes;
using Kafka.Ksql.Linq.Configuration;
using Kafka.Ksql.Linq.Configuration.Abstractions;
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Mapping;
using Kafka.Ksql.Linq.Messaging.Configuration;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Messaging.Producers;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;
using ConfluentSchemaRegistry = Confluent.SchemaRegistry;

namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal sealed class KafkaConsumerManager : IDisposable
{
    private readonly KsqlDslOptions _options;
    private readonly ILogger? _logger;
    private readonly Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient> _schemaRegistryClient;
    private readonly Dictionary<Type, EntityModel> _entityModels;
    private readonly MappingRegistry _mappingRegistry;
    private readonly DlqOptions _dlq;
    private readonly IRateLimiter _limiter;
    private readonly IDlqProducer _dlqProducer;
    private bool _disposed;

#pragma warning disable CS0067 // Event is never used
    public event Func<byte[]?, Exception, string, int, long, DateTime, Headers?, string, string, Task>? DeserializationError;
#pragma warning restore CS0067

    public KafkaConsumerManager(
        MappingRegistry mapping,
        IOptions<KsqlDslOptions> options,
        Dictionary<Type, EntityModel> entityModels,
        IDlqProducer dlqProducer,
        ILoggerFactory? loggerFactory = null,
        IRateLimiter? limiter = null)
    {
        _mappingRegistry = mapping;
        _options = options?.Value ?? throw new ArgumentNullException(nameof(options));
        _entityModels = entityModels ?? new();
        _dlqProducer = dlqProducer;
        _logger = loggerFactory?.Create
==== ICommitManager.cs ====
namespace Kafka.Ksql.Linq.Messaging.Consumers;

public interface ICommitManager
{
    void Commit(object entity);
}

==== NoopCommitManager.cs ====
namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal class NoopCommitManager : ICommitManager
{
    public void Commit(object entity) { }
}
### assistant — 2025-08-11T08:29:27.775114+00:00

print(Path('/mnt/data/EventSet.cs').read_text())
### tool — 2025-08-11T08:29:28.879645+00:00

if (!shouldContinue)
                {
                    continue;
                }

                throw;
            }

            if (!hasNext)
                yield break;

            yield return enumerator.Current;
        }
    }


    public virtual async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.EntityType == typeof(Messaging.DlqEnvelope))
            throw new InvalidOperationException("DLQは無限列挙/履歴列であり、バッチ取得・件数指定取得は現状未対応です");

        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        var results = new List<T>();

        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            results.Add(item);
        }

        return results;
    }
    /// <summary>
    /// ABSTRACT: Producer functionality - implemented in derived classes
    /// </summary>
    protected abstract Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken);

    /// <summary>
    /// IEntitySet<T> implementation: producer operations
    /// </summary>
    public virtual async Task AddAsync(T entity, Dictionary<string, string>? headers = null, CancellationToken cancellationToken = default)
    {
        if (entity == null)
            throw new ArgumentNullException(nameof(entity));

        await SendEntityAsync(entity, headers, cancellationToken);
    }

    public virtual Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"RemoveAsync is not supported for {GetType().Name}.");
    }

    /// <summary>
    /// Retrieves messages from the underlying consumer.
    /// Separated for ease of testing.
    /// </summary>
    /// <param name="context">Active KsqlContext</param>
    /// <param name="cancellationToken">Cancellation token</param>
    /// <returns>Message stream with headers</returns>
    protected virtual IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> ConsumeAsync(
        KsqlContext context,
        CancellationToken cancellationToken)
    {
        return context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
    }
    /// <summary>
    /// REDESIGNED: ForEachAsync supporting continuous Kafka consumption
    /// Design change: ToListAsync() is disallowed; now based on GetAsyncEnumerator
    /// </summary>
    public virtual Task ForEachAsync(Func<T, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        return ForEachAsync((item, headers, _) => action(item), timeout, cancellationToken);
    }

    [Obsolete("Use ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task>)")]
    public virtual Task ForEachAsync(Func<T, Dictionary<string,string>, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
        => ForEachAsync((e, h, _) => action(e, h), timeout, cancellationToken);

    public virtual async Task ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        var context = GetContext() as KsqlContext
            ?? throw new InvalidOperationException("KsqlContext is required");

        using var linkedCts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);

        if (timeout != default && timeout != TimeSpan.Zero)
        {
            linkedCts.CancelAfter(timeout);
        }
        await foreach (var (entity, headers, meta) in ConsumeAsync(context, linkedCts.Token))
        {
            if (headers.TryGetValue("is_dummy", out var dummyHeader) && bool.TryParse(dummyHeader, out var isDummy) && isDummy)
            {
                continue;
            }

            var maxAttempts = _errorHandlingContext.ErrorAction == ErrorAction.Retry
                ? _errorHandlingContext.RetryCount + 1
                : 1;

            for (var attempt = 1; attempt <= maxAttempts; attempt++)
            {
                try
                {
                    await action(entity, headers, meta);
                    break;
                }
                catch (Exception ex)
                {
                    _errorHandlingContext.CurrentAttempt = attempt;

                    if (attempt < maxAttempts && _errorHandlingContext.ErrorAction == ErrorAction.Retry)
                    {
                        await Task.Delay(_errorHandlingContext.RetryInterval, linkedCts.Token);
                        continue;
                    }

                    var dlq = context.DlqOptions;
                    if (_dlqProducer != null && dlq.EnableForHandlerError && DlqGuard.ShouldSend(dlq, context.DlqLimiter, ex.GetType()))
                    {
                        var env = DlqEnvelopeFactory.From(
                            meta, ex,
                            dlq.ApplicationId, dlq.ConsumerGroup, dlq.Host,
                            dlq.ErrorMessageMaxLength, dlq.StackTraceMaxLength, dlq.NormalizeStackTraceWhitespace);
                        await _dlqProducer.ProduceAsync(env, linkedCts.Token).ConfigureAwait(false);
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IEntitySet<T> implementation: retrieve metadata
    /// </summary>
    public string GetTopicName() => (_entityModel.TopicName ?? _entityModel.EntityType.Name).ToLowerInvariant();

    public EntityModel GetEntityModel() => _entityModel;

    public IKsqlContext GetContext() => _context;

    /// <summary>
    /// Create message context for error handling
    /// </summary>
    private KafkaMessageContext CreateMessageContext(T item)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["entity_type"] = typeof(T).Name,
                ["topic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }

    /// <summary>
    /// Configure the error handling policy
    /// </summary>
    internal virtual EventSet<T> WithErrorPolicy(ErrorHandlingPolicy policy)
    {
        if (policy == null)
            throw new ArgumentNullException(nameof(policy));

        _errorHandlingContext.ErrorAction = policy.Action;
        _errorHandlingContext.RetryCount = policy.RetryCount;
        _errorHandlingContext.RetryInterval = policy.RetryInterval;
        _errorHandlingContext.CustomHandler = policy.CustomHandler;

        return this;
    }

    public override string ToString()
    {
        return $"EventSet<{typeof(T).Name}> - Topic: {GetTopicName()}";
    }



    /// <summary>
    /// Specifies the number of retries.
    /// Used when ErrorAction.Retry is selected.
    /// </summary>
    /// <param name="maxRetries">Maximum retry count</param>
    /// <param name="retryInterval">Retry interval (optional)</param>
    /// <returns>EventSet with retry configuration applied</returns>
    public EventSet<T> WithRetry(int maxRetries, TimeSpan? retryInterval = null)
    {
        if (maxRetries < 0)
            throw new ArgumentException("Retry count must be zero or greater", nameof(maxRetries));

        var newContext = new ErrorHandlingContext
        {
            ErrorAction = _errorHandlingContext.ErrorAction,
            RetryCount = maxRetries,
            RetryInterval = retryInterval ?? TimeSpan.FromSeconds(1)
        };

        return CreateNewInstance(_context, _entityModel, newContext, _dlqErrorSink);
    }

    /// <summary>
    /// Passes the POCO to the business logic.
    /// After receiving from Kafka, each element is transformed using the supplied function.
    /// Exceptions and retries are handled based on the OnError and WithRetry settings.
    /// </summary>
    /// <typeparam name="TResult">Result type</typeparam>
    /// <param name="mapper">Mapping function</param>
    /// <returns>The mapped EventSet</returns>
    public async Task<EventSet<TResult>> Map<TResult>(Func<T, Task<TResult>> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = await ToListAsync();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            await ProcessItemWithErrorHandling(
                item,
                mapper,
                results,
                itemErrorContext);
        }

        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    /// <summary>
    /// Synchronous version of the Map function
    /// </summary>
    public EventSet<TResult> Map<TResult>(Func<T, TResult> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = ToListAsync().GetAwaiter().GetResult();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            ProcessItemWithErrorHandlingSync(
                item,
                mapper,
                results,
                itemErrorContext);
        }
        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    // Abstract method: create a new instance in derived classes
    protected virtual EventSet<T> CreateNewInstance(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorContext, IErrorSink? dlqErrorSink)
    {
        // Default implementation: concrete classes must override
        throw new NotImplementedException("Derived classes must implement CreateNewInstance");
    }

    private EntityModel CreateEntityModelForType<TResult>() where TResult : class
    {
        return new EntityModel
        {
            EntityType = typeof(TResult),
            TopicName = $"{typeof(TResult).Name.ToLowerInvariant()}_mapped",
            AllProperties = typeof(TResult).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(),
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    /// <summary>
    /// Item-level processing with error handling (async version)
    /// </summary>
    private async Task ProcessItemWithErrorHandling<TResult>(
        T item,
        Func<T, Task<TResult>> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = await mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    await Task.Delay(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = await errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext));

                if (!shouldContinue)
                {
                    return; // Skip this item and move to the next
                }
            }
        }
    }

    /// <summary>
    /// Item-level processing with error handling (sync version)
    /// </summary>
    private void ProcessItemWithErrorHandlingSync<TResult>(
        T item,
        Func<T, TResult> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    Thread.Sleep(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext)).GetAwaiter().GetResult();

                if (!shouldContinue)
                {
                    return; // Skip this item and proceed to the next
                }
            }
        }
    }

    /// <summary>
    /// Create a message context
    /// </summary>
    private KafkaMessageContext CreateContext(T item, ErrorHandlingContext errorContext)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["original_topic"] = GetTopicName(),
                ["original_partition"] = 0, // Replace with actual value
                ["original_offset"] = 0, // Replace with actual value
                ["retry_count"] = errorContext.CurrentAttempt,
                ["error_phase"] = "Processing"
            }
        };
    }

}
internal class MappedEventSet<T> : EventSet<T> where T : class
{
    private readonly List<T> _mapped;
    private readonly EntityModel _originalEntityModel;

    public MappedEventSet(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
        : base(context, CreateMappedEntityModel<T>(originalEntityModel), errorSink)
    {
        _mapped = mappedItems ?? throw new ArgumentNullException(nameof(mappedItems));
        _originalEntityModel = originalEntityModel;
    }

    /// <summary>
    /// NEW: GetAsyncEnumerator implementation for fixed lists
    /// Returns each _mapped[i] sequentially via yield return
    /// </summary>
    public override async IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default)
    {
        foreach (var item in _mapped)
        {
            if (cancellationToken.IsCancellationRequested)
                yield break;

            yield return item;

            // Inserted to treat the loop asynchronously (avoid CPU intensive work)
            await Task.Yield();
        }
    }

    /// <summary>
    /// OPTIMIZATION: ToListAsync - already a fixed list so return immediately
    /// </summary>
    public override async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        // Already a fixed list; return a copy
        await Task.CompletedTask;
        return new List<T>(_mapped);
    }

    /// <summary>
    /// Data after Map cannot be sent via Producer
    /// </summary>
    protected override Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken)
    {
        throw new NotSupportedException(
            $"MappedEventSet<{typeof(T).Name}> does not support AddAsync operations. " +
            "Mapped data is read-only and derived from transformation operations.");
    }

    public override Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"MappedEventSet<{typeof(T).Name}> does not support RemoveAsync operations.");
    }

    /// <summary>
    /// Helper method to create a MappedEventSet
    /// </summary>
    public static MappedEventSet<T> Create(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, errorSink);
    }

    /// <summary>
    /// Create a MappedEventSet with DLQ support
    /// </summary>
    public static MappedEventSet<T> CreateWithDlq(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink dlqErrorSink)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, dlqErrorSink);
    }

    /// <summary>
    /// Create an EntityModel for mapped data
    /// </summary>
    private static EntityModel CreateMappedEntityModel<TMapped>(EntityModel originalModel) where TMapped : class
    {
        return new EntityModel
        {
            EntityType = typeof(TMapped),
            TopicName = $"{originalModel.GetTopicName()}_mapped",
            AllProperties = typeof(TMapped).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(), // No key after mapping
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    public override string ToString()
    {
        return $"MappedEventSet<{typeof(T).Name}> - Items: {_mapped.Count}";
    }
}
### assistant — 2025-08-11T08:29:29.835182+00:00

print(Path('/mnt/data/KafkaConsumerManager.cs').read_text()[:8000])
### tool — 2025-08-11T08:29:31.066949+00:00

using Confluent.Kafka;
using Confluent.Kafka.SyncOverAsync;
using Confluent.SchemaRegistry.Serdes;
using Kafka.Ksql.Linq.Configuration;
using Kafka.Ksql.Linq.Configuration.Abstractions;
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Mapping;
using Kafka.Ksql.Linq.Messaging.Configuration;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Messaging.Producers;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;
using ConfluentSchemaRegistry = Confluent.SchemaRegistry;

namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal sealed class KafkaConsumerManager : IDisposable
{
    private readonly KsqlDslOptions _options;
    private readonly ILogger? _logger;
    private readonly Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient> _schemaRegistryClient;
    private readonly Dictionary<Type, EntityModel> _entityModels;
    private readonly MappingRegistry _mappingRegistry;
    private readonly DlqOptions _dlq;
    private readonly IRateLimiter _limiter;
    private readonly IDlqProducer _dlqProducer;
    private bool _disposed;

#pragma warning disable CS0067 // Event is never used
    public event Func<byte[]?, Exception, string, int, long, DateTime, Headers?, string, string, Task>? DeserializationError;
#pragma warning restore CS0067

    public KafkaConsumerManager(
        MappingRegistry mapping,
        IOptions<KsqlDslOptions> options,
        Dictionary<Type, EntityModel> entityModels,
        IDlqProducer dlqProducer,
        ILoggerFactory? loggerFactory = null,
        IRateLimiter? limiter = null)
    {
        _mappingRegistry = mapping;
        _options = options?.Value ?? throw new ArgumentNullException(nameof(options));
        _entityModels = entityModels ?? new();
        _dlqProducer = dlqProducer;
        _logger = loggerFactory?.CreateLogger<KafkaConsumerManager>();
        _schemaRegistryClient = new Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient>(CreateSchemaRegistryClient);
        _dlq = _options.DlqOptions;
        _limiter = limiter ?? new SimpleRateLimiter(_dlq.MaxPerSecond);
    }



    public async IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)> ConsumeAsync<TPOCO>(
        [EnumeratorCancellation] CancellationToken cancellationToken = default) where TPOCO : class
    {
        var model = GetEntityModel<TPOCO>();
        var topic = model.GetTopicName();
        var mapping = _mappingRegistry.GetMapping(typeof(TPOCO));
        var config = BuildConsumerConfig(topic, null);

        var method = typeof(KafkaConsumerManager)
            .GetMethod(nameof(ConsumeInternal), System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance)!
            .MakeGenericMethod(mapping.AvroKeyType!, mapping.AvroValueType!, typeof(TPOCO));

        var enumerable = (IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)>)method
            .Invoke(this, new object?[] { topic, config, mapping, cancellationToken })!;

        await foreach (var item in enumerable.WithCancellation(cancellationToken))
            yield return item;
    }

    private async IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)> ConsumeInternal<TKey, TValue, TPOCO>(
        string topicName,
        ConsumerConfig config,
        KeyValueTypeMapping mapping,
        [EnumeratorCancellation] CancellationToken cancellationToken)
        where TKey : class where TValue : class where TPOCO : class
    {
        using var consumer = CreateConsumer<TKey, TValue>(config);
        consumer.Subscribe(topicName);

        while (!cancellationToken.IsCancellationRequested)
        {
            ConsumeResult<TKey, TValue>? result;
            try
            {
                result = consumer.Consume(cancellationToken);
            }
            catch (OperationCanceledException)
            {
                break;
            }
            if (result == null || result.IsPartitionEOF)
                continue;

            TPOCO entity;
            Dictionary<string, string> headers;
            MessageMeta meta;
            try
            {
                entity = (TPOCO)mapping.CombineFromAvroKeyValue(result.Message.Key, result.Message.Value!, typeof(TPOCO));
                headers = new Dictionary<string, string>();
                if (result.Message.Headers != null)
                {
                    foreach (var h in result.Message.Headers)
                        headers[h.Key] = System.Text.Encoding.UTF8.GetString(h.GetValueBytes());
                }

                meta = new MessageMeta(
                    Topic: result.Topic,
                    Partition: result.Partition,
                    Offset: result.Offset,
                    TimestampUtc: result.Message.Timestamp.UtcDateTime,
                    SchemaIdKey: TryGetSchemaId(result.Message.Key as byte[]),
                    SchemaIdValue: TryGetSchemaId(result.Message.Value as byte[]),
                    KeyIsNull: result.Message.Key is null,
                    HeaderAllowList: ExtractAllowedHeaders(result.Message.Headers, _dlq.HeaderAllowList, _dlq.HeaderValueMaxLength)
                );
            }
            catch (Exception ex)
            {
                await HandleMappingException(result, ex, _dlqProducer, consumer, _dlq, _limiter, cancellationToken).ConfigureAwait(false);
                continue;
            }

            yield return (entity, headers, meta);
            await Task.CompletedTask;
        }
    }

    internal static async Task HandleMappingException<TKey, TValue>(
        ConsumeResult<TKey, TValue> result,
        Exception ex,
        IDlqProducer dlqProducer,
        IConsumer<TKey, TValue> consumer,
        DlqOptions options,
        IRateLimiter limiter,
        CancellationToken cancellationToken)
        where TKey : class where TValue : class
    {
        if (options.EnableForDeserializationError && DlqGuard.ShouldSend(options, limiter, ex.GetType()))
        {
            var allowHeaders = ExtractAllowedHeaders(result.Message.Headers, options.HeaderAllowList, options.HeaderValueMaxLength);
            var env = DlqEnvelopeFactory.From(result, ex,
                options.ApplicationId, options.ConsumerGroup, options.Host, allowHeaders,
                options.ErrorMessageMaxLength, options.StackTraceMaxLength, options.NormalizeStackTraceWhitespace);
            await dlqProducer.ProduceAsync(env, cancellationToken).ConfigureAwait(false);
        }
        consumer.Commit(result);
    }

    private IConsumer<TKey, TValue> CreateConsumer<TKey, TValue>(ConsumerConfig config)
        where TKey : class where TValue : class
    {
        return new ConsumerBuilder<TKey, TValue>(config)
            .SetKeyDeserializer(new AvroDeserializer<TKey>(_schemaRegistryClient.Value).AsSyncOverAsync())
            .SetValueDeserializer(new AvroDeserializer<TValue>(_schemaRegistryClient.Value).AsSyncOverAsync())
            .Build();
    }

    private ConfluentSchemaRegistry.ISchemaRegistryClient CreateSchemaRegistryClient()
    {
        var cfg = new ConfluentSchemaRegistry.SchemaRegistryConfig { Url = _options.SchemaRegistry.Url };
        return new ConfluentSchemaRegistry.CachedSchemaRegistryClient(cfg);
    }

    private EntityModel GetEntityModel<T>() where T : class
    {
        if (_entityModels.TryGetValue(typeof(T), out var model))
            return model;
        throw new InvalidOperationException($"Entity model not found for {typeof(T).Name}");
    }

    private ConsumerConfig BuildConsumerConfig(string topicName, KafkaSubscriptionOptions? subscriptionOptions)
    {
        var topicConfig = _options.Topics.TryGetValue(topicName, out var cfg) ? cfg : new TopicSection();
        var groupId = subscriptionOptions?.GroupId ?? topicConfig.Consumer.GroupId;
        if
### assistant — 2025-08-11T08:29:32.006432+00:00

import re, pathlib, itertools, sys, os, json, textwrap
text = Path('/mnt/data/EventSet.cs').read_text()
# find occurrences of ICommitManager and commit
for m in re.finditer(r'ICommitManager|Commit\(|CommitAsync|Manual', text):
    start = max(0, m.start()-300)
    end = min(len(text), m.end()+300)
    print(text[start:end])
    print('---')
### tool — 2025-08-11T08:29:37.621896+00:00

cted readonly IKsqlContext _context;
    protected readonly EntityModel _entityModel;
    private readonly ErrorHandlingContext _errorHandlingContext;
    private IErrorSink? _dlqErrorSink;
    private readonly Messaging.Producers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new Argu
---
cers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (con
---
rorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorHandlingContext, IErrorSink? dlqErrorSink,
        Messaging.Producers.IDlqProducer? dlqProducer, Messaging.Consumers.ICommitManager? commitManager)
    {
        _context = context;
        _entityModel = entityModel;
        _errorHandlingContext = errorHandlingContext;
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (_dlqErrorSink != null)
        {
---
onsumerGroup, dlq.Host,
                            dlq.ErrorMessageMaxLength, dlq.StackTraceMaxLength, dlq.NormalizeStackTraceWhitespace);
                        await _dlqProducer.ProduceAsync(env, linkedCts.Token).ConfigureAwait(false);
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IEntitySet<T> implementation: retrieve metadata
    /// </summary>
    public string GetTopicName() => (_entityModel.TopicName ?? _entityModel.EntityType.Name).ToLowerInvariant();

    publ
---
string, object>
            {
                ["entity_type"] = typeof(T).Name,
                ["topic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types dep
---
T).Name,
                ["topic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async I
---
pic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEa
---
sing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken ca
---
 instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualComm
---
 UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }

    /// <summary>
    /// Configure the error handling policy
    /// </summary>
    internal virtual Eve
---
syncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }

    /// <summary>
    /// Configure the error handling policy
    /// </summary>
    internal virtual EventSet<T> WithErrorPolicy(ErrorHandlingPolicy policy)
    {
    
---
### assistant — 2025-08-11T08:29:39.349064+00:00

idx = text.find("protected EventSet")
print(text[idx:idx+1000])
### tool — 2025-08-11T08:29:40.455066+00:00

protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (context is KsqlContext kctx)
        {
            _entityModel = kctx.EnsureEntityModel(typeof(T), entityModel);
        }
        else
        {
            _entityModel = entityModel ?? throw new ArgumentNullException(nameof(entityModel));
        }

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHand
### assistant — 2025-08-11T08:29:41.305417+00:00

for m in re.finditer(r'ForEachAsync', text):
    print(m.start())
### tool — 2025-08-11T08:29:42.238052+00:00

[[REDACTED_PHONE]]
[[REDACTED_PHONE]]
[[REDACTED_PHONE]]
### assistant — 2025-08-11T08:29:43.165083+00:00

print(text[3400:4600])
### tool — 2025-08-11T08:29:44.924055+00:00

(),
                    Tags = new Dictionary<string, object>
                    {
                        ["processing_phase"] = "ForEachAsync"
                    }
                };

                var shouldContinue = await _errorHandlingContext.HandleErrorAsync(default(T)!, ex, ctx);

                if (!shouldContinue)
                {
                    continue;
                }

                throw;
            }

            if (!hasNext)
                yield break;

            yield return enumerator.Current;
        }
    }


    public virtual async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.EntityType == typeof(Messaging.DlqEnvelope))
            throw new InvalidOperationException("DLQは無限列挙/履歴列であり、バッチ取得・件数指定取得は現状未対応です");

        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        var results = new List<T>();

        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            re
### assistant — 2025-08-11T08:29:45.882376+00:00

print(Path('/mnt/data/ICommitManager.cs').read_text())
### tool — 2025-08-11T08:29:51.153616+00:00

namespace Kafka.Ksql.Linq.Messaging.Consumers;

public interface ICommitManager
{
    void Commit(object entity);
}
### assistant — 2025-08-11T08:29:52.069383+00:00

kc = Path('/mnt/data/KafkaConsumerManager.cs').read_text()
print("length", len(kc))
for m in re.finditer(r'CommitManager|ICommitManager|NoopCommitManager|manual', kc, re.IGNORECASE):
    start=max(0,m.start()-300); end=min(len(kc), m.end()+300)
    print(kc[start:end], "\n---")
### tool — 2025-08-11T08:29:53.686632+00:00

length 13775
### assistant — 2025-08-11T08:29:54.541176+00:00

print(kc[-2000:])
### tool — 2025-08-11T08:29:55.451788+00:00

rtitionOffset>(partitions);
        for (var i = 0; i < partitions; i++)
            tps.Add(new TopicPartitionOffset(topic, new Partition(i), new Offset(0)));
        consumer.Assign(tps);
        consumer.Commit(tps);
    }

    public void Dispose()
    {
        if (_disposed) return;
        if (_schemaRegistryClient.IsValueCreated)
            _schemaRegistryClient.Value.Dispose();
        _disposed = true;
    }

    private static int? TryGetSchemaId(byte[]? payload)
    {
        if (payload is { Length: >= 5 } && payload[0] == 0)
            return System.Buffers.Binary.BinaryPrimitives.ReadInt32BigEndian(payload.AsSpan(1, 4));
        return null;
    }

    private static System.Collections.Generic.IReadOnlyDictionary<string, string> ExtractAllowedHeaders(
        Headers? headers, System.Collections.Generic.IEnumerable<string> allowList, int maxLen = 1024)
    {
        var dict = new System.Collections.Generic.Dictionary<string, string>(System.StringComparer.OrdinalIgnoreCase);
        if (headers is null) return dict;

        var set = allowList is System.Collections.Generic.HashSet<string> hs ? hs :
                  new System.Collections.Generic.HashSet<string>(allowList ?? System.Array.Empty<string>(), System.StringComparer.OrdinalIgnoreCase);

        foreach (var h in headers)
        {
            if (!set.Contains(h.Key)) continue;
            var bytes = h.GetValueBytes() ?? System.Array.Empty<byte>();
            string val;
            try
            {
                val = System.Text.Encoding.UTF8.GetString(bytes);
                if (!System.Text.Encoding.UTF8.GetBytes(val).AsSpan().SequenceEqual(bytes))
                    val = "base64:" + System.Convert.ToBase64String(bytes);
            }
            catch
            {
                val = "base64:" + System.Convert.ToBase64String(bytes);
            }
            if (val.Length > maxLen) val = val[..maxLen];
            dict[h.Key] = val;
        }
        return dict;
    }
}
### assistant — 2025-08-11T08:29:56.497626+00:00

for m in re.finditer(r'Commit\(', text):
    print(m.start(), text[m.start()-60:m.start()+120])
### tool — 2025-08-11T08:29:57.823150+00:00

9322 
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IE
### assistant — 2025-08-11T08:29:58.796105+00:00

print(text[9000:9600])
### tool — 2025-08-11T08:30:02.397457+00:00

q.ApplicationId, dlq.ConsumerGroup, dlq.Host,
                            dlq.ErrorMessageMaxLength, dlq.StackTraceMaxLength, dlq.NormalizeStackTraceWhitespace);
                        await _dlqProducer.ProduceAsync(env, linkedCts.Token).ConfigureAwait(false);
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IEntitySet<T> implementation: retrieve metadata
    /// </summary>
    public string GetTopicName() => (_entityModel.TopicName ?? _entityModel.EntityType.Name).
### assistant — 2025-08-11T08:30:03.332763+00:00

for m in re.finditer(r'public .*Commit', text):
    print(m.start(), text[m.start()-100:m.start()+200])
### assistant — 2025-08-11T08:30:06.290894+00:00

if 'ConditionalWeakTable' in kc:
    print("Has CWT")
else:
    print("No CWT found")
### tool — 2025-08-11T08:30:10.455806+00:00

No CWT found
### assistant — 2025-08-11T08:30:11.394342+00:00

# Try to find where EventSet enumerates consumer
for kw in ['Consume', 'enumerator', 'GetAsyncEnumeratorWrapper', 'InternalStream']:
    pos = text.find(kw)
    print(kw, pos)
### tool — 2025-08-11T08:30:16.899188+00:00

Consume 943
enumerator 3016
GetAsyncEnumeratorWrapper 2888
InternalStream -1
### assistant — 2025-08-11T08:30:17.794195+00:00

print(text[2800:3600])
### tool — 2025-08-11T08:30:18.645108+00:00

(CancellationToken cancellationToken = default);

    private async IAsyncEnumerable<T> GetAsyncEnumeratorWrapper([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await using var enumerator = GetAsyncEnumerator(cancellationToken);

        while (true)
        {
            bool hasNext;
            try
            {
                hasNext = await enumerator.MoveNextAsync();
            }
            catch (Exception ex)
            {
                var ctx = new KafkaMessageContext
                {
                    MessageId = Guid.NewGuid().ToString(),
                    Tags = new Dictionary<string, object>
                    {
                        ["processing_phase"] = "ForEachAsync"
                    }
                };
### assistant — 2025-08-11T08:30:19.726729+00:00

いいね、`ctx.Samples.Commit(poco)` でいきましょう。**呼び出し側の列挙は常に `T` のまま**、manual/auto を内部で吸収します。添付コードの構成（`EventSet<T>` に `_commitManager`、`ICommitManager` と `NoopCommitManager` あり）にピッタリはまります。

# 変更ポイント（最小差分）

## 1) `EventSet<T>` に公開 `Commit` を追加
`_commitManager?.Commit(entity)` は内部（DLQ後）でしか呼ばれていないので、**呼び出し側から叩ける薄い委譲**を1本生やします。

```diff
*** a/EventSet.cs
--- b/EventSet.cs
@@
     private readonly Messaging.Consumers.ICommitManager? _commitManager;
@@
     }
 
+    /// <summary>
+    /// 呼び出し側から手動コミットを行う。autocommit 時は Noop 実装により無効動作。
+    /// </summary>
+    public void Commit(T entity)
+    {
+        // entity は ForEachAsync で返したインスタンスを渡す前提
+        _commitManager?.Commit(entity!);
+    }
```

> 補足  
> - 既存の `ICommitManager` は `void Commit(object entity)` なので **非同期にしない**（現在のIFに合わせる）。  
> - autocommit のときは `NoopCommitManager` が差さっているはずなので **no-op** で互換維持。

## 2) 呼び出し側（テスト）を書き換え
キャストを廃し、`T` のままコミットできます。

```diff
- await foreach (var obj in ctx.Samples.ForEachAsync(consumeCts.Token))
- {
-     var msg = (IManualCommitMessage<ManualCommitContext.Sample>)obj;
-     if (msg.Value.Id == 3)
-     {
-         await msg.CommitAsync();
-         break;
-     }
- }
+ await foreach (var sample in ctx.Samples.ForEachAsync(consumeCts.Token))
+ {
+     if (sample.Id == 3)
+     {
+         ctx.Samples.Commit(sample); // autocommitでもno-op
+         break;
+     }
+ }
```

2回目の検証側も同様に `Commit(sample)` でOK（またはその場は不要なら省略可）。

---

# 仕組みの前提（この設計が成立する理由）
- `EventSet<T>` にはすでに `_commitManager` が注入されており、内部でも `Commit(entity)` 呼び出しがあります（DLQ処理部の直後）。  
- **manual時**：実体の `CommitManager` 側で「列挙中に返した `entity` とその `TopicPartitionOffset` の対応」を保持している前提（一般的には `ConditionalWeakTable<object, TPO>` など）。`Commit(entity)` で対応するオフセットを同期コミット。  
- **autocommit時**：`NoopCommitManager` 実装で無視。

> もし実装が未だなら：  
> `CommitManager` 実装で、**ForEachAsync の yield 直前**に `entity→TPO` を登録し、`Commit(entity)` で取り出して `consumer.Commit(tpo)` → 登録解除、という流れにしてください。参照同一性で追跡するため `T : class` 前提が堅いです。

---

# 追加の小ネタ（必要なら）
- バッチ用 `Commit(IEnumerable<T> entities)` を足すのも容易（内部で順に `_commitManager.Commit`）。  
- 別名で `CommitCurrent()`（最後に返した1件をコミット）を用意すると、呼び出し側が `poco` を持ち回らなくて済む（`AsyncLocal` で追跡）。

---

この変更で、**autocommit ⇄ manual commit の切替時にも呼び出し側の構造は不変**、かつ**キャスト不要**になります。差分がもっと欲しければ `diff` 完全版も出します。  

— 送信日時: [[REDACTED_PHONE]]:35 (JST)
### assistant — 2025-08-11T08:30:31.331281+00:00

_(empty)_
### assistant — 2025-08-11T08:31:47.656066+00:00

了解、**既存IFを崩さず**に「`ctx.Samples.Commit(poco)`」で手動コミットできるよう、**`EventSet<T>` だけ最小差分**で拡張します。  
（`ICommitManager` は既存のまま `Commit(object)` を呼び出します。autocommit 時は `NoopCommitManager` のまま no-op です。）  
※下記はそのまま適用できる差分です。

---

# 変更点（最小差分）

## 1) `EventSet<T>` に公開 `Commit(T entity)` を追加
- 呼び出し側は **常に `T` を扱う**ままで OK（キャスト不要・ジェネリック指定不要）。
- manual 時のみ実コミット、autocommit 時は `NoopCommitManager` で no-op。

```diff
*** a/EventSet.cs
--- b/EventSet.cs
@@
     private readonly Messaging.Consumers.ICommitManager? _commitManager;
@@
     }
 
+    /// <summary>
+    /// 呼び出し側から手動コミットを行う（autocommit時はno-op）。
+    /// ForEachAsync で受け取った entity インスタンスを渡すこと。
+    /// </summary>
+    public void Commit(T entity)
+    {
+        if (entity is null) throw new ArgumentNullException(nameof(entity));
+        _commitManager?.Commit(entity);
+    }
```

## 2) （任意・推奨）コミット対象の追跡フックを追加  
**既存IFは変更せず**、手動コミット実装側（`ICommitManager` の具体型）が **“今返した `entity` → オフセット情報”** を受け取れるように、**オプショナルな内部インターフェース**を挿入します。  
これにより「autocommit⇄manual の切替で呼び出し側は無変更」のまま、手動コミット側がインラインに紐づけ可能です。
### 2-1) 追跡用の内部IFを `EventSet.cs` 内に宣言
```diff
*** a/EventSet.cs
--- b/EventSet.cs
@@
 namespace Kafka.Ksql.Linq;
 
 /// <summary>
 /// Base class for EventSet implementing IEntitySet<T>
 /// Reason for modification: unified with KsqlContext and added IEntitySet<T> implementation
 /// </summary>
 public abstract class EventSet<T> : IEntitySet<T> where T : class
 {
+    // 任意実装：CommitManager が entity と meta を紐づけたい場合に実装する
+    internal interface ICommitRegistrar
+    {
+        void Track(object entity, MessageMeta meta);
+    }
@@
     protected virtual IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> ConsumeAsync(
         KsqlContext context,
         CancellationToken cancellationToken)
     {
-        return context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
+        // 元の列挙に、必要なら “コミット追跡” を差し込む
+        var source = context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
+        return TrackCommitIfSupported(source);
     }
+
+    // _commitManager が ICommitRegistrar を実装している場合だけ entity→meta を紐づける
+    private async IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> TrackCommitIfSupported(
+        IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> source,
+        [EnumeratorCancellation] CancellationToken cancellationToken = default)
+    {
+        var registrar = _commitManager as ICommitRegistrar;
+        await foreach (var (entity, headers, meta) in source.WithCancellation(cancellationToken))
+        {
+            registrar?.Track(entity!, meta);
+            yield return (entity, headers, meta);
+        }
+    }
```

> メモ  
> - **既存の `ICommitManager` は一切変更不要**。  
> - manual 用の実装側（あなたの実プロジェクトの CommitManager）が `ICommitManager` に **加えて** `EventSet<T>.ICommitRegistrar` を **実装すると**、`Track(entity, meta)` 経由で **オフセット情報（`MessageMeta`）の受け取り**が可能になります。  
> - autocommit では `NoopCommitManager`（既存）なので何も起きません（no-op）。

---

# 呼び出し側（テスト）の書き換え（キャスト排除）
```diff
- await foreach (var obj in ctx.Samples.ForEachAsync(consumeCts.Token))
- {
-     var msg = (IManualCommitMessage<ManualCommitContext.Sample>)obj;
-     if (msg.Value.Id == 3)
-     {
-         await msg.CommitAsync();
-         break;
-     }
- }
+ await foreach (var sample in ctx.Samples.ForEachAsync(consumeCts.Token))
+ {
+     if (sample.Id == 3)
+     {
+         ctx.Samples.Commit(sample); // manual: 実コミット / autocommit: no-op
+         break;
+     }
+ }
```

2回目の検証側も同様に `Commit(sample)` でOK（必要なら）。

---

# 実装側メモ（既存構成にそのまま乗せる）
- `KafkaConsumerManager.ConsumeAsync<TPOCO>()` は **`(TPOCO, Headers, MessageMeta)`** を返してくれています（既存）。  
  `EventSet<T>.ConsumeAsync` の差し込みで **`entity → meta` の紐づけ**が可能になりました。 fileciteturn1file1
- autocommit 時の no-op は既存の `NoopCommitManager` が担保（`Commit(object){}`）。 fileciteturn1file2
- `ICommitManager` は **現状のまま**（`void Commit(object entity)`）。 fileciteturn1file0

> manual 用 `CommitManager` 実装のヒント  
> - `Track(entity, meta)` で **参照同一性**ベースに `entity → meta` を保持（`ConditionalWeakTable<object, MessageMeta>` が安全）。  
> - `Commit(entity)` で取り出した `meta` の `Topic/Partition/Offset` を使って **同期コミット**（あなたの実装内の Kafka Consumer で `Commit(TopicPartitionOffset)`）。  
> - コミット後は登録解除（リーク防止、二重コミット防止）。

---

この差分なら、**既存の型やIFはそのまま**、呼び出し側は **`T` を扱ったまま** `Commit(sample)` を呼ぶだけです。  
他ファイルの変更は **不要** です（manual 用の CommitManager 実装だけ、任意で `ICommitRegistrar` を実装してください）。

— 送信日時: [[REDACTED_PHONE]]:46 (JST)
### system — 2025-08-11T08:31:47.935317+00:00

_(empty)_
### tool — 2025-08-11T08:31:47.940805+00:00

_(empty)_
### tool — 2025-08-11T08:31:47.951618+00:00

fileciteturn1file0

namespace Kafka.Ksql.Linq.Messaging.Consumers;

public interface ICommitManager
{
    void Commit(object entity);
}
### tool — 2025-08-11T08:31:47.963176+00:00

fileciteturn1file1

using Confluent.Kafka;
using Confluent.Kafka.SyncOverAsync;
using Confluent.SchemaRegistry.Serdes;
using Kafka.Ksql.Linq.Configuration;
using Kafka.Ksql.Linq.Configuration.Abstractions;
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Mapping;
using Kafka.Ksql.Linq.Messaging.Configuration;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Messaging.Producers;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;
using ConfluentSchemaRegistry = Confluent.SchemaRegistry;

namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal sealed class KafkaConsumerManager : IDisposable
{
    private readonly KsqlDslOptions _options;
    private readonly ILogger? _logger;
    private readonly Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient> _schemaRegistryClient;
    private readonly Dictionary<Type, EntityModel> _entityModels;
    private readonly MappingRegistry _mappingRegistry;
    private readonly DlqOptions _dlq;
    private readonly IRateLimiter _limiter;
    private readonly IDlqProducer _dlqProducer;
    private bool _disposed;

#pragma warning disable CS0067 // Event is never used
    public event Func<byte[]?, Exception, string, int, long, DateTime, Headers?, string, string, Task>? DeserializationError;
#pragma warning restore CS0067

    public KafkaConsumerManager(
        MappingRegistry mapping,
        IOptions<KsqlDslOptions> options,
        Dictionary<Type, EntityModel> entityModels,
        IDlqProducer dlqProducer,
        ILoggerFactory? loggerFactory = null,
        IRateLimiter? limiter = null)
    {
        _mappingRegistry = mapping;
        _options = options?.Value ?? throw new ArgumentNullException(nameof(options));
        _entityModels = entityModels ?? new();
        _dlqProducer = dlqProducer;
        _logger = loggerFactory?.CreateLogger<KafkaConsumerManager>();
        _schemaRegistryClient = new Lazy<ConfluentSchemaRegistry.ISchemaRegistryClient>(CreateSchemaRegistryClient);
        _dlq = _options.DlqOptions;
        _limiter = limiter ?? new SimpleRateLimiter(_dlq.MaxPerSecond);
    }



    public async IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)> ConsumeAsync<TPOCO>(
        [EnumeratorCancellation] CancellationToken cancellationToken = default) where TPOCO : class
    {
        var model = GetEntityModel<TPOCO>();
        var topic = model.GetTopicName();
        var mapping = _mappingRegistry.GetMapping(typeof(TPOCO));
        var config = BuildConsumerConfig(topic, null);

        var method = typeof(KafkaConsumerManager)
            .GetMethod(nameof(ConsumeInternal), System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance)!
            .MakeGenericMethod(mapping.AvroKeyType!, mapping.AvroValueType!, typeof(TPOCO));

        var enumerable = (IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)>)method
            .Invoke(this, new object?[] { topic, config, mapping, cancellationToken })!;

        await foreach (var item in enumerable.WithCancellation(cancellationToken))
            yield return item;
    }

    private async IAsyncEnumerable<(TPOCO, Dictionary<string, string>, MessageMeta)> ConsumeInternal<TKey, TValue, TPOCO>(
        string topicName,
        ConsumerConfig config,
        KeyValueTypeMapping mapping,
        [EnumeratorCancellation] CancellationToken cancellationToken)
        where TKey : class where TValue : class where TPOCO : class
    {
        using var consumer = CreateConsumer<TKey, TValue>(config);
        consumer.Subscribe(topicName);

        while (!cancellationToken.IsCancellationRequested)
        {
            ConsumeResult<TKey, TValue>? result;
            try
            {
                result = consumer.Consume(cancellationToken);
            }
            catch (OperationCanceledException)
            {
                break;
            }
            if (result == null || result.IsPartitionEOF)
                continue;

            TPOCO entity;
            Dictionary<string, string> headers;
            MessageMeta meta;
            try
            {
                entity = (TPOCO)mapping.CombineFromAvroKeyValue(result.Message.Key, result.Message.Value!, typeof(TPOCO));
                headers = new Dictionary<string, string>();
                if (result.Message.Headers != null)
                {
                    foreach (var h in result.Message.Headers)
                        headers[h.Key] = System.Text.Encoding.UTF8.GetString(h.GetValueBytes());
                }

                meta = new MessageMeta(
                    Topic: result.Topic,
                    Partition: result.Partition,
                    Offset: result.Offset,
                    TimestampUtc: result.Message.Timestamp.UtcDateTime,
                    SchemaIdKey: TryGetSchemaId(result.Message.Key as byte[]),
                    SchemaIdValue: TryGetSchemaId(result.Message.Value as byte[]),
                    KeyIsNull: result.Message.Key is null,
                    HeaderAllowList: ExtractAllowedHeaders(result.Message.Headers, _dlq.HeaderAllowList, _dlq.HeaderValueMaxLength)
                );
            }
            catch (Exception ex)
            {
                await HandleMappingException(result, ex, _dlqProducer, consumer, _dlq, _limiter, cancellationToken).ConfigureAwait(false);
                continue;
            }

            yield return (entity, headers, meta);
            await Task.CompletedTask;
        }
    }

    internal static async Task HandleMappingException<TKey, TValue>(
        ConsumeResult<TKey, TValue> result,
        Exception ex,
        IDlqProducer dlqProducer,
        IConsumer<TKey, TValue> consumer,
        DlqOptions options,
        IRateLimiter limiter,
        CancellationToken cancellationToken)
        where TKey : class where TValue : class
    {
        if (options.EnableForDeserializationError && DlqGuard.ShouldSend(options, limiter, ex.GetType()))
        {
            var allowHeaders = ExtractAllowedHeaders(result.Message.Headers, options.HeaderAllowList, options.HeaderValueMaxLength);
            var env = DlqEnvelopeFactory.From(result, ex,
                options.ApplicationId, options.ConsumerGroup, options.Host, allowHeaders,
                options.ErrorMessageMaxLength, options.StackTraceMaxLength, options.NormalizeStackTraceWhitespace);
            await dlqProducer.ProduceAsync(env, cancellationToken).ConfigureAwait(false);
        }
        consumer.Commit(result);
    }

    private IConsumer<TKey, TValue> CreateConsumer<TKey, TValue>(ConsumerConfig config)
        where TKey : class where TValue : class
    {
        return new ConsumerBuilder<TKey, TValue>(config)
            .SetKeyDeserializer(new AvroDeserializer<TKey>(_schemaRegistryClient.Value).AsSyncOverAsync())
            .SetValueDeserializer(new AvroDeserializer<TValue>(_schemaRegistryClient.Value).AsSyncOverAsync())
            .Build();
    }

    private ConfluentSchemaRegistry.ISchemaRegistryClient CreateSchemaRegistryClient()
    {
        var cfg = new ConfluentSchemaRegistry.SchemaRegistryConfig { Url = _options.SchemaRegistry.Url };
        return new ConfluentSchemaRegistry.CachedSchemaRegistryClient(cfg);
    }

    private EntityModel GetEntityModel<T>() where T : class
    {
        if (_entityModels.TryGetValue(typeof(T), out var model))
            return model;
        throw new InvalidOperationException($"Entity model not found for {typeof(T).Name}");
    }

    private ConsumerConfig BuildConsumerConfig(string topicName, KafkaSubscriptionOptions? subscriptionOptions)
    {
        var topicConfig = _options.Topics.TryGetValue(topicName, out var cfg) ? cfg : new TopicSection();
        var groupId = subscriptionOptions?.GroupId ?? topicConfig.Consumer.GroupId;
        if (string.IsNullOrWhiteSpace(groupId))
            groupId = "default-group";

        var consumerConfig = new ConsumerConfig
        {
            BootstrapServers = _options.Common.BootstrapServers,
            ClientId = _options.Common.ClientId,
            GroupId = groupId,
            AutoOffsetReset = Enum.Parse<AutoOffsetReset>(topicConfig.Consumer.AutoOffsetReset),
            EnableAutoCommit = topicConfig.Consumer.EnableAutoCommit,
            AutoCommitIntervalMs = topicConfig.Consumer.AutoCommitIntervalMs,
            SessionTimeoutMs = topicConfig.Consumer.SessionTimeoutMs,
            HeartbeatIntervalMs = topicConfig.Consumer.HeartbeatIntervalMs,
            MaxPollIntervalMs = topicConfig.Consumer.MaxPollIntervalMs,
            FetchMinBytes = topicConfig.Consumer.FetchMinBytes,
            FetchMaxBytes = topicConfig.Consumer.FetchMaxBytes,
            IsolationLevel = Enum.Parse<IsolationLevel>(topicConfig.Consumer.IsolationLevel)
        };

        if (subscriptionOptions != null)
        {
            if (subscriptionOptions.AutoCommit.HasValue)
                consumerConfig.EnableAutoCommit = subscriptionOptions.AutoCommit.Value;
            if (subscriptionOptions.SessionTimeout.HasValue)
                consumerConfig.SessionTimeoutMs = (int)subscriptionOptions.SessionTimeout.Value.TotalMilliseconds;
            if (subscriptionOptions.HeartbeatInterval.HasValue)
                consumerConfig.HeartbeatIntervalMs = (int)subscriptionOptions.HeartbeatInterval.Value.TotalMilliseconds;
            if (subscriptionOptions.MaxPollInterval.HasValue)
                consumerConfig.MaxPollIntervalMs = (int)subscriptionOptions.MaxPollInterval.Value.TotalMilliseconds;
        }

        if (_options.Common.SecurityProtocol != SecurityProtocol.Plaintext)
        {
            consumerConfig.SecurityProtocol = _options.Common.SecurityProtocol;
            if (_options.Common.SaslMechanism.HasValue)
            {
                consumerConfig.SaslMechanism = _options.Common.SaslMechanism.Value;
                consumerConfig.SaslUsername = _options.Common.SaslUsername;
                consumerConfig.SaslPassword = _options.Common.SaslPassword;
            }

            if (!string.IsNullOrEmpty(_options.Common.SslCaLocation))
            {
                consumerConfig.SslCaLocation = _options.Common.SslCaLocation;
                consumerConfig.SslCertificateLocation = _options.Common.SslCertificateLocation;
                consumerConfig.SslKeyLocation = _options.Common.SslKeyLocation;
                consumerConfig.SslKeyPassword = _options.Common.SslKeyPassword;
            }
        }

        foreach (var kvp in topicConfig.Consumer.AdditionalProperties)
            consumerConfig.Set(kvp.Key, kvp.Value);

        return consumerConfig;
    }

    public void ResetOffsetsToBeginning<TPOCO>() where TPOCO : class
    {
        var model = GetEntityModel<TPOCO>();
        var topic = model.GetTopicName();
        var mapping = _mappingRegistry.GetMapping(typeof(TPOCO));
        var config = BuildConsumerConfig(topic, null);
        config.EnableAutoCommit = false;
        var method = typeof(KafkaConsumerManager)
            .GetMethod(nameof(ResetOffsetsToBeginningInternal), System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance)!
            .MakeGenericMethod(mapping.AvroKeyType!, mapping.AvroValueType!);
        method.Invoke(this, new object?[] { topic, config, model.Partitions });
    }

    private void ResetOffsetsToBeginningInternal<TKey, TValue>(string topic, ConsumerConfig config, int partitions)
        where TKey : class where TValue : class
    {
        using var consumer = CreateConsumer<TKey, TValue>(config);
        var tps = new List<TopicPartitionOffset>(partitions);
        for (var i = 0; i < partitions; i++)
            tps.Add(new TopicPartitionOffset(topic, new Partition(i), new Offset(0)));
        consumer.Assign(tps);
        consumer.Commit(tps);
    }

    public void Dispose()
    {
        if (_disposed) return;
        if (_schemaRegistryClient.IsValueCreated)
            _schemaRegistryClient.Value.Dispose();
        _disposed = true;
    }

    private static int? TryGetSchemaId(byte[]? payload)
    {
        if (payload is { Length: >= 5 } && payload[0] == 0)
            return System.Buffers.Binary.BinaryPrimitives.ReadInt32BigEndian(payload.AsSpan(1, 4));
        return null;
    }

    private static System.Collections.Generic.IReadOnlyDictionary<string, string> ExtractAllowedHeaders(
        Headers? headers, System.Collections.Generic.IEnumerable<string> allowList, int maxLen = 1024)
    {
        var dict = new System.Collections.Generic.Dictionary<string, string>(System.StringComparer.OrdinalIgnoreCase);
        if (headers is null) return dict;

        var set = allowList is System.Collections.Generic.HashSet<string> hs ? hs :
                  new System.Collections.Generic.HashSet<string>(allowList ?? System.Array.Empty<string>(), System.StringComparer.OrdinalIgnoreCase);

        foreach (var h in headers)
        {
            if (!set.Contains(h.Key)) continue;
            var bytes = h.GetValueBytes() ?? System.Array.Empty<byte>();
            string val;
            try
            {
                val = System.Text.Encoding.UTF8.GetString(bytes);
                if (!System.Text.Encoding.UTF8.GetBytes(val).AsSpan().SequenceEqual(bytes))
                    val = "base64:" + System.Convert.ToBase64String(bytes);
            }
            catch
            {
                val = "base64:" + System.Convert.ToBase64String(bytes);
            }
            if (val.Length > maxLen) val = val[..maxLen];
            dict[h.Key] = val;
        }
        return dict;
    }
}
### tool — 2025-08-11T08:31:47.978117+00:00

fileciteturn1file2

namespace Kafka.Ksql.Linq.Messaging.Consumers;

internal class NoopCommitManager : ICommitManager
{
    public void Commit(object entity) { }
}
### user — 2025-08-11T08:31:47.982000+00:00

既存のコードを利用する形でお願いします
### tool — 2025-08-11T08:31:47.991503+00:00

fileciteturn1file3

using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Messaging.Internal;
using Kafka.Ksql.Linq.Messaging;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Query.Abstractions;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;

namespace Kafka.Ksql.Linq;

/// <summary>
/// Base class for EventSet implementing IEntitySet<T>
/// Reason for modification: unified with KsqlContext and added IEntitySet<T> implementation
/// </summary>
public abstract class EventSet<T> : IEntitySet<T> where T : class
{
    protected readonly IKsqlContext _context;
    protected readonly EntityModel _entityModel;
    private readonly ErrorHandlingContext _errorHandlingContext;
    private IErrorSink? _dlqErrorSink;
    private readonly Messaging.Producers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (context is KsqlContext kctx)
        {
            _entityModel = kctx.EnsureEntityModel(typeof(T), entityModel);
        }
        else
        {
            _entityModel = entityModel ?? throw new ArgumentNullException(nameof(entityModel));
        }

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorHandlingContext, IErrorSink? dlqErrorSink,
        Messaging.Producers.IDlqProducer? dlqProducer, Messaging.Consumers.ICommitManager? commitManager)
    {
        _context = context;
        _entityModel = entityModel;
        _errorHandlingContext = errorHandlingContext;
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    /// <summary>
    /// NEW: made abstract - must be implemented by concrete classes
    /// Unifies continuous Kafka consumption and returning a fixed list
    /// </summary>
    public abstract IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default);

    private async IAsyncEnumerable<T> GetAsyncEnumeratorWrapper([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await using var enumerator = GetAsyncEnumerator(cancellationToken);

        while (true)
        {
            bool hasNext;
            try
            {
                hasNext = await enumerator.MoveNextAsync();
            }
            catch (Exception ex)
            {
                var ctx = new KafkaMessageContext
                {
                    MessageId = Guid.NewGuid().ToString(),
                    Tags = new Dictionary<string, object>
                    {
                        ["processing_phase"] = "ForEachAsync"
                    }
                };

                var shouldContinue = await _errorHandlingContext.HandleErrorAsync(default(T)!, ex, ctx);

                if (!shouldContinue)
                {
                    continue;
                }

                throw;
            }

            if (!hasNext)
                yield break;

            yield return enumerator.Current;
        }
    }


    public virtual async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.EntityType == typeof(Messaging.DlqEnvelope))
            throw new InvalidOperationException("DLQは無限列挙/履歴列であり、バッチ取得・件数指定取得は現状未対応です");

        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        var results = new List<T>();

        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            results.Add(item);
        }

        return results;
    }
    /// <summary>
    /// ABSTRACT: Producer functionality - implemented in derived classes
    /// </summary>
    protected abstract Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken);

    /// <summary>
    /// IEntitySet<T> implementation: producer operations
    /// </summary>
    public virtual async Task AddAsync(T entity, Dictionary<string, string>? headers = null, CancellationToken cancellationToken = default)
    {
        if (entity == null)
            throw new ArgumentNullException(nameof(entity));

        await SendEntityAsync(entity, headers, cancellationToken);
    }

    public virtual Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"RemoveAsync is not supported for {GetType().Name}.");
    }

    /// <summary>
    /// Retrieves messages from the underlying consumer.
    /// Separated for ease of testing.
    /// </summary>
    /// <param name="context">Active KsqlContext</param>
    /// <param name="cancellationToken">Cancellation token</param>
    /// <returns>Message stream with headers</returns>
    protected virtual IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> ConsumeAsync(
        KsqlContext context,
        CancellationToken cancellationToken)
    {
        return context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
    }
    /// <summary>
    /// REDESIGNED: ForEachAsync supporting continuous Kafka consumption
    /// Design change: ToListAsync() is disallowed; now based on GetAsyncEnumerator
    /// </summary>
    public virtual Task ForEachAsync(Func<T, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        return ForEachAsync((item, headers, _) => action(item), timeout, cancellationToken);
    }

    [Obsolete("Use ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task>)")]
    public virtual Task ForEachAsync(Func<T, Dictionary<string,string>, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
        => ForEachAsync((e, h, _) => action(e, h), timeout, cancellationToken);

    public virtual async Task ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        var context = GetContext() as KsqlContext
            ?? throw new InvalidOperationException("KsqlContext is required");

        using var linkedCts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);

        if (timeout != default && timeout != TimeSpan.Zero)
        {
            linkedCts.CancelAfter(timeout);
        }
        await foreach (var (entity, headers, meta) in ConsumeAsync(context, linkedCts.Token))
        {
            if (headers.TryGetValue("is_dummy", out var dummyHeader) && bool.TryParse(dummyHeader, out var isDummy) && isDummy)
            {
                continue;
            }

            var maxAttempts = _errorHandlingContext.ErrorAction == ErrorAction.Retry
                ? _errorHandlingContext.RetryCount + 1
                : 1;

            for (var attempt = 1; attempt <= maxAttempts; attempt++)
            {
                try
                {
                    await action(entity, headers, meta);
                    break;
                }
                catch (Exception ex)
                {
                    _errorHandlingContext.CurrentAttempt = attempt;

                    if (attempt < maxAttempts && _errorHandlingContext.ErrorAction == ErrorAction.Retry)
                    {
                        await Task.Delay(_errorHandlingContext.RetryInterval, linkedCts.Token);
                        continue;
                    }

                    var dlq = context.DlqOptions;
                    if (_dlqProducer != null && dlq.EnableForHandlerError && DlqGuard.ShouldSend(dlq, context.DlqLimiter, ex.GetType()))
                    {
                        var env = DlqEnvelopeFactory.From(
                            meta, ex,
                            dlq.ApplicationId, dlq.ConsumerGroup, dlq.Host,
                            dlq.ErrorMessageMaxLength, dlq.StackTraceMaxLength, dlq.NormalizeStackTraceWhitespace);
                        await _dlqProducer.ProduceAsync(env, linkedCts.Token).ConfigureAwait(false);
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IEntitySet<T> implementation: retrieve metadata
    /// </summary>
    public string GetTopicName() => (_entityModel.TopicName ?? _entityModel.EntityType.Name).ToLowerInvariant();

    public EntityModel GetEntityModel() => _entityModel;

    public IKsqlContext GetContext() => _context;

    /// <summary>
    /// Create message context for error handling
    /// </summary>
    private KafkaMessageContext CreateMessageContext(T item)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["entity_type"] = typeof(T).Name,
                ["topic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }

    /// <summary>
    /// Configure the error handling policy
    /// </summary>
    internal virtual EventSet<T> WithErrorPolicy(ErrorHandlingPolicy policy)
    {
        if (policy == null)
            throw new ArgumentNullException(nameof(policy));

        _errorHandlingContext.ErrorAction = policy.Action;
        _errorHandlingContext.RetryCount = policy.RetryCount;
        _errorHandlingContext.RetryInterval = policy.RetryInterval;
        _errorHandlingContext.CustomHandler = policy.CustomHandler;

        return this;
    }

    public override string ToString()
    {
        return $"EventSet<{typeof(T).Name}> - Topic: {GetTopicName()}";
    }



    /// <summary>
    /// Specifies the number of retries.
    /// Used when ErrorAction.Retry is selected.
    /// </summary>
    /// <param name="maxRetries">Maximum retry count</param>
    /// <param name="retryInterval">Retry interval (optional)</param>
    /// <returns>EventSet with retry configuration applied</returns>
    public EventSet<T> WithRetry(int maxRetries, TimeSpan? retryInterval = null)
    {
        if (maxRetries < 0)
            throw new ArgumentException("Retry count must be zero or greater", nameof(maxRetries));

        var newContext = new ErrorHandlingContext
        {
            ErrorAction = _errorHandlingContext.ErrorAction,
            RetryCount = maxRetries,
            RetryInterval = retryInterval ?? TimeSpan.FromSeconds(1)
        };

        return CreateNewInstance(_context, _entityModel, newContext, _dlqErrorSink);
    }

    /// <summary>
    /// Passes the POCO to the business logic.
    /// After receiving from Kafka, each element is transformed using the supplied function.
    /// Exceptions and retries are handled based on the OnError and WithRetry settings.
    /// </summary>
    /// <typeparam name="TResult">Result type</typeparam>
    /// <param name="mapper">Mapping function</param>
    /// <returns>The mapped EventSet</returns>
    public async Task<EventSet<TResult>> Map<TResult>(Func<T, Task<TResult>> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = await ToListAsync();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            await ProcessItemWithErrorHandling(
                item,
                mapper,
                results,
                itemErrorContext);
        }

        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    /// <summary>
    /// Synchronous version of the Map function
    /// </summary>
    public EventSet<TResult> Map<TResult>(Func<T, TResult> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = ToListAsync().GetAwaiter().GetResult();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            ProcessItemWithErrorHandlingSync(
                item,
                mapper,
                results,
                itemErrorContext);
        }
        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    // Abstract method: create a new instance in derived classes
    protected virtual EventSet<T> CreateNewInstance(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorContext, IErrorSink? dlqErrorSink)
    {
        // Default implementation: concrete classes must override
        throw new NotImplementedException("Derived classes must implement CreateNewInstance");
    }

    private EntityModel CreateEntityModelForType<TResult>() where TResult : class
    {
        return new EntityModel
        {
            EntityType = typeof(TResult),
            TopicName = $"{typeof(TResult).Name.ToLowerInvariant()}_mapped",
            AllProperties = typeof(TResult).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(),
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    /// <summary>
    /// Item-level processing with error handling (async version)
    /// </summary>
    private async Task ProcessItemWithErrorHandling<TResult>(
        T item,
        Func<T, Task<TResult>> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = await mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    await Task.Delay(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = await errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext));

                if (!shouldContinue)
                {
                    return; // Skip this item and move to the next
                }
            }
        }
    }

    /// <summary>
    /// Item-level processing with error handling (sync version)
    /// </summary>
    private void ProcessItemWithErrorHandlingSync<TResult>(
        T item,
        Func<T, TResult> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    Thread.Sleep(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext)).GetAwaiter().GetResult();

                if (!shouldContinue)
                {
                    return; // Skip this item and proceed to the next
                }
            }
        }
    }

    /// <summary>
    /// Create a message context
    /// </summary>
    private KafkaMessageContext CreateContext(T item, ErrorHandlingContext errorContext)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["original_topic"] = GetTopicName(),
                ["original_partition"] = 0, // Replace with actual value
                ["original_offset"] = 0, // Replace with actual value
                ["retry_count"] = errorContext.CurrentAttempt,
                ["error_phase"] = "Processing"
            }
        };
    }

}
internal class MappedEventSet<T> : EventSet<T> where T : class
{
    private readonly List<T> _mapped;
    private readonly EntityModel _originalEntityModel;

    public MappedEventSet(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
        : base(context, CreateMappedEntityModel<T>(originalEntityModel), errorSink)
    {
        _mapped = mappedItems ?? throw new ArgumentNullException(nameof(mappedItems));
        _originalEntityModel = originalEntityModel;
    }

    /// <summary>
    /// NEW: GetAsyncEnumerator implementation for fixed lists
    /// Returns each _mapped[i] sequentially via yield return
    /// </summary>
    public override async IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default)
    {
        foreach (var item in _mapped)
        {
            if (cancellationToken.IsCancellationRequested)
                yield break;

            yield return item;

            // Inserted to treat the loop asynchronously (avoid CPU intensive work)
            await Task.Yield();
        }
    }

    /// <summary>
    /// OPTIMIZATION: ToListAsync - already a fixed list so return immediately
    /// </summary>
    public override async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        // Already a fixed list; return a copy
        await Task.CompletedTask;
        return new List<T>(_mapped);
    }

    /// <summary>
    /// Data after Map cannot be sent via Producer
    /// </summary>
    protected override Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken)
    {
        throw new NotSupportedException(
            $"MappedEventSet<{typeof(T).Name}> does not support AddAsync operations. " +
            "Mapped data is read-only and derived from transformation operations.");
    }

    public override Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"MappedEventSet<{typeof(T).Name}> does not support RemoveAsync operations.");
    }

    /// <summary>
    /// Helper method to create a MappedEventSet
    /// </summary>
    public static MappedEventSet<T> Create(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, errorSink);
    }

    /// <summary>
    /// Create a MappedEventSet with DLQ support
    /// </summary>
    public static MappedEventSet<T> CreateWithDlq(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink dlqErrorSink)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, dlqErrorSink);
    }

    /// <summary>
    /// Create an EntityModel for mapped data
    /// </summary>
    private static EntityModel CreateMappedEntityModel<TMapped>(EntityModel originalModel) where TMapped : class
    {
        return new EntityModel
        {
            EntityType = typeof(TMapped),
            TopicName = $"{originalModel.GetTopicName()}_mapped",
            AllProperties = typeof(TMapped).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(), // No key after mapping
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    public override string ToString()
    {
        return $"MappedEventSet<{typeof(T).Name}> - Items: {_mapped.Count}";
    }
}
### tool — 2025-08-11T08:31:48.004250+00:00

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.
### assistant — 2025-08-11T08:31:49.606016+00:00

_(empty)_
### assistant — 2025-08-11T08:32:05.693919+00:00

_(empty)_
### user — 2025-08-11T09:35:01.742000+00:00

public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }
戻り値がobject　これをpocoとしたい
### assistant — 2025-08-11T09:35:14.570187+00:00

import os, textwrap, json, pathlib, sys, re, itertools, math
### system — 2025-08-11T09:35:14.988974+00:00

_(empty)_
### assistant — 2025-08-11T09:35:15.017523+00:00

_(empty)_
### assistant — 2025-08-11T09:35:15.595810+00:00

_(empty)_
### tool — 2025-08-11T09:35:27.578165+00:00

Code execution state reset.
### assistant — 2025-08-11T09:35:27.779911+00:00

from pathlib import Path, PurePosixPath
base=Path('/mnt/data')
list(base.iterdir())
### tool — 2025-08-11T09:35:29.492795+00:00

[PosixPath('/mnt/data/ICommitManager.cs'),
 PosixPath('/mnt/data/KafkaConsumerManager.cs'),
 PosixPath('/mnt/data/EventSet.cs'),
 PosixPath('/mnt/data/NoopCommitManager.cs')]
### assistant — 2025-08-11T09:35:30.593121+00:00

print(Path('/mnt/data/EventSet.cs').read_text()[:2000])
### tool — 2025-08-11T09:35:33.293034+00:00

using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Messaging.Internal;
using Kafka.Ksql.Linq.Messaging;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Query.Abstractions;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;

namespace Kafka.Ksql.Linq;

/// <summary>
/// Base class for EventSet implementing IEntitySet<T>
/// Reason for modification: unified with KsqlContext and added IEntitySet<T> implementation
/// </summary>
public abstract class EventSet<T> : IEntitySet<T> where T : class
{
    protected readonly IKsqlContext _context;
    protected readonly EntityModel _entityModel;
    private readonly ErrorHandlingContext _errorHandlingContext;
    private IErrorSink? _dlqErrorSink;
    private readonly Messaging.Producers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (context is KsqlContext kctx)
        {
            _entityModel = kctx.EnsureEntityModel(typeof(T), entityModel);
        }
        else
        {
            _entityModel = entityModel ?? throw new ArgumentNullException(nameof(entityModel));
        }

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHandlingContex
### assistant — 2025-08-11T09:35:34.435172+00:00

print(Path('/mnt/data/EventSet.cs').read_text())
### tool — 2025-08-11T09:35:36.132866+00:00

if (!shouldContinue)
                {
                    continue;
                }

                throw;
            }

            if (!hasNext)
                yield break;

            yield return enumerator.Current;
        }
    }


    public virtual async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.EntityType == typeof(Messaging.DlqEnvelope))
            throw new InvalidOperationException("DLQは無限列挙/履歴列であり、バッチ取得・件数指定取得は現状未対応です");

        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        var results = new List<T>();

        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            results.Add(item);
        }

        return results;
    }
    /// <summary>
    /// ABSTRACT: Producer functionality - implemented in derived classes
    /// </summary>
    protected abstract Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken);

    /// <summary>
    /// IEntitySet<T> implementation: producer operations
    /// </summary>
    public virtual async Task AddAsync(T entity, Dictionary<string, string>? headers = null, CancellationToken cancellationToken = default)
    {
        if (entity == null)
            throw new ArgumentNullException(nameof(entity));

        await SendEntityAsync(entity, headers, cancellationToken);
    }

    public virtual Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"RemoveAsync is not supported for {GetType().Name}.");
    }

    /// <summary>
    /// Retrieves messages from the underlying consumer.
    /// Separated for ease of testing.
    /// </summary>
    /// <param name="context">Active KsqlContext</param>
    /// <param name="cancellationToken">Cancellation token</param>
    /// <returns>Message stream with headers</returns>
    protected virtual IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> ConsumeAsync(
        KsqlContext context,
        CancellationToken cancellationToken)
    {
        return context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
    }
    /// <summary>
    /// REDESIGNED: ForEachAsync supporting continuous Kafka consumption
    /// Design change: ToListAsync() is disallowed; now based on GetAsyncEnumerator
    /// </summary>
    public virtual Task ForEachAsync(Func<T, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        return ForEachAsync((item, headers, _) => action(item), timeout, cancellationToken);
    }

    [Obsolete("Use ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task>)")]
    public virtual Task ForEachAsync(Func<T, Dictionary<string,string>, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
        => ForEachAsync((e, h, _) => action(e, h), timeout, cancellationToken);

    public virtual async Task ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        var context = GetContext() as KsqlContext
            ?? throw new InvalidOperationException("KsqlContext is required");

        using var linkedCts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);

        if (timeout != default && timeout != TimeSpan.Zero)
        {
            linkedCts.CancelAfter(timeout);
        }
        await foreach (var (entity, headers, meta) in ConsumeAsync(context, linkedCts.Token))
        {
            if (headers.TryGetValue("is_dummy", out var dummyHeader) && bool.TryParse(dummyHeader, out var isDummy) && isDummy)
            {
                continue;
            }

            var maxAttempts = _errorHandlingContext.ErrorAction == ErrorAction.Retry
                ? _errorHandlingContext.RetryCount + 1
                : 1;

            for (var attempt = 1; attempt <= maxAttempts; attempt++)
            {
                try
                {
                    await action(entity, headers, meta);
                    break;
                }
                catch (Exception ex)
                {
                    _errorHandlingContext.CurrentAttempt = attempt;

                    if (attempt < maxAttempts && _errorHandlingContext.ErrorAction == ErrorAction.Retry)
                    {
                        await Task.Delay(_errorHandlingContext.RetryInterval, linkedCts.Token);
                        continue;
                    }

                    var dlq = context.DlqOptions;
                    if (_dlqProducer != null && dlq.EnableForHandlerError && DlqGuard.ShouldSend(dlq, context.DlqLimiter, ex.GetType()))
                    {
                        var env = DlqEnvelopeFactory.From(
                            meta, ex,
                            dlq.ApplicationId, dlq.ConsumerGroup, dlq.Host,
                            dlq.ErrorMessageMaxLength, dlq.StackTraceMaxLength, dlq.NormalizeStackTraceWhitespace);
                        await _dlqProducer.ProduceAsync(env, linkedCts.Token).ConfigureAwait(false);
                    }

                    _commitManager?.Commit(entity);
                    break;
                }
            }
        }
    }

    /// <summary>
    /// IEntitySet<T> implementation: retrieve metadata
    /// </summary>
    public string GetTopicName() => (_entityModel.TopicName ?? _entityModel.EntityType.Name).ToLowerInvariant();

    public EntityModel GetEntityModel() => _entityModel;

    public IKsqlContext GetContext() => _context;

    /// <summary>
    /// Create message context for error handling
    /// </summary>
    private KafkaMessageContext CreateMessageContext(T item)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["entity_type"] = typeof(T).Name,
                ["topic_name"] = GetTopicName(),
                ["processing_phase"] = "ForEachAsync",
                ["timestamp"] = DateTime.UtcNow
            }
        };
    }

    /// <summary>
    /// Creates a ManualCommitMessage instance
    /// </summary>
    protected virtual IManualCommitMessage<T> CreateManualCommitMessage(T item)
    {
        return new ManualCommitMessage<T>(item, () => Task.CompletedTask, () => Task.CompletedTask);
    }

    /// <summary>
    /// Yields messages as different types depending on UseManualCommit
    /// </summary>
    public virtual async IAsyncEnumerable<object> ForEachAsync([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            if (_entityModel.UseManualCommit)
            {
                yield return CreateManualCommitMessage(item);
            }
            else
            {
                yield return item;
            }
        }
    }

    /// <summary>
    /// Configure the error handling policy
    /// </summary>
    internal virtual EventSet<T> WithErrorPolicy(ErrorHandlingPolicy policy)
    {
        if (policy == null)
            throw new ArgumentNullException(nameof(policy));

        _errorHandlingContext.ErrorAction = policy.Action;
        _errorHandlingContext.RetryCount = policy.RetryCount;
        _errorHandlingContext.RetryInterval = policy.RetryInterval;
        _errorHandlingContext.CustomHandler = policy.CustomHandler;

        return this;
    }

    public override string ToString()
    {
        return $"EventSet<{typeof(T).Name}> - Topic: {GetTopicName()}";
    }



    /// <summary>
    /// Specifies the number of retries.
    /// Used when ErrorAction.Retry is selected.
    /// </summary>
    /// <param name="maxRetries">Maximum retry count</param>
    /// <param name="retryInterval">Retry interval (optional)</param>
    /// <returns>EventSet with retry configuration applied</returns>
    public EventSet<T> WithRetry(int maxRetries, TimeSpan? retryInterval = null)
    {
        if (maxRetries < 0)
            throw new ArgumentException("Retry count must be zero or greater", nameof(maxRetries));

        var newContext = new ErrorHandlingContext
        {
            ErrorAction = _errorHandlingContext.ErrorAction,
            RetryCount = maxRetries,
            RetryInterval = retryInterval ?? TimeSpan.FromSeconds(1)
        };

        return CreateNewInstance(_context, _entityModel, newContext, _dlqErrorSink);
    }

    /// <summary>
    /// Passes the POCO to the business logic.
    /// After receiving from Kafka, each element is transformed using the supplied function.
    /// Exceptions and retries are handled based on the OnError and WithRetry settings.
    /// </summary>
    /// <typeparam name="TResult">Result type</typeparam>
    /// <param name="mapper">Mapping function</param>
    /// <returns>The mapped EventSet</returns>
    public async Task<EventSet<TResult>> Map<TResult>(Func<T, Task<TResult>> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = await ToListAsync();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            await ProcessItemWithErrorHandling(
                item,
                mapper,
                results,
                itemErrorContext);
        }

        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    /// <summary>
    /// Synchronous version of the Map function
    /// </summary>
    public EventSet<TResult> Map<TResult>(Func<T, TResult> mapper) where TResult : class
    {
        if (mapper == null)
            throw new ArgumentNullException(nameof(mapper));

        var results = new List<TResult>();
        var sourceData = ToListAsync().GetAwaiter().GetResult();

        foreach (var item in sourceData)
        {
            var itemErrorContext = new ErrorHandlingContext
            {
                ErrorAction = _errorHandlingContext.ErrorAction,
                RetryCount = _errorHandlingContext.RetryCount,
                RetryInterval = _errorHandlingContext.RetryInterval
            };

            ProcessItemWithErrorHandlingSync(
                item,
                mapper,
                results,
                itemErrorContext);
        }
        var resultEntityModel = CreateEntityModelForType<TResult>();
        return new MappedEventSet<TResult>(results, _context, resultEntityModel, _dlqErrorSink);
    }

    // Abstract method: create a new instance in derived classes
    protected virtual EventSet<T> CreateNewInstance(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorContext, IErrorSink? dlqErrorSink)
    {
        // Default implementation: concrete classes must override
        throw new NotImplementedException("Derived classes must implement CreateNewInstance");
    }

    private EntityModel CreateEntityModelForType<TResult>() where TResult : class
    {
        return new EntityModel
        {
            EntityType = typeof(TResult),
            TopicName = $"{typeof(TResult).Name.ToLowerInvariant()}_mapped",
            AllProperties = typeof(TResult).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(),
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    /// <summary>
    /// Item-level processing with error handling (async version)
    /// </summary>
    private async Task ProcessItemWithErrorHandling<TResult>(
        T item,
        Func<T, Task<TResult>> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = await mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    await Task.Delay(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = await errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext));

                if (!shouldContinue)
                {
                    return; // Skip this item and move to the next
                }
            }
        }
    }

    /// <summary>
    /// Item-level processing with error handling (sync version)
    /// </summary>
    private void ProcessItemWithErrorHandlingSync<TResult>(
        T item,
        Func<T, TResult> mapper,
        List<TResult> results,
        ErrorHandlingContext errorContext) where TResult : class
    {
        var maxAttempts = errorContext.ErrorAction == ErrorAction.Retry
            ? errorContext.RetryCount + 1
            : 1;

        for (int attempt = 1; attempt <= maxAttempts; attempt++)
        {
            try
            {
                var result = mapper(item);
                results.Add(result);
                return; // Processing completed successfully
            }
            catch (Exception ex)
            {
                errorContext.CurrentAttempt = attempt;

                // Retry regardless of ErrorAction if this is not the final attempt
                if (attempt < maxAttempts && errorContext.ErrorAction == ErrorAction.Retry)
                {
                    Console.WriteLine($"[{DateTime.UtcNow:HH:mm:ss}] Retry {attempt}/{errorContext.RetryCount}: {ex.Message}");
                    Thread.Sleep(errorContext.RetryInterval);
                    continue;
                }

                // Perform error handling on the last attempt or when not retrying
                var shouldContinue = errorContext.HandleErrorAsync(item, ex, CreateContext(item, errorContext)).GetAwaiter().GetResult();

                if (!shouldContinue)
                {
                    return; // Skip this item and proceed to the next
                }
            }
        }
    }

    /// <summary>
    /// Create a message context
    /// </summary>
    private KafkaMessageContext CreateContext(T item, ErrorHandlingContext errorContext)
    {
        return new KafkaMessageContext
        {
            MessageId = Guid.NewGuid().ToString(),
            Tags = new Dictionary<string, object>
            {
                ["original_topic"] = GetTopicName(),
                ["original_partition"] = 0, // Replace with actual value
                ["original_offset"] = 0, // Replace with actual value
                ["retry_count"] = errorContext.CurrentAttempt,
                ["error_phase"] = "Processing"
            }
        };
    }

}
internal class MappedEventSet<T> : EventSet<T> where T : class
{
    private readonly List<T> _mapped;
    private readonly EntityModel _originalEntityModel;

    public MappedEventSet(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
        : base(context, CreateMappedEntityModel<T>(originalEntityModel), errorSink)
    {
        _mapped = mappedItems ?? throw new ArgumentNullException(nameof(mappedItems));
        _originalEntityModel = originalEntityModel;
    }

    /// <summary>
    /// NEW: GetAsyncEnumerator implementation for fixed lists
    /// Returns each _mapped[i] sequentially via yield return
    /// </summary>
    public override async IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default)
    {
        foreach (var item in _mapped)
        {
            if (cancellationToken.IsCancellationRequested)
                yield break;

            yield return item;

            // Inserted to treat the loop asynchronously (avoid CPU intensive work)
            await Task.Yield();
        }
    }

    /// <summary>
    /// OPTIMIZATION: ToListAsync - already a fixed list so return immediately
    /// </summary>
    public override async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        // Already a fixed list; return a copy
        await Task.CompletedTask;
        return new List<T>(_mapped);
    }

    /// <summary>
    /// Data after Map cannot be sent via Producer
    /// </summary>
    protected override Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken)
    {
        throw new NotSupportedException(
            $"MappedEventSet<{typeof(T).Name}> does not support AddAsync operations. " +
            "Mapped data is read-only and derived from transformation operations.");
    }

    public override Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"MappedEventSet<{typeof(T).Name}> does not support RemoveAsync operations.");
    }

    /// <summary>
    /// Helper method to create a MappedEventSet
    /// </summary>
    public static MappedEventSet<T> Create(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink? errorSink = null)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, errorSink);
    }

    /// <summary>
    /// Create a MappedEventSet with DLQ support
    /// </summary>
    public static MappedEventSet<T> CreateWithDlq(List<T> mappedItems, IKsqlContext context, EntityModel originalEntityModel, IErrorSink dlqErrorSink)
    {
        return new MappedEventSet<T>(mappedItems, context, originalEntityModel, dlqErrorSink);
    }

    /// <summary>
    /// Create an EntityModel for mapped data
    /// </summary>
    private static EntityModel CreateMappedEntityModel<TMapped>(EntityModel originalModel) where TMapped : class
    {
        return new EntityModel
        {
            EntityType = typeof(TMapped),
            TopicName = $"{originalModel.GetTopicName()}_mapped",
            AllProperties = typeof(TMapped).GetProperties(),
            KeyProperties = Array.Empty<System.Reflection.PropertyInfo>(), // No key after mapping
            ValidationResult = new ValidationResult { IsValid = true }
        };
    }

    public override string ToString()
    {
        return $"MappedEventSet<{typeof(T).Name}> - Items: {_mapped.Count}";
    }
}
### assistant — 2025-08-11T09:35:37.401509+00:00

text=Path('/mnt/data/EventSet.cs').read_text()
lines=text.splitlines()
print(len(lines))
print('\n'.join(lines[:200]))
### tool — 2025-08-11T09:35:39.142158+00:00

624
using Kafka.Ksql.Linq.Core.Abstractions;
using Kafka.Ksql.Linq.Core.Extensions;
using Kafka.Ksql.Linq.Messaging.Internal;
using Kafka.Ksql.Linq.Messaging;
using Kafka.Ksql.Linq.Core.Dlq;
using Kafka.Ksql.Linq.Query.Abstractions;
using System;
using System.Collections.Generic;
using System.Runtime.CompilerServices;
using System.Threading;
using System.Threading.Tasks;

namespace Kafka.Ksql.Linq;

/// <summary>
/// Base class for EventSet implementing IEntitySet<T>
/// Reason for modification: unified with KsqlContext and added IEntitySet<T> implementation
/// </summary>
public abstract class EventSet<T> : IEntitySet<T> where T : class
{
    protected readonly IKsqlContext _context;
    protected readonly EntityModel _entityModel;
    private readonly ErrorHandlingContext _errorHandlingContext;
    private IErrorSink? _dlqErrorSink;
    private readonly Messaging.Producers.IDlqProducer? _dlqProducer;
    private readonly Messaging.Consumers.ICommitManager? _commitManager;

    protected EventSet(IKsqlContext context, EntityModel? entityModel = null, IErrorSink? dlqErrorSink = null,
        Messaging.Producers.IDlqProducer? dlqProducer = null, Messaging.Consumers.ICommitManager? commitManager = null)
    {
        _context = context ?? throw new ArgumentNullException(nameof(context));
        _errorHandlingContext = new ErrorHandlingContext();
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (context is KsqlContext kctx)
        {
            _entityModel = kctx.EnsureEntityModel(typeof(T), entityModel);
        }
        else
        {
            _entityModel = entityModel ?? throw new ArgumentNullException(nameof(entityModel));
        }

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    private EventSet(IKsqlContext context, EntityModel entityModel, ErrorHandlingContext errorHandlingContext, IErrorSink? dlqErrorSink,
        Messaging.Producers.IDlqProducer? dlqProducer, Messaging.Consumers.ICommitManager? commitManager)
    {
        _context = context;
        _entityModel = entityModel;
        _errorHandlingContext = errorHandlingContext;
        _dlqErrorSink = dlqErrorSink;
        _dlqProducer = dlqProducer;
        _commitManager = commitManager;

        if (_dlqErrorSink != null)
        {
            _errorHandlingContext.ErrorOccurred += (ctx, msg) => _dlqErrorSink.HandleErrorAsync(ctx, msg);
        }
    }

    /// <summary>
    /// NEW: made abstract - must be implemented by concrete classes
    /// Unifies continuous Kafka consumption and returning a fixed list
    /// </summary>
    public abstract IAsyncEnumerator<T> GetAsyncEnumerator(CancellationToken cancellationToken = default);

    private async IAsyncEnumerable<T> GetAsyncEnumeratorWrapper([EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        await using var enumerator = GetAsyncEnumerator(cancellationToken);

        while (true)
        {
            bool hasNext;
            try
            {
                hasNext = await enumerator.MoveNextAsync();
            }
            catch (Exception ex)
            {
                var ctx = new KafkaMessageContext
                {
                    MessageId = Guid.NewGuid().ToString(),
                    Tags = new Dictionary<string, object>
                    {
                        ["processing_phase"] = "ForEachAsync"
                    }
                };

                var shouldContinue = await _errorHandlingContext.HandleErrorAsync(default(T)!, ex, ctx);

                if (!shouldContinue)
                {
                    continue;
                }

                throw;
            }

            if (!hasNext)
                yield break;

            yield return enumerator.Current;
        }
    }


    public virtual async Task<List<T>> ToListAsync(CancellationToken cancellationToken = default)
    {
        if (_entityModel.EntityType == typeof(Messaging.DlqEnvelope))
            throw new InvalidOperationException("DLQは無限列挙/履歴列であり、バッチ取得・件数指定取得は現状未対応です");

        if (_entityModel.GetExplicitStreamTableType() == StreamTableType.Stream)
            throw new InvalidOperationException("ToListAsync() is not supported on a Stream source. Use ForEachAsync or subscribe for event consumption.");

        var results = new List<T>();

        await foreach (var item in GetAsyncEnumeratorWrapper(cancellationToken))
        {
            results.Add(item);
        }

        return results;
    }
    /// <summary>
    /// ABSTRACT: Producer functionality - implemented in derived classes
    /// </summary>
    protected abstract Task SendEntityAsync(T entity, Dictionary<string, string>? headers, CancellationToken cancellationToken);

    /// <summary>
    /// IEntitySet<T> implementation: producer operations
    /// </summary>
    public virtual async Task AddAsync(T entity, Dictionary<string, string>? headers = null, CancellationToken cancellationToken = default)
    {
        if (entity == null)
            throw new ArgumentNullException(nameof(entity));

        await SendEntityAsync(entity, headers, cancellationToken);
    }

    public virtual Task RemoveAsync(T entity, CancellationToken cancellationToken = default)
    {
        throw new NotSupportedException($"RemoveAsync is not supported for {GetType().Name}.");
    }

    /// <summary>
    /// Retrieves messages from the underlying consumer.
    /// Separated for ease of testing.
    /// </summary>
    /// <param name="context">Active KsqlContext</param>
    /// <param name="cancellationToken">Cancellation token</param>
    /// <returns>Message stream with headers</returns>
    protected virtual IAsyncEnumerable<(T Entity, Dictionary<string, string> Headers, MessageMeta Meta)> ConsumeAsync(
        KsqlContext context,
        CancellationToken cancellationToken)
    {
        return context.GetConsumerManager().ConsumeAsync<T>(cancellationToken);
    }
    /// <summary>
    /// REDESIGNED: ForEachAsync supporting continuous Kafka consumption
    /// Design change: ToListAsync() is disallowed; now based on GetAsyncEnumerator
    /// </summary>
    public virtual Task ForEachAsync(Func<T, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        return ForEachAsync((item, headers, _) => action(item), timeout, cancellationToken);
    }

    [Obsolete("Use ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task>)")]
    public virtual Task ForEachAsync(Func<T, Dictionary<string,string>, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
        => ForEachAsync((e, h, _) => action(e, h), timeout, cancellationToken);

    public virtual async Task ForEachAsync(Func<T, Dictionary<string,string>, MessageMeta, Task> action, TimeSpan timeout = default, CancellationToken cancellationToken = default)
    {
        if (action == null)
            throw new ArgumentNullException(nameof(action));

        var context = GetContext() as KsqlContext
            ?? throw new InvalidOperationException("KsqlContext is required");

        using var linkedCts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);

        if (timeout != default && timeout != TimeSpan.Zero)
        {
            linkedCts.CancelAfter(timeout);
        }
        await foreach (var (entity, headers, meta) in ConsumeAsync(context, linkedCts.Token))
        {
            if (headers.TryGetValue("is_dummy", out var dummyHeader) && bool.TryParse(dummyHeader, out var isDummy) && isDummy)
            {
                continue;
            }
### assistant — 2025-08-11T09:35:40.247548+00:00

# Show more lines around Real ForEachAsync definition
for i,l in enumerate(lines):
    if 'ForEachAsync' in l:
        print(i,l)
