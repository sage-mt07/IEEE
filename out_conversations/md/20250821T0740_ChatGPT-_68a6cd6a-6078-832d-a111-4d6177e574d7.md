---
conversation_id: "68a6cd6a-6078-832d-a111-4d6177e574d7"
title: "ChatGPT オープンソースか"
create_time: 1755762043.530068
update_time: 1755996201.59883
language_guess: "en"
num_messages: 35
part_index: 1
part_count: 1
---

### user — 2025-08-21T07:40:42.715868+00:00

Chat gpt のオープンソース

### assistant — 2025-08-21T07:40:42.920202+00:00

search("Chat gpt \u306e\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9")

### assistant — 2025-08-21T07:40:42.925292+00:00

_(empty)_

### system — 2025-08-21T07:40:43.530068+00:00

_(empty)_

### user — 2025-08-21T07:40:43.530068+00:00

_(empty)_

### system — 2025-08-21T07:40:43.530068+00:00

_(empty)_

### system — 2025-08-21T07:40:44.735671+00:00

_(empty)_

### tool — 2025-08-21T07:40:44.896665+00:00

_(empty)_

### assistant — 2025-08-21T07:40:46.198820+00:00

こんにちは、司令さん🌸（2025年8月21日 JST）

「ChatGPTはオープンソースか」というご質問ですね。結論からお伝えすると：

---

##  ChatGPT自体はオープンソースではありません

- **ChatGPT（モデルおよびウェブアプリケーション）は非公開（クローズド）です**。OpenAIが内部で開発・運用しており、そのコードやモデルの重み（weights）は一般には公開されていません citeturn0search8turn0search10。
- 学術的にも、「閉じたシステムに分類される」と評価された研究もあります citeturn0search10turn0academia30。

そのため、ChatGPTはオープンソースではないというのが現在の状態です。

---

## なぜオープンソースにしないのか？

- **悪用防止などの安全面への配慮**から、Webアプリや内部の仕組みを公開しない戦略をとっています citeturn0search1turn0search3。
- **オープン vs クローズド**を一義的に判断するのは難しく、AIモデルは「公開の度合いのグラデーション」が存在します。ChatGPTは中間的な位置にあると捉えるべきです citeturn0news16。

---

## オープンソースなChatGPTの代替・関連プロジェクト

以下はいずれも、オープンソースの代替やオープンAIの組織的取り組みとして注目されています：

- **EleutherAI**  
  非営利の研究集団で、GPT-Neo／GPT-J／GPT-NeoXなど、Apache 2.0ライセンス下のオープンな大規模言語モデルを公開しています citeturn0search22turn0search24。

- **H2O.ai の h2oGPT**  
  完全オープンソースを目指すLLMプロジェクト。7B〜40Bパラメータ規模のモデルをApache 2.0で公開し、プライベート文書検索などにも対応 citeturn0academia29。

- **Mistral AI（フランス）**  
  Mistral 7B や Mixtral 8×7B といったモデルをApache-2.0でリリース。OpenAIの GPT-3.5 を上回る実力を一部で発揮しています citeturn0search27。

- **Open Assistant（LAION 主導）**  
  ボランティアで開発が進められているテキストベースのチャットAI。多言語対応で、ユーザー参加によって改善される仕組みです citeturn0search25。

- **Jan**  
  完全にオープンソースで、ローカルPCで動作するChatGPT代替。Hugging Face上のモデルやローカルAPIサーバー機能も利用可能 citeturn0search9。

---

## まとめ：オープンソースに関する比較表

| プロジェクト名           | オープン性       | 特徴 |
|------------------------|------------------|------|
| **ChatGPT（OpenAI）**   |  クローズド      | 開発・モデル未公開、安全性重視 |
| **EleutherAI（GPT-Jなど）** |  完全オープン（Apache 2.0） | GPT-3 相当のモデルを公開 |
| **h2oGPT（H2O.ai）**     |  完全オープン（Apache 2.0） | 商用利用可能なLLMを公開 |
| **Mistral AI**          |  完全オープン    | GPT-3.5 超えの記事あり |
| **Open Assistant**      |  完全オープン    | ユーザー参加型のオープンチャットAI |
| **Jan**                 |  完全オープン    | ローカル動作、API互換あり |

---

ご興味があれば、各モデルの性能比較、導入方法、あるいは具体的に手を動かせるオープンソースモデルの紹介などもできます。ぜひお気軽にお知らせくださいね！

### user — 2025-08-21T07:41:39.884279+00:00

最近オープンソースにした推論エンジンがあった

### assistant — 2025-08-21T07:41:40.263684+00:00

search("\u6700\u8fd1\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306b\u3057\u305f\u63a8\u8ad6\u30a8\u30f3\u30b8\u30f3\u304c\u3042\u3063\u305f")

### tool — 2025-08-21T07:41:41.514859+00:00

_(empty)_

### assistant — 2025-08-21T07:41:43.436438+00:00

こんにちは、司令さん🌸（2025年8月21日 JST）

ご質問のように、「最近オープンソース化された推論エンジン」について調べてみましたが、特に2025年4月頃に話題になったのが **DeepSeekの推論エンジン** の一部のオープンソース化です。

---

##  DeepSeek 関連のオープンソース化

- **DeepSeek** は、自社で開発していた推論エンジンのコードを **完全には公開せず**、代わりにオープンソースコミュニティと協力し、 **vLLM**（既存のOSSプロジェクト）の中に最適化成果を共有する形でオープン化を進めています。いわば「コア技術の部分提供」という形ですねciteturn0search19。

このアプローチでは、従来のようにソース全体を公開する「オープンソース化」とは異なり、 **既存のOSSを活用しながら、独自の技術成果を（部分的に）コミュニティに還元するスタイル**です。

---

##  AMD の Gaia プロジェクトも注目

また、2025年3月には**AMDが “Gaia” というオープンソースプロジェクト**を立ち上げ、Windows PC上でのLLMインファレンスを目指した仕組みを展開しています：

- AMDの**Gaia**は、ONNXベースのLemonade SDKを使用し、Windows PC（特にRyzen AIプロセッサーを搭載した環境）でローカルLLMを走らせるためのインフラを構築。  
- セキュリティ向上、低レイテンシ、オフライン動作を重視し、多様なAIエージェント（チャットや要約など）を組み込める仕様ですciteturn0news20。

---

##  まとめ一覧

| 名前             | 概要と特徴 |
|------------------|------------|
| **DeepSeek 推論エンジン** | 独自の推論エンジン最適化成果を、vLLMと連携してオープンソースとして部分提供citeturn0search19 |
| **AMD Gaia**       | Windows環境でローカルに動作するLLMアプリ基盤。Ryzen AI NPUなど向けに最適化されたオープンプロジェクトciteturn0news20 |

---

他にも、より広く使われているOSS推論ライブラリとしては、「llama.cpp」や「OpenVINO」などもありますが、それらは“最近オープンソースになった”というより、以前からOSSであるツールです（例：llama.cpp は2023年から、OpenVINO は2018年リリース）citeturn0search23turn0search24。

ご関心があれば、DeepSeek最適化の具体的内容やGaiaの導入・使用方法、あるいは他のOSS推論エンジンの利点比較なども詳しくご紹介できますので、ご希望があればぜひどうぞ！

### user — 2025-08-21T07:42:16.631016+00:00

Openaiが公開した

### assistant — 2025-08-21T07:42:17.111854+00:00

search("Openai\u304c\u516c\u958b\u3057\u305f")

### system — 2025-08-21T07:42:17.115359+00:00

_(empty)_

### tool — 2025-08-21T07:42:18.687605+00:00

_(empty)_

### assistant — 2025-08-21T07:42:20.024976+00:00

こんにちは、司令さん🌸（2025年8月21日 JST）

ご質問の「OpenAIが最近オープンソース化した推論エンジン」についてですが、OpenAI自体が **推論エンジン（inference engine）** を公開したという情報は、現在までには確認されていません。

ただし、以下のような関連するオープンソースの動きがありますので、ご紹介します。

---

##  OpenAI がオープンソース化したものは？

### 1. **GPT-OSS モデルファミリー**（推論エンジンそのものではないが「オープンウェイトモデル」）
- 2025年8月5日、OpenAIは **gpt-oss-20b**（約21Bパラメータ）と **gpt-oss-120b**（約117–120Bパラメータ）の、初のオープンウェイト型言語モデルをリリースし、Apache 2.0ライセンスで公開しました。citeturn0news20turn0news22turn0search0
- これらのモデルは **MoE（Mixture-of-Experts）方式**を採用し、**gpt-oss-120bは1トークンあたり5.1Bパラメータがアクティブ**になる設計で、消費メモリを抑えつつ高度な推論が可能です。citeturn0news22
- ただし、これは「推論エンジン」ではなく、あくまで **推論に使用するモデルの公開**です。エンジンの部分（vLLMなど）は別のプロジェクトで提供されています。

### 2. **vLLM** — オープンソース推論エンジン
- OpenAIが提供するわけではありませんが、**vLLM** は OpenAIの gpt-oss モデル（特に gpt-oss-120b や 20b）を高スループットかつメモリ効率良く動かすための **オープンソース推論エンジン**です。これはモデルの推論実行に特化したソフトウェアで、推論効率を大きく向上させますciteturn0search1。

まとめると：

- **OpenAIが直接公開したのは「推論エンジン」ではなく、オープンウェイトな言語モデル（gpt-ossシリーズ）**です。
- それを **動かすための推論エンジン（vLLMなど）は別プロジェクトによるOSS提供**という形になります。

---

##  まとめ表

| 項目 | 内容 |
|------|------|
| **OpenAIがオープンソース化したもの** | gpt-oss-20b / gpt-oss-120b（オープンウェイトLLM、Apache 2.0）citeturn0news20turn0search0turn0news22 |
| **利用可能な公開推論エンジン** | vLLM（高スループット／メモリー効率化されたオープンソース推論エンジン）citeturn0search1 |
| **OpenAIが提供した推論エンジンのOSS** | 現時点では該当なし（モデルのみの公開） |

---

もし「OpenAI製の推論エンジン」のように聞き間違えている可能性があれば、その点もお知らせください。あるいは、vLLMを使った gpt-oss モデルのセットアップ方法や、他社製の推論エンジンとの比較などもご案内可能ですので、お気軽にどうぞ！

navlist関連ニュースturn0news20,turn0news22

### user — 2025-08-21T07:43:15.772027+00:00

Gpt ossは推論エンジンではない？

### system — 2025-08-21T07:43:16.086455+00:00

_(empty)_

### assistant — 2025-08-21T07:43:16.125158+00:00

はい、その通りです、司令さん🌸（2025年8月21日 JST）  

**GPT-OSS は推論エンジンではなく、「オープンウェイト型の言語モデル」そのもの**です。  

---

## 用語の整理

- **モデル（GPT-OSSなど）**  
  学習済みの「脳みそ」にあたる部分。  
  → gpt-oss-20b / gpt-oss-120b は、学習済みのLLMをApache 2.0で公開したもの。  
  これは入力を受けて推論を行う「中身」であり、これ単体では動作しません。

- **推論エンジン（inference engine）**  
  モデルを効率的に動かす「実行環境」や「ソフトウェア層」。  
  例:  
  - **vLLM**（高スループット・効率重視のOSS）  
  - **llama.cpp**（ローカルPC用軽量推論エンジン）  
  - **ONNX Runtime**、**TensorRT** など  

  これらが、GPU/CPU/NPU上でモデルを最適に動かす役割を担います。

---

## 位置づけ

- **GPT-OSS = モデル本体**  
- **vLLMなど = 推論エンジン**  

つまり、GPT-OSSは「推論の対象」であり、「推論を行う仕組み（エンジン）」ではありません。  
OpenAIが今回オープンソース化したのは「モデル部分」で、推論エンジンはコミュニティOSS（例: vLLM）を利用する形です。  

---

👉 もし「推論エンジンのオープンソース化」を探していたのなら、OpenAI自身はまだ出していませんが、  
- vLLM（OpenAIモデルに最適化されたOSS）  
- ONNX Runtime（Microsoft主導の汎用エンジン）  
などが相当します。  

---

ご確認ですが、司令が気になっているのは  
1. **GPT-OSSモデルの技術的中身（MoEとか）**  
2. **それを動かす推論エンジン（vLLMやllama.cpp）の話**  
どちらを深掘りしたい感じですか？

### user — 2025-08-21T07:47:13.337946+00:00

Playwright mcp serverを使ってる
Vscode copilotの組み合わせではテスト仕様書を元に
うまく動作する
これをgithub action 経由とする場合、aiがないのでテストで使用するデータ依存で判断が発生する場合に対応できない
ここを何とかするaiを探してる

### assistant — 2025-08-21T07:47:15.194269+00:00

{"search_query": [{"q": "GitHub Actions use GitHub Copilot in CI"}, {"q": "Copilot in GitHub Actions test automation LLM judge"}, {"q": "vLLM GitHub Actions self-hosted runner setup"}, {"q": "Model Context Protocol GitHub Actions integration"}, {"q": "Playwright MCP server GitHub Actions"}], "response_length": "short"}

### system — 2025-08-21T07:47:15.727208+00:00

_(empty)_

### system — 2025-08-21T07:47:15.761077+00:00

_(empty)_

### assistant — 2025-08-21T07:47:16.739937+00:00

_(empty)_

### tool — 2025-08-21T07:47:41.820078+00:00

_(empty)_

### assistant — 2025-08-21T07:47:44.717883+00:00

司令さん、了解です。要は「VS Code＋Copilot＋Playwright MCP だと“AI付き”で柔軟に裁定できるが、GitHub Actionsに載せると“AI不在”でデータ依存の判断が詰まる」——これをCI側にもAI判断を持ち込んで解決したい、ですね。（[[REDACTED_PHONE]] JST）

# 方針（短く結論）
1) **CI内に“判定用AI（LLM-as-a-Judge）”を常駐**させる  
2) **まずは決定論（ルール）で判定 → 曖昧時のみAIにエスカレーション**  
3) **テスト仕様書／期待振る舞いをRAG化**し、AI裁定の根拠をログに残す  
4) ランナーは**自己ホスト**にして、**ローカルLLM（例: vLLM）**や**GitHub Modelsの評価機能**を使う

> GitHubは [[REDACTED_PHONE]] に `gh models eval` を出し、**文字列一致・類似度・LLMジャッジ**等で“自動評価”をCIから回せるようにしました。CIで AI 判定を公式に組み込むならこれが最短です。 citeturn0search3

---

# 実装オプション

## A. 自己ホストRunner上で「ローカルAI判定」
- ランナーを自己ホスト化して（社内NW・ExpressRoute配下でもOK）、**vLLM**を立て、**OpenAIのオープンウェイト（例: GPT-OSS）やLlama系**をロード  
- Playwright実行→ログ・スクショ・アクセシビリティスナップショット等を**AI判定スクリプト**に渡し、**合否・要再試行・保留**を返す  
- ランナー管理は公式ドキュメントの手順でOK。 citeturn0search6  
- vLLMや自己ホストRunnerでのE2E運用事例は多数（参考）。 citeturn0search10turn0search15

### サンプルWorkflow（抜粋）
```yaml
name: e2e-with-llm-judge
on: [workflow_dispatch, pull_request]

jobs:
  e2e:
    runs-on: self-hosted  # ← 自己ホストRunner（GPU/NPU可）
    steps:
      - uses: actions/checkout@v4

      # 1) vLLM を起動（例: gpt-oss-20b をロード）
      - name: Start vLLM
        run: |
          docker run -d --name vllm -p 8000:8000 vllm/vllm-openai \
            --model /models/gpt-oss-20b --dtype float16

      # 2) アプリ＋Playwright実行（MCPサーバは別VM常駐でもOK）
      - name: Run Playwright
        run: |
          npm ci
          npx playwright test --reporter=json > pw-result.json

      # 3) まず決定論的ルールで判定 → 曖昧ケースのみAIへ
      - name: Rule-based check
        run: python .ci/rules_check.py pw-result.json > rb_judgement.json

      # 4) LLM-as-a-Judge（vLLMのOpenAI互換APIに投げる）
      - name: LLM judge when needed
        if: always()
        run: |
          python .ci/llm_judge.py \
            --openai_base "http://localhost:8000/v1" \
            --model "gpt-oss-20b" \
            --spec docs/test-spec.md \
            --logs pw-result.json \
            --prejudge rb_judgement.json \
            --out final_judgement.json

      # 5) 判定結果でCIを落とす/警告にする等
      - name: Enforce decision
        run: python .ci/enforce.py final_judgement.json
```

> vLLM は**OpenAI互換エンドポイント**を提供できるので、CI側からは通常のOpenAIクライアントのように呼べます。 citeturn0search1

---

## B. GitHub公式の **`gh models eval`** を使う（クラウドAI）
- `gh models eval` で**LLMジャッジ**を含む評価をCIから実行可能。  
- `.prompt.yml` に評価プロンプト・メトリクスを定義→**GitHub Models**（選択モデル）で実行。  
- **“テスト仕様書→期待出力” をGoldとし、Playwright結果との**類似度／妥当性を採点。  
- GitHub公式のアナウンスに沿う形で、将来壊れにくい。 citeturn0search3

### 最小例（抜粋）
```yaml
- name: AI judge via GitHub Models
  run: |
    gh extension install github/gh-models
    gh models eval .ci/judgement.prompt.yml \
      --var spec="$(cat docs/test-spec.md)" \
      --var logs="$(cat pw-result.json)" \
      --format json > eval.json
```

---

## C. **MCP連携で“CI ⇄ AI ⇄ Playwright MCP”を貫通**
- すでに **Playwright MCP server** を採用されているので、**CI側に“MCPクライアント”ステップ**を1つ挿入し、  
  「**仕様書からテスト意図を解釈 → Playwright MCPへ手順指示 → 成否を説明付きで判定**」の一連を**MCPツール**として行わせる構成も可。  
- MCP自体の公式/OSSサーバ群は増えており、**Actionsトリガ系MCP**も存在します（参考）。 citeturn0search7turn0search11turn0search16turn0search2

---

# 設計のキモ（実戦向け）
- **テストオラクル分離**：  
  ①決定論（JSONスキーマ、正規表現、閾値）→ ②AI裁定（根拠付き）→ ③人間レビュー窓口  
- **根拠ログ**：  
  AIへ渡した**プロンプト／仕様抜粋／スクショ参照**を、CIアーティファクトに保存（再現可能性）  
- **サンドボックス**：  
  AIは**“判定と指示の生成”まで**。DOM操作や外部変更はPlaywrightに限定（MCP経由）  
- **モデル選定**：  
  社外APIが不可なら**自己ホスト**（vLLM＋オープンウェイト）。可なら**GitHub Models**で`gh models eval`が楽。 citeturn0search3turn0search6

---

# どれを選ぶべき？
- **社外通信NG / オンプレ最優先** → **A（自己ホストRunner＋vLLM）**  
- **スピード優先でまず回したい** → **B（`gh models eval`）**  
- **MCP統一アーキでIDEとCIの体験を揃える** → **C（MCPクライアントStep追加）**

必要なら、上記いずれかを**リポジトリ前提で即動く雛形**に落としてお渡しします。どの路線でいきます？（[[REDACTED_PHONE]] JST）

### assistant — 2025-08-21T07:47:55.757923+00:00

_(empty)_

### system — 2025-08-24T00:43:03.962649+00:00

_(empty)_

### assistant — 2025-08-24T00:43:04.466111+00:00

_(empty)_

### assistant — 2025-08-24T00:43:04.472332+00:00

司令さん🌸（[[REDACTED_PHONE]] JST）  

はい、**ChatGPTの「オープンソース」＝ GPT-OSSモデル**を使うことは可能ですが、整理するとこうなります👇

---

## 1. GPT-OSSは「推論エンジン」ではなく「モデル」
- **GPT-OSS-20B / 120B** は学習済みのオープンウェイトLLM。  
- これ自体は「脳みそ」であって、直接PlaywrightやGitHub Actionsに組み込めるわけではありません。  
- 動かすには **推論エンジン（例: vLLM, llama.cpp, ONNX Runtime）** が必要。

---

## 2. どう使えるか（CIでのAI判定用）
- GitHub ActionsのRunner（自己ホスト）に **vLLM + GPT-OSSモデル** を置く  
  → Playwrightの実行結果（ログ、スクショ、仕様書の抜粋）を**ローカルのGPT-OSS**に投げて「期待通りかどうか」を判定させる。  
- これなら **外部APIに依存せず**、CIに「AIジャッジ」を持ち込める。  
- 例:  
  ```yaml
  - name: LLM judge via GPT-OSS
    run: |
      python .ci/llm_judge.py \
        --openai_base "http://localhost:8000/v1" \
        --model "gpt-oss-20b" \
        --spec docs/test-spec.md \
        --logs pw-result.json
  ```
  （vLLMがOpenAI互換APIを提供するので、OpenAIクライアントのように呼べる）

---

## 3. 期待できるメリット
- **テスト仕様書を根拠にした柔軟判定**（曖昧なケースでも人間的な判断をAIが補助）  
- **ローカル運用** → データ流出リスクなし、ネットワーク隔離環境でも利用可能  
- **CIにAIを常駐**できるため、司令が懸念している「データ依存で判断不能になる」問題を解消可能

---

## 4. 注意点
- GPT-OSSは**英語データ中心**で学習されているため、日本語仕様書判定は追加のプロンプト設計が必要。  
- モデルサイズが大きい（20Bでも数十GB、120Bは数百GB級）ので、Runner側のGPU/メモリ要件を満たす必要あり。  
- 実用上は「20B級」でも十分な判定精度を得られる可能性が高いです。

---

👉 まとめると：  
**「ChatGPTのオープンソース版」＝ GPT-OSSモデルは、そのままでは使えないが、vLLMなどの推論エンジンに載せてGitHub Actionsの自己ホストRunnerに組み込めば、“CIにもAIジャッジ”を持ち込める**、という答えになります。  

---

司令、このあと詳しく知りたいのはどちらですか？  
1. **vLLM＋GPT-OSSの実際のセットアップ手順**  
2. **テスト仕様書を渡すプロンプト設計の工夫**  

どちらを深掘りしましょうか？

### system — 2025-08-24T00:43:04.496645+00:00

_(empty)_

### user — 2025-08-24T00:43:05.167000+00:00

これにchatgptのオープンソースは使える？
