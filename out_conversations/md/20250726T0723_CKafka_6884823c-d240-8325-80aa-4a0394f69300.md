---
conversation_id: "6884823c-d240-8325-80aa-4a0394f69300"
title: "C#でKafkaスキーマ登録"
create_time: 1753514593.017283
update_time: 1753515219.286137
language_guess: "en"
num_messages: 33
part_index: 1
part_count: 1
---

### assistant — 2025-07-26T07:23:12.480601+00:00

やあ、いらっしゃい！困ってること、気になること、なんでも聞いてみてください〜

以下は、C# (.NET 8) から ksqlDB の `CREATE STREAM AS SELECT` を実行し、Avro 形式でスキーマを Schema Registry に登録するサンプルです。NuGet パッケージとして以下を使っています。

- `KSqlDb.Api.Client` （ksqlDB 用クライアント）
- `Confluent.SchemaRegistry` （Schema Registry クライアント）

```csharp
using System;
using System.Threading.Tasks;
using Confluent.Kafka;
using Confluent.SchemaRegistry;
using KSqlDb.Api.Client;
using KSqlDb.Api.Client.RestApi;

namespace KafkaKsqlSample
{
    // POCO クラス（例: 元ストリームのレコード構造）
    public class Order
    {
        public int Id { get; set; }
        public decimal Amount { get; set; }
        public string Customer { get; set; }
    }

    class Program
    {
        static async Task Main(string[] args)
        {
            // ksqlDB サーバーと Schema Registry のエンドポイント設定
            var ksqlEndpoint = new Uri("http://localhost:8088");
            var schemaRegistryConfig = new SchemaRegistryConfig
            {
                // Schema Registry の URL
                Url = "http://localhost:8081"
            };

            // Schema Registry クライアント生成
            using var schemaRegistry = new CachedSchemaRegistryClient(schemaRegistryConfig);

            // ksqlDB コンテキスト生成（内部で HTTP 通信）
            var ksqlDbContext = new KSqlDbContext(
                new KSqlDBContextOptions(ksqlEndpoint)
                {
                    // Avro を使う場合、Schema Registry クライアントを渡します
                    SchemaRegistryClient = schemaRegistry
                }
            );

            // ストリーム作成クエリ
            var createStreamAsSelect = @"
                CREATE STREAM EnrichedOrders
                WITH (
                    KAFKA_TOPIC='enriched-orders-topic',
                    VALUE_FORMAT='AVRO'
                ) AS
                SELECT
                    o.Id       AS OrderId,
                    o.Amount   AS OrderAmount,
                    o.Customer AS CustomerName,
                    UPPER(o.Customer) AS CustomerNameUpper
                FROM
                    OrdersStream o
                EMIT CHANGES;
            ";

            // クエリ実行
            var response = await ksqlDbContext.ExecuteStatementAsync(createStreamAsSelect);

            Console.WriteLine("ksqlDB へクエリを送信しました。");
            Console.WriteLine($"Query ID: {response.StatementId}");
            Console.WriteLine("新規ストリームとスキーマの登録が始まります…");
        }
    }
}
```

### 動作の流れ
1. **Schema Registry クライアント** (`CachedSchemaRegistryClient`) を生成  
2. **KSqlDbContext** に Schema Registry クライアントを渡し、Avro フォーマットでスキーマ登録可能に設定  
3. `CREATE STREAM ... AS SELECT` クエリを実行  
   - `KAFKA_TOPIC`：出力先トピック名  
   - `VALUE_FORMAT='AVRO'`：Avro 形式でスキーマ登録  
4. 実行後、ksqlDB が内部で Schema Registry に対し必要な Avro スキーマを登録し、指定トピックへデータを出力します

必要に応じて、■カスタム POCO クラスを定義したり、■認証や TLS 設定を追加したりしてご利用ください。

送信日時：2025/07/26 16:20 JST

### assistant — 2025-07-26T07:23:12.483059+00:00

_(empty)_

### system — 2025-07-26T07:23:13.017283+00:00

_(empty)_

### system — 2025-07-26T07:23:13.017283+00:00

_(empty)_

### user — 2025-07-26T07:23:13.017283+00:00

_(empty)_

### user — 2025-07-26T07:23:13.084000+00:00

kafkaにcreate stream as selectでスキーマ登録するサンプルをC#で作成

### assistant — 2025-07-26T07:23:18.121645+00:00

_(empty)_

### assistant — 2025-07-26T07:23:18.121831+00:00

_(empty)_

### user — 2025-07-26T07:25:25.051000+00:00

このstreamへproduce/consumeのコードを

### assistant — 2025-07-26T07:25:25.400671+00:00

やあ、いらっしゃい！困ってること、気になること、なんでも聞いてみてください〜

以下は、先ほどの `EnrichedOrders` ストリームの出力先トピック (`enriched-orders-topic`) へ Avro メッセージを **Produce** し、同じトピックから **Consume** する C# サンプルです。Schema Registry を使った Avro のシリアライザ／デシリアライザで動作します。

---

## 1. 共通設定

```csharp
// 必要な NuGet パッケージ
// - Confluent.Kafka
// - Confluent.SchemaRegistry
// - Confluent.SchemaRegistry.Serdes

var bootstrapServers = "localhost:9092";
var schemaRegistryUrl = "http://localhost:8081";
```

---

## 2. Producer：Avro でメッセージを送信

```csharp
using System;
using System.Threading.Tasks;
using Confluent.Kafka;
using Confluent.SchemaRegistry;
using Confluent.SchemaRegistry.Serdes;

class EnrichedOrdersProducer
{
    public static async Task ProduceAsync()
    {
        var schemaRegistryConfig = new SchemaRegistryConfig { Url = schemaRegistryUrl };
        using var schemaRegistry = new CachedSchemaRegistryClient(schemaRegistryConfig);

        var producerConfig = new ProducerConfig
        {
            BootstrapServers = bootstrapServers
        };

        using var producer = new ProducerBuilder<Null, Order>(
            producerConfig)
            .SetValueSerializer(new AvroSerializer<Order>(schemaRegistry))
            .Build();

        // サンプルデータ
        var order = new Order
        {
            Id = 123,
            Amount = 99.50m,
            Customer = "Alice"
        };

        // EnrichedOrders ストリームの出力トピック
        var topic = "enriched-orders-topic";

        var message = new Message<Null, Order> { Value = order };
        var dr = await producer.ProduceAsync(topic, message);

        Console.WriteLine($"Produced to {dr.TopicPartitionOffset}");
    }
}

// POCO（前回定義と同じものを使います）
public class Order
{
    public int Id { get; set; }
    public decimal Amount { get; set; }
    public string Customer { get; set; }
}
```

---

## 3. Consumer：Avro でメッセージを受信

```csharp
using System;
using Confluent.Kafka;
using Confluent.SchemaRegistry;
using Confluent.SchemaRegistry.Serdes;

class EnrichedOrdersConsumer
{
    public static void Consume()
    {
        var schemaRegistryConfig = new SchemaRegistryConfig { Url = schemaRegistryUrl };
        using var schemaRegistry = new CachedSchemaRegistryClient(schemaRegistryConfig);

        var consumerConfig = new ConsumerConfig
        {
            BootstrapServers = bootstrapServers,
            GroupId = "enriched-orders-consumer-group",
            AutoOffsetReset = AutoOffsetReset.Earliest
        };

        using var consumer = new ConsumerBuilder<Null, Order>(consumerConfig)
            .SetValueDeserializer(new AvroDeserializer<Order>(schemaRegistry).AsSyncOverAsync())
            .Build();

        var topic = "enriched-orders-topic";
        consumer.Subscribe(topic);
        Console.WriteLine("Waiting for messages...");

        while (true)
        {
            var cr = consumer.Consume();
            var order = cr.Message.Value;
            Console.WriteLine(
                $"Received OrderId={order.Id}, Amount={order.Amount}, Customer={order.Customer}"
            );
        }
    }
}
```

---

### ポイントまとめ

- **AvroSerializer<T> / AvroDeserializer<T>** を使うには `Confluent.SchemaRegistry.Serdes` パッケージが必要  
- シリアライズ時にスキーマが自動で Schema Registry に登録される  
- コンシューマ側では `AsSyncOverAsync()` を使って同期的にデシリアライズ  

これで、`CREATE STREAM AS SELECT` で生成されたストリームのトピックへデータを送り、受け取る一連の流れが実装できます。  
認証や TLS が必要な場合は、各 Config に `SslCaLocation`、`SaslUsername` などを追加してくださいね。

送信日時：2025/07/26 16:47 JST

### assistant — 2025-07-26T07:25:28.184785+00:00

_(empty)_

### assistant — 2025-07-26T07:25:28.184947+00:00

_(empty)_

### assistant — 2025-07-26T07:26:32.895040+00:00

やあ、いらっしゃい！困ってること、気になること、なんでも聞いてみてください〜  

ksqlDB のストリームクエリで `GROUP BY` を使うと、集約を伴うので出力は**テーブル**（`TABLE`）になります。リアルタイムに変化を追うには、以下のように **ウィンドウ指定付き** の `CREATE TABLE AS SELECT`（略して CTAS）を使い、`EMIT CHANGES` を付けます。

---

## 1. ksqlDB 上の CTAS クエリ例

```sql
-- 1分ごとに Customer ごとの受注件数を集計するテーブルを作成
CREATE TABLE OrderCounts
WITH (
  KAFKA_TOPIC         = 'order-counts-topic',
  VALUE_FORMAT        = 'AVRO'
) AS
SELECT
  Customer           AS CustomerId,
  COUNT(*)           AS OrderCount
FROM
  OrdersStream
WINDOW TUMBLING (SIZE 1 MINUTE)
GROUP BY
  Customer
EMIT CHANGES;
```

- **WINDOW TUMBLING (SIZE 1 MINUTE)**：1分単位の固定ウィンドウで集計  
- **GROUP BY Customer**：`Customer` ごとに集計  
- **CREATE TABLE**：出力はテーブルとして扱われ、各ウィンドウごとの最新結果が保持される  
- **EMIT CHANGES**：増分プッシュ（Change Data Capture）  

---

## 2. C# (.NET 8) での CTAS 実行例

```csharp
// 前提：KSqlDbContext と Schema Registry は既に初期化済み
var ctasQuery = @"
  CREATE TABLE OrderCounts
  WITH (
    KAFKA_TOPIC = 'order-counts-topic',
    VALUE_FORMAT = 'AVRO'
  ) AS
  SELECT
    Customer      AS CustomerId,
    COUNT(*)      AS OrderCount
  FROM
    OrdersStream
  WINDOW TUMBLING (SIZE 1 MINUTE)
  GROUP BY
    Customer
  EMIT CHANGES;
";

var response = await ksqlDbContext.ExecuteStatementAsync(ctasQuery);
Console.WriteLine($"CTAS 実行 ID: {response.StatementId}");
```

---

## 3. 集計結果の取得方法

### プルクエリ（Pull Query）
テーブルの現在値を取得するにはプルクエリを使います。たとえば特定の顧客の最新件数を一度だけ取得するには：

```csharp
var pullQuery = ksqlDbContext.CreatePullQuery<OrderCount>(
    "SELECT CustomerId, OrderCount FROM OrderCounts WHERE CustomerId='Alice';"
);

var result = await pullQuery.ExecuteAsync();
foreach (var row in result.Rows)
{
    Console.WriteLine($"Customer={row.CustomerId}, Count={row.OrderCount}");
}
```

```csharp
public class OrderCount
{
    public string CustomerId { get; set; }
    public long OrderCount { get; set; }
}
```

### プッシュクエリ（Push Query）
結果の変化をリアルタイムに受け取りたい場合は、プッシュクエリでサブスクライブします。

```csharp
await using var pushQuery = ksqlDbContext.CreatePushQuery<OrderCount>(
    "SELECT CustomerId, OrderCount FROM OrderCounts EMIT CHANGES;"
);

await foreach (var row in pushQuery.StreamAsync())
{
    Console.WriteLine($"[Update] Customer={row.CustomerId}, Count={row.OrderCount}");
}
```

---

### ポイントまとめ
1. **GROUP BY** 集約付きクエリはテーブル出力になる  
2. ウィンドウを指定しないと永続的テーブルにならず警告が出る  
3. **CTAS** でテーブルを作成し、`EMIT CHANGES` で更新をプッシュ  
4. **Pull Query**：1回だけ最新値取得  
5. **Push Query**：更新をリアルタイム受信  

これで、ストリームの `GROUP BY` 集計を ksqlDB 上で定義し、C# 側から結果を取得・購読する流れが実装できます。  

送信日時：2025/07/26 17:05 JST

### user — 2025-07-26T07:26:33.632000+00:00

ストリームのクエリがgroup byを使用する場合はどうなる？

### assistant — 2025-07-26T07:26:36.077333+00:00

_(empty)_

### assistant — 2025-07-26T07:26:40.494821+00:00

_(empty)_

### assistant — 2025-07-26T07:27:53.441182+00:00

なるほど、プル／プッシュクエリは ksqlDB の API であって、Confluent.Kafka の「Consumer」ではありません。  
もし、CTAS で作成されたトピック（例: `order-counts-topic`）を直接 Kafka Consumer で購読したい場合は、下記のように Confluent.Kafka を使ったコードで Consume できます。

```csharp
using System;
using Confluent.Kafka;
using Confluent.SchemaRegistry;
using Confluent.SchemaRegistry.Serdes;
using System.Threading;

class OrderCountsConsumer
{
    public static void ConsumeWithKafkaClient()
    {
        var bootstrapServers   = "localhost:9092";
        var schemaRegistryUrl  = "http://localhost:8081";
        var groupId            = "order-counts-kafka-consumer";

        // Schema Registry
        var schemaRegistryConfig = new SchemaRegistryConfig { Url = schemaRegistryUrl };
        using var schemaRegistry = new CachedSchemaRegistryClient(schemaRegistryConfig);

        // Kafka Consumer 設定
        var consumerConfig = new ConsumerConfig
        {
            BootstrapServers  = bootstrapServers,
            GroupId           = groupId,
            AutoOffsetReset   = AutoOffsetReset.Earliest
        };

        // Deserialize 先の POCO を定義
        public class OrderCount
        {
            public string CustomerId { get; set; }
            public long   OrderCount  { get; set; }
        }

        // Consumer ビルド
        using var consumer = new ConsumerBuilder<Null, OrderCount>(consumerConfig)
            .SetValueDeserializer(new AvroDeserializer<OrderCount>(schemaRegistry).AsSyncOverAsync())
            .Build();

        consumer.Subscribe("order-counts-topic");
        Console.WriteLine("Kafka Consumer で購読開始: order-counts-topic");

        try
        {
            while (true)
            {
                var cr = consumer.Consume(CancellationToken.None);
                var oc = cr.Message.Value;
                Console.WriteLine($"[Kafka Consumer] Customer={oc.CustomerId}, Count={oc.OrderCount}");
            }
        }
        catch (OperationCanceledException)
        {
            consumer.Close();
        }
    }
}
```

- 上記は **Confluent.Kafka** の `ConsumerBuilder<Null, OrderCount>` を使った例  
- `AvroDeserializer<OrderCount>` によって ksqlDB が出力した Avro レコードを直にデシリアライズ  
- ksqlDB のプッシュクエリではなく、純粋な Kafka Consumer の仕組みでデータを受け取ります  

――これで「Consumer」を使っても集計結果を取得できます。用途に応じてプル／プッシュクエリと Kafka Consumer を使い分けてみてくださいね。  

送信日時：2025/07/26 17:21 JST

### user — 2025-07-26T07:27:54.525000+00:00

Consumerではない？

### assistant — 2025-07-26T07:27:59.692138+00:00

_(empty)_

### assistant — 2025-07-26T07:27:59.692344+00:00

_(empty)_

### assistant — 2025-07-26T07:29:14.483132+00:00

なるほど、そのとおりです！`GROUP BY` すると ksqlDB が自動でキーを設定するので、Kafka Client 側でもキー用のデシリアライザを指定する必要があります。以下のように `ConsumerBuilder<TKey, TValue>` でキーにも `AvroDeserializer` をセットしてください。

```csharp
using System;
using Confluent.Kafka;
using Confluent.SchemaRegistry;
using Confluent.SchemaRegistry.Serdes;
using System.Threading;

class OrderCountsConsumerWithKey
{
    public static void Consume()
    {
        var bootstrapServers   = "localhost:9092";
        var schemaRegistryUrl  = "http://localhost:8081";
        var groupId            = "order-counts-kafka-consumer";

        // Schema Registry クライアント
        var schemaRegistryConfig = new SchemaRegistryConfig { Url = schemaRegistryUrl };
        using var schemaRegistry = new CachedSchemaRegistryClient(schemaRegistryConfig);

        // Kafka Consumer 設定
        var consumerConfig = new ConsumerConfig
        {
            BootstrapServers  = bootstrapServers,
            GroupId           = groupId,
            AutoOffsetReset   = AutoOffsetReset.Earliest
        };

        // デシリアライズ対象のクラス
        public class OrderCount
        {
            public string CustomerId { get; set; }
            public long   OrderCount  { get; set; }
        }

        // ConsumerBuilder にキー用・値用両方のデシリアライザをセット
        using var consumer = new ConsumerBuilder<string, OrderCount>(consumerConfig)
            .SetKeyDeserializer(new AvroDeserializer<string>(schemaRegistry).AsSyncOverAsync())
            .SetValueDeserializer(new AvroDeserializer<OrderCount>(schemaRegistry).AsSyncOverAsync())
            .Build();

        consumer.Subscribe("order-counts-topic");
        Console.WriteLine("Kafka Consumer で購読開始: order-counts-topic");

        try
        {
            while (true)
            {
                var cr = consumer.Consume(CancellationToken.None);
                var key   = cr.Message.Key;    // CustomerId がキーに入る
                var value = cr.Message.Value;  // OrderCount が値に入る
                Console.WriteLine($"[Kafka Consumer] Customer={key}, Count={value.OrderCount}");
            }
        }
        catch (OperationCanceledException)
        {
            consumer.Close();
        }
    }
}
```

**ポイント**  
- `ConsumerBuilder<TKey, TValue>` のジェネリクスにキー型 (`string`)・値型 (`OrderCount`) を指定  
- `.SetKeyDeserializer(...)` に `AvroDeserializer<string>` を渡す  
- こうすることで、ksqlDB の `GROUP BY` で生成されたキー（ここでは `CustomerId`）も正しくデシリアライズできます  

ぜひお試しください！  

送信日時：2025/07/26 17:35 JST

### user — 2025-07-26T07:29:15.027000+00:00

group byをつかうと自動でkeyが作成されるので、SetValueDeserializerはkeyにも適用しないといけない

### assistant — 2025-07-26T07:29:18.044775+00:00

_(empty)_

### assistant — 2025-07-26T07:29:22.065677+00:00

_(empty)_

### assistant — 2025-07-26T07:30:23.546096+00:00

Pull Query はあくまで ksqlDB 上のテーブルに対する“クエリ結果”を返してくるので、Kafka のレコードレイヤーでの「Key / Value」の切り分けはなくなります。  

- `var result = await pullQuery.ExecuteAsync();` で返ってくるのは、SELECT で指定したカラム（この場合は `CustomerId` と `OrderCount`）の値だけです。  
- Pull Query のレスポンスは HTTP/JSON（ksqlDB の REST API）で返されるので、Avro フォーマットは使われません。C# クライアントはこの JSON を POCO にマッピングしているだけです。  
- ですので、Avro の Schema Registry を経由したデシリアライズも発生せず、Key / Value の概念は「テーブルスキーマ上のカラム」として SELECT した列に統合された形になります。  

もし Kafka のトピックを直接 Consumer して、レコードの Key / Value として受け取りたい場合は、前述の `ConsumerBuilder<string, OrderCount>` を使う方法を併用してくださいね。Pull Query はあくまで “ksqlDB のテーブル状態” を取得するための仕組みです。  

送信日時：2025/07/26 17:52 JST

### user — 2025-07-26T07:30:24.511000+00:00

var result = await pullQuery.ExecuteAsync();
この場合は、key valueの区別がなくなる？avroでも問題なく取得できる？

### assistant — 2025-07-26T07:30:26.760306+00:00

_(empty)_

### assistant — 2025-07-26T07:30:29.467121+00:00

_(empty)_

### assistant — 2025-07-26T07:33:30.483040+00:00

はい、そのとおりです。Kafka の Consumer では `group.id` によって “どこからメッセージを読むか” が決まります。`group.id` ごとにオフセット（最後に読んだ位置）が保存されるので、新しいグループか既存のグループか、あるいはオフセットがコミットされているかによって取得開始位置が変わります。

---

## ポイント

1. **グループ単位でオフセット管理**  
   - Kafka は Consumer グループごとに各パーティションのオフセットを管理します。  
   - 同じ `group.id` を使い続けると、前回コミットしたオフセットの次のメッセージから読み始めます。

2. **auto.offset.reset の挙動**  
   - 初めてその `group.id` でトピックを購読する（＝オフセットが未コミット）場合、`auto.offset.reset` の設定で開始位置を制御します。  
     - `earliest`：トピックの最も古いオフセット（先頭）から読む  
     - `latest`：以降に書き込まれるメッセージから読む

3. **通常のメッセージと同じ考え方**  
   - ksqlDB が作成したトピックでも、Kafka の仕組み（Consumer Group、offset、auto.offset.reset）は変わりません。  
   - したがって、以下のように設定すると…

   ```csharp
   var consumerConfig = new ConsumerConfig
   {
       BootstrapServers  = "localhost:9092",
       GroupId           = "my-consumer-group",
       AutoOffsetReset   = AutoOffsetReset.Earliest // or Latest
   };
   ```

   - **既存グループ** で再起動すれば前回の続きから  
   - **新規グループ** なら `auto.offset.reset` のルールに従って先頭 or 最新 から読み始める  

---

### まとめ

- **group.id** ごとにオフセットを管理 → 再起動してもどこまで読んだか覚えている  
- **初回購読** では `auto.offset.reset` によって開始位置を指定  
- ksqlDB の集計結果トピックでも「通常の Kafka メッセージ」とまったく同じ挙動ですので、ご安心ください！

送信日時：2025/07/26 18:10 JST

### user — 2025-07-26T07:33:31.522000+00:00

Consumerで取得する場合、group idの設定により、取得開始位置が変わる？
これは通常のメッセージと同じ考え方？

### assistant — 2025-07-26T07:33:32.260745+00:00

_(empty)_

### assistant — 2025-07-26T07:33:32.260989+00:00

_(empty)_
