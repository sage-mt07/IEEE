---
conversation_id: "67a79550-e9a4-8007-b185-753c75f4a9f8"
title: "protobuf-net と KSQL 連携"
create_time: 1739035985.070734
update_time: 1740113721.759861
language_guess: "en"
num_messages: 8
part_index: 1
part_count: 1
---

### system — 2025-02-08T17:33:05.070734+00:00

_(empty)_

### user — 2025-02-08T17:33:05.072669+00:00

protobuf-netを中心に作成したアプリがKSQLを利用する簡単な方法

### tool — 2025-02-08T17:33:32.522842+00:00

**理解する方法**

ユーザーの質問を理解するために、少し整理してみます。「protobuf-netを中心に作成したアプリがKSQLを利用する簡単な方法」というのは、protobuf-netを使ったアプリでKSQLをどう使うかという意味だと思います。KSQLとは、KafkaのストリーミングSQLエンジンですね。protobuf-netは、.NETでProtocol Buffersを使うライブラリです。この質問が求めているのは、protobuf-netとKSQLの統合方法に対するシンプルな方法だと思います。

**KSQLとprotobuf-netの統合方法**

「protobuf-net」は、Googleのプロトコルバッファ（Protobuf）を使うための.NETライブラリです。そして、「KSQL」(またはksqlDB)は、Apache Kafka上で動作するストリーミングSQLエンジンですね。質問は、protobuf-netを使って構築されたアプリケーションにKSQLを組み合わせる方法を求めているのかもしれません。データフォーマットをprotobufからKSQLがサポートする形式（JSONやAVRO）に変換するためには、ConfluentのSchema Registryを使うのが簡単な方法かもしれません。

**protobuf-netとKSQLの統合方法**

「protobuf-netを中心に作成したアプリでKSQLを利用する簡単な方法」について、最もシンプルな方法は、まずデータをprotobuf形式でシリアライズし、Kafkaトピックに書き込みます。その後、ksqlDBを使って、このトピックのデータを消費する方法です。ただし、KSQLはデフォルトでProtobufをサポートしていないため、Confluent Schema Registryを利用し、プロトコル変換を行います。必要に応じて、JSONやAvroフォーマットも併用可能です。

### assistant — 2025-02-08T17:33:32.522983+00:00

以下は、protobuf-net を使ってシリアライズしたデータを Kafka 経由で KSQL（ksqlDB）で扱うための、比較的シンプルなアプローチの例です。状況や要件に合わせて、以下のいずれかの方法を検討してください。

---

## 1. JSON 変換を利用する方法

**概要:**  
KSQL は JSON 形式をネイティブにサポートしているため、protobuf で生成したオブジェクトを一度 JSON に変換して Kafka に送信すれば、KSQL 側での定義・処理が容易になります。

**手順:**  
1. **シリアライズ／変換処理の追加**  
   - アプリ内で通常は protobuf-net によりオブジェクトをバイナリにシリアライズしていると思いますが、必要に応じてそのオブジェクトを JSON へ変換する（または、protobuf でシリアライズした後に JSON へ変換するロジックを追加する）。
   - .NET では `System.Text.Json` や `Newtonsoft.Json` などを使って JSON 変換できます。

2. **Kafka Producer で送信**  
   - 変換済みの JSON データを Kafka のトピックに送信します。

3. **KSQL 側でストリーム定義**  
   - KSQL/ksqlDB で該当トピックを JSON 形式のストリームとして定義します。例えば：
     ```sql
     CREATE STREAM my_stream (
         field1 VARCHAR,
         field2 INT,
         ...
     ) WITH (
         KAFKA_TOPIC = 'your_topic',
         VALUE_FORMAT = 'JSON'
     );
     ```

**メリット:**  
- KSQL でのデータ確認やデバッグが容易  
- 変換ロジックがシンプル（既存の JSON シリアライズライブラリが利用できる）

---

## 2. Confluent Schema Registry と Protobuf Converter を利用する方法

**概要:**  
Protobuf のままのコンパクトなバイナリデータを扱いたい場合、Confluent の Schema Registry と Protobuf 用のシリアライザ／デシリアライザ（Serdes）を使う方法があります。これにより、スキーマの管理もでき、Kafka 側に Protobuf 形式のデータをそのまま送信できます。

**手順:**  
1. **Kafka クライアントの設定**  
   - .NET 用の Confluent.Kafka ライブラリと、`Confluent.SchemaRegistry`、`Confluent.SchemaRegistry.Serdes.Protobuf` などの NuGet パッケージを導入します。

2. **Producer で Protobuf データ送信**  
   - protobuf-net でオブジェクトをシリアライズし、そのデータを Protobuf 用シリアライザでラップして Kafka に送信します。シリアライザは自動的に Schema Registry へスキーマ登録を行います。

3. **KSQL 側での定義**  
   - ksqlDB（または KSQL）が Protobuf 形式を直接サポートしている場合は、そのフォーマットでストリームを定義します。  
     ※ KSQL のバージョンによっては Protobuf のネイティブサポートがない場合があるため、事前に利用するバージョンでのサポート状況を確認してください。  
   - サポートがない場合は、Kafka Connect などを利用して別トピックに JSON/Avro へ変換してから処理する方法も考えられます。

**メリット:**  
- データサイズが小さく、効率的なバイナリ転送が可能  
- スキーマ管理が容易（スキーマの進化にも対応）

---

## 3. Kafka Connect を利用した中継変換

**概要:**  
アプリケーションのロジックに変更を加えず、Kafka Connect の変換機能を利用して protobuf 形式のデータを JSON や Avro に変換し、KSQL で扱いやすいフォーマットにする方法です。

**手順:**  
1. **アプリケーション側:**  
   - protobuf-net でシリアライズしたデータをそのまま Kafka のトピックに送信する。

2. **Kafka Connect の設定:**  
   - 適切な変換（例：データフォーマット変換コネクタ）を設定し、元のトピックのデータを JSON や Avro に変換して新しいトピックに書き出す。

3. **KSQL 側での定義:**  
   - 変換後のトピックを JSON/Avro 形式のストリームとして定義する。

**メリット:**  
- アプリケーション側のコード変更が不要  
- 既存の Kafka Connect コネクタを利用できる

---

## まとめ

- **最もシンプルな方法** は、アプリケーション内で protobuf-net によるシリアライズの後、JSON 変換を追加して Kafka に送信し、KSQL 側で JSON ストリームとして定義する方法です。
- **Protobuf のまま利用したい場合** は、Confluent Schema Registry と Protobuf 用 Serdes を利用することで、シンプルかつ効率的に連携できます（ただし、KSQL 側の対応状況に注意）。
- **Kafka Connect を利用する方法** は、アプリ側を変更せずに変換を実施できるため、既存システムへの導入がしやすいです。

以上のいずれかの方法を採用することで、protobuf-net を中心に作成したアプリケーションから Kafka 経由で KSQL を利用したストリーム処理が実現できます。あなたのシステム環境や要件（例えば、データ転送量、運用の容易さ、既存のインフラなど）に合わせて、最適な方法を選択してください。

### user — 2025-02-08T17:34:42.229323+00:00

protobuf-net によるシリアライズの後、JSON 変換を追加して Kafka に送信し、KSQL 側で JSON ストリームとして定義する方法のサンプルコードを示せ

### tool — 2025-02-08T17:34:55.853999+00:00

**Providing code for Kafka with protobuf-net**

ユーザーが求めているのは、protobuf-net を使ってオブジェクトのシリアライズ後、そのデータを JSON に変換し、Kafka に送信し、KSQL 側で JSON ストリームとして定義するサンプルコードです。まず、C# を使って protobuf-net でシリアライズしたオブジェクトを JSON に変換し、Confluent.Kafka プロデューサーで Kafka に送信するコードを示す必要があります。その後、KSQL でそのデータを JSON ストリームとして定義方法に触れます。

**Clarifying protobuf-net and JSON conversion**

ユーザーの質問では、protobuf-netでシリアライズした後にJSONに変換する方法を求めていますが、protobuf-netは通常バイナリ形式でシリアライズします。質問の意図は、データがprotobuf-netで生成され、その後JSONに変換されてKafkaに送信されるパイプラインを作成することのようです。オブジェクトにProtoContractやProtoMember属性を付けて、protobuf-netを使ってシリアライズし、その後System.Text.JsonやNewtonsoft.JsonでJSON変換する方法を示す必要がありそうです。

### assistant — 2025-02-08T17:34:55.854118+00:00

以下は、C# アプリケーション内で、

1. **protobuf-net によるシリアライズ**  
2. 同一オブジェクトを **JSON に変換**  
3. 変換した JSON を **Confluent.Kafka を使って Kafka トピックへ送信**

するサンプルコードです。さらに、KSQL（ksqlDB）側でそのトピックを JSON 形式のストリームとして定義する例も示します。

---

## C# サンプルコード

以下のコード例では、NuGet パッケージとして以下を導入してください。

- [protobuf-net](https://www.nuget.org/packages/protobuf-net/)
- [Confluent.Kafka](https://www.nuget.org/packages/Confluent.Kafka/)
- .NET 標準の [System.Text.Json](https://docs.microsoft.com/ja-jp/dotnet/api/system.text.json)（または Newtonsoft.Json でも可）

```csharp
using System;
using System.IO;
using System.Text.Json;  // JSON 変換に利用
using Confluent.Kafka;
using ProtoBuf;

namespace KafkaProtobufJsonSample
{
    // データクラスに protobuf-net の属性を付与
    [ProtoContract]
    public class MyData
    {
        [ProtoMember(1)]
        public int Id { get; set; }

        [ProtoMember(2)]
        public string Name { get; set; }

        [ProtoMember(3)]
        public DateTime Timestamp { get; set; }
    }

    class Program
    {
        static void Main(string[] args)
        {
            // サンプルデータの生成
            MyData data = new MyData
            {
                Id = 1001,
                Name = "サンプルデータ",
                Timestamp = DateTime.UtcNow
            };

            // ① protobuf-net によるシリアライズ（バイナリ形式）
            byte[] binaryData;
            using (var ms = new MemoryStream())
            {
                Serializer.Serialize(ms, data);
                binaryData = ms.ToArray();
            }
            Console.WriteLine("Protobuf バイナリデータのサイズ: " + binaryData.Length);

            // ② 同じオブジェクトを JSON に変換（KSQL で扱うため）
            // ※ JSON シリアライズ時は、DateTime は ISO 8601 形式などに変換されます
            string jsonData = JsonSerializer.Serialize(data);
            Console.WriteLine("JSON データ: " + jsonData);

            // ③ Confluent.Kafka を利用して JSON データを Kafka に送信
            var config = new ProducerConfig
            {
                // Kafka ブローカーのアドレス（環境に合わせて変更してください）
                BootstrapServers = "localhost:9092"
            };

            // キーは不要な場合は Null、値に JSON 文字列を指定
            using (var producer = new ProducerBuilder<Null, string>(config).Build())
            {
                string topic = "my_topic"; // Kafka のトピック名
                var message = new Message<Null, string> { Value = jsonData };

                // 非同期でメッセージ送信
                producer.Produce(topic, message, deliveryReport =>
                {
                    if (deliveryReport.Error.IsError)
                    {
                        Console.WriteLine($"配信エラー: {deliveryReport.Error.Reason}");
                    }
                    else
                    {
                        Console.WriteLine($"メッセージ配信成功: {deliveryReport.TopicPartitionOffset}");
                    }
                });

                // 保留中のメッセージを確実に送信するために Flush
                producer.Flush(TimeSpan.FromSeconds(10));
            }
        }
    }
}
```

### コード解説

- **データクラス `MyData`**  
  - `[ProtoContract]` と `[ProtoMember]` 属性により、protobuf-net でシリアライズ可能にしています。

- **シリアライズ処理**  
  - `Serializer.Serialize` により、`MyData` オブジェクトをバイナリ形式にシリアライズしています。
  - ※ このバイナリデータはログや別用途に利用できますが、ここでは KSQL 用に JSON 変換も行っています。

- **JSON 変換**  
  - `JsonSerializer.Serialize` を利用して、同一のオブジェクトを JSON 文字列へ変換しています。  
    KSQL（ksqlDB）は JSON 形式をネイティブに扱えるため、この形式で Kafka に送信します。

- **Kafka 送信**  
  - Confluent.Kafka の Producer を使い、トピック `"my_topic"` に JSON 文字列を送信します。

---

## KSQL（ksqlDB）側のストリーム定義例

Kafka に送信した JSON データを KSQL で扱うため、以下のようにストリームを定義します。以下は ksqlDB のコンソールや REST API で実行できる SQL 文の例です。

```sql
CREATE STREAM my_stream (
  Id INT,
  Name VARCHAR,
  Timestamp VARCHAR  -- ※ 日付は文字列として受け取り、必要に応じて TIMESTAMP 型に変換してください
) WITH (
  KAFKA_TOPIC = 'my_topic',
  VALUE_FORMAT = 'JSON'
);
```

### ポイント

- **フィールド定義**  
  - JSON 内の各フィールド（ここでは `Id`、`Name`、`Timestamp`）に合わせた型を指定します。  
  - 日付フィールドについては、JSON で ISO 8601 形式などの場合、文字列として受け取った後に KSQL 内でパースする方法もあります。

- **WITH オプション**  
  - `KAFKA_TOPIC` で Kafka のトピック名を指定し、  
  - `VALUE_FORMAT` を `'JSON'` とすることで、JSON 形式のメッセージとして解釈されます。

---

以上のように、アプリケーション側で protobuf-net によるシリアライズを実施した上で JSON に変換し、Kafka に送信することで、KSQL 側では JSON ストリームとして容易にデータ処理が可能となります。環境や要件に合わせて各設定を調整してください。
