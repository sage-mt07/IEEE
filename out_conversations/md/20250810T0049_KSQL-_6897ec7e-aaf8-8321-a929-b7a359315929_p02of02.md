---
conversation_id: "6897ec7e-aaf8-8321-a929-b7a359315929"
title: "KSQL トピックエラー回避"
create_time: 1754786944.048574
update_time: 1754824151.645842
language_guess: "en"
num_messages: 209
part_index: 2
part_count: 2
---

### テスト設計上の注意
- Kafka メッセージ送信は `Chr.Avro.Confluent` を利用した POCO 型の自動スキーマ連携を推奨します。
- `GROUP BY` を指定した場合は Push Query として `EMIT CHANGES` が自動付与されます。Pull Query や TABLE クエリで `GROUP BY` を利用するとエラーとなります。
- `WINDOW` 句は `GROUP BY` の直後に配置してください。
- `CASE` 式では `THEN`/`ELSE` の型を必ず一致させる必要があります。
- `MIN`/`MAX` などのストリーム専用集計関数を含む場合、クエリは自動的に `CREATE STREAM` として生成されます。`CREATE TABLE` を明示するとエラーになります。

## Context クラスとベースインタフェース

| API                   | 説明                                   | 対象レイヤ | 実装状態 |
|------------------------|----------------------------------------|------------|---------|
| `IKsqlContext`         | コンテキスト操作の抽象インタフェース   | Context    | ✅      |
| `KafkaContextCore`     | `IKsqlContext` 実装の基底クラス        | Context    | ✅      |
| `KsqlContext`          | Kafka連携を統合した抽象コンテキスト    | Context    | ✅      |
| `KsqlContextBuilder`   | `KsqlContextOptions` 構築用ビルダー   | Application| ✅      |
| `KsqlContextOptions`   | スキーマレジストリ等の設定保持        | Application| ✅      |
| `IEventSet<T>`         | LINQ/Streaming操作の共通インタフェース| Stream/Table| ✅     |
| `IManualCommitMessage<T>` | 手動コミットメッセージ             | Subscription| ✅     |

## LINQ 風 DSL 一覧

| DSL メソッド                   | 説明                          | 戻り値型                          | 対象レイヤ    | 実装状態 |
|--------------------------------|-------------------------------|-----------------------------------|---------------|---------|
| `.Where(predicate)`            | 条件フィルタ                  | `IEventSet<T>`                    | Stream/Table  | ✅      |
| `.Window(WindowDef \| TimeSpan)` | タイムウィンドウ指定       | `IQueryable<T>`                   | Stream        | ✅      |
| `.Window(int minutes)`          | `WindowMinutes`によるフィルタ  | `IEntitySet<T>`                  | Stream/Table  | ✅      |
| `.Window().BasedOn<TSchedule>(keySelector, ?openProp, ?closeProp)` | `[ScheduleRange]` 属性、または `openProp`/`closeProp` パラメータで開始/終了を示すスケジュールPOCOに基づきウィンドウを生成 | `IQueryable<T>` | Stream | ✅ |
| `.GroupBy(...)`                | グループ化および集約          | `IEventSet<IGrouping<TKey, T>>`   | Stream/Table  | ✅      |
| `.OnError(ErrorAction)`        | エラー処理方針指定            | `EventSet<T>`                     | Stream        | ✅      |
| `.WithRetry(int)`              | リトライ設定                  | `EventSet<T>`                     | Stream        | ✅      |
| `.StartErrorHandling()`        | エラーチェーン開始            | `IErrorHandlingChain<T>`          | Stream        | ✅      |
| `.WithManualCommit()`          | 手動コミットモード切替        | `IEntityBuilder<T>`               | Subscription  | ✅      |
| `.Limit(int)`                  | **保持件数制限。Table型(Set<T>)でのみ利用可。OnModelCreatingで定義し、超過分は自動削除される。** | `IEntitySet<T>`                  | Table  | ✅      |

- `ToList`/`ToListAsync` は Pull Query として実行されます【F:src/Query/Pipeline/DMLQueryGenerator.cs†L27-L34】。
- `WithManualCommit()` を指定しない `ForEachAsync()` は自動コミット動作となります【F:docs/old/manual_commit.md†L1-L23】。
- `OnError(ErrorAction.DLQ)` を指定すると DLQ トピックへ送信されます【F:docs/old/defaults.md†L52-L52】。
- `ctx.Set<DlqEnvelope>()` を指定すると DLQ ストリームを取得できます。DLQ は無限ログのため `Take()` や `ToListAsync()` などの一括取得 API は利用できず、`ForEachAsync()` のみサポートします。また DLQ ストリームで `.OnError(ErrorAction.DLQ)` を指定すると無限ループになるため禁止されています。
- Messaging クラス自体は DLQ 送信処理を持たず、`ErrorOccurred`/`DeserializationError`/`ProduceError` などのイベントを通じて外部で DLQ 送信を行います。
- `Set<T>().Limit(n)` は Table 型の保持件数を制限する DSL です。`OnModelCreating` 内で指定し、超過分のレコードは自動削除されます。Stream 型や実行時クエリでは利用できません。
- バーエンティティでは `WithWindow().Select<TBar>()` で `BarTime` に代入した式が自動的に記録され、`Limit` の並び替えに使用されます。
- `RemoveAsync(key)` は値 `null` のトムストーンを送り、KTable やキャッシュから該当キーのデータを削除します。
- `.Window().BasedOn<TSchedule>` を用いる場合、バーは `[ScheduleRange]` 属性、または `openProp`/`closeProp` パラメータで示された `Open` ～ `Close` の範囲に含まれるデータのみで構成されます。日足生成で `Close` が 6:30 のときは、6:30 未満のデータが当日の終値として扱われます。
- バーやウィンドウ定義は必ず `KafkaKsqlContext.OnModelCreating` 内で宣言してください。アプリケーション側では定義済みの `Set<T>` を参照するだけです。
- `WithWindow<Rate, MarketSchedule>()` に続けて `.Select<RateCandle>()` を呼び出すことで、レートからバーエンティティを構成できます。
- `WithWindow<TEntity, TSchedule>(windows, timeSelector, rateKey, scheduleKey)` として `timeSelector` 引数でウィンドウを区切る時刻プロパティを明示します。

これらの戻り値型を把握することで、DSLチェーンにおける次の操作を判断しやすくなります。特に `OnError()` や `WithRetry()` は `EventSet<T>` を返すため、続けて `IEventSet` 系メソッドを利用できます。
### ToQuery チェーン

| メソッド | 説明 | 注意点 |
|---|---|---|
| `.From<T>()` | ビュー定義の開始 | - |
| `.Join<T2>(condition)` | 2テーブルまでの内部結合 | 後続に `.Where` 必須 |
| `.Where(predicate)` | 結合条件やフィルタ | `.Join` 使用時は必須 |
| `.Select(selector)` | 投影 | 呼び出し順序は `From`→`Join?`→`Where`→`Select` |

これらは `modelBuilder.Entity<T>().ToQuery(q => q ... )` 内で使用し、`KsqlQueryModel` に変換されます。

## 属性 (Attribute) 定義

| 属性                       | 役割                           | 実装状態 |
|----------------------------|--------------------------------|---------|
| `MaxLengthAttribute`       | 文字列長制限                   | ✅      |
| `ScheduleRangeAttribute`   | 取引開始・終了をまとめて指定する属性 | 🚧 |

`WithDeadLetterQueue()` は過去の設計で提案されましたが、現在は `OnError(ErrorAction.DLQ)` に置き換えられています。

## 構成オプションとビルダー

| API                        | 説明                             | 実装状態 |
|----------------------------|----------------------------------|---------|
| `KsqlDslOptions`           | DLQ 設定や ValidationMode など DSL 全体の構成を保持 | ✅ |
| `ModelBuilder`             | POCO から `EntityModel` を構築するビルダー | ✅ |
| `KafkaAdminService`        | DLQ トピック作成などの管理操作  | ✅      |
| `AvroOperationRetrySettings`| Avro操作ごとのリトライ設定     | ✅      |
| `AvroRetryPolicy`          | リトライ回数や遅延などの詳細ポリシー | ✅  |

`KsqlDslOptions.DlqTopicName` は既定で `"dead-letter-queue"` です【F:src/Core/Dlq/DlqProducer.cs†L248-L256】。

<a id="fluent-api-list"></a>### Fluent API 一覧

| メソッド | 説明 |
|----------|------|
| `Entity<T>(readOnly = false, writeOnly = false)` | エンティティ登録とアクセスモード指定 |
| `.AsStream()` | ストリーム型として登録 |
| `.AsTable(topicName = null, useCache = true)` | テーブル型として登録 |
| `.WithManualCommit()` | 手動コミットモード有効化 |
| `.ToQuery(build)` | 新DSLでのクエリ定義 |

<a id="fluent-api-guide"></a>
## Fluent API ガイドライン

POCO モデルを Fluent API で構成する際の設計指針と移行フローをまとめます。属性ベースからの移行後は
`IEntityBuilder<T>` を用いて宣言的に設定を行います。
### 1. 基本方針
 - `[KsqlTopic]` や `[KsqlKey]` などの属性でトピック名やキー順序を宣言します。
 - Fluent API はクエリ構築やモード指定のみを担い、スキーマ情報は属性に集約します。
 - エンティティ登録時は `readonly` `writeonly` `readwrite` の 3 種類で役割を指定し、未指定時は `readwrite` とみなします。
### 2. 推奨記述例
```csharp
[KsqlTopic("orders")]
public class Order
{
    [KsqlKey(Order = 0)]
    public int Id { get; set; }
    public decimal Amount { get; set; }
}

void OnModelCreating(ModelBuilder builder)
{
    builder.Entity<Order>(writeOnly: true);
}
```
`[KsqlTopic]` や `[KsqlDecimal]` 属性でトピックや精度を宣言できます。
### 3. 既存 POCO → Fluent API 移行フロー
1. POCO へ `[KsqlTopic]` と `[KsqlKey]` を付与してスキーマ情報を記述する。
2. `OnModelCreating` では `Entity<T>()` の登録のみ行い、その他は属性に委ねる。
3. テストを実行してキー順序やトピック設定が正しいか確認する。
   旧属性に関する詳細は `docs/namespaces/core_namespace_doc.md` を参照してください。
### 4. MappingManager との連携
`MappingManager` を利用して key/value を抽出する例です。詳細は `docs/architecture/key_value_flow.md` を参照してください。
```csharp
var ctx = new MyKsqlContext(options);
var mapping = ctx.MappingManager;
var entity = new Order { Id = 1, Amount = 100 };
var (key, value) = mapping.ExtractKeyValue(entity);
await ctx.AddAsync(entity, headers: new Dictionary<string, string> { ["is_dummy"] = "true" });
```
#### ベストプラクティス
- エンティティ登録は `OnModelCreating` 内で一括定義する。
- `MappingManager` を毎回 `new` しない。DI コンテナで共有し、モデル登録漏れを防ぐ。
### 5. 追加検討事項
- `[KsqlTopic]` 属性で指定できない詳細設定の扱いを検討中。
- MappingManager のキャッシュ戦略（スレッドセーフな実装範囲）を確定する必要あり。
### 6. サンプル実装での気づき
- `AddSampleModels` 拡張で `MappingManager` への登録をまとめると漏れ防止になる。
- 複合キーは `Dictionary<string, object>` として抽出されるため、型安全ラッパーの検討余地あり。
- `Dictionary<string,string>` 型のプロパティは Avro の `map` (`{"type": "map", "values": "string"}`) として扱われる。
  - キー・値ともに文字列のみサポート。その他の型やネスト構造は未対応。
  - `null` は許容されないため、プロパティは空ディクショナリで初期化する。
- `decimal` プロパティは `DecimalPrecisionConfig` で設定された `precision`/`scale` を持つ Avro `bytes` (`logicalType: decimal`) として生成される。
- 複数エンティティを登録するヘルパーがあると `OnModelCreating` の記述量を抑えられる。
### 7. AddAsync 統一に伴うポイント
- メッセージ送信 API は `AddAsync` に一本化した。旧 `ProduceAsync` は廃止予定。
- LINQ クエリ解析から `MappingManager.ExtractKeyValue()` を経由し `AddAsync` を呼び出す流れをサンプル化。
- 詳細なコード例は [architecture/query_to_addasync_sample.md](architecture/query_to_addasync_sample.md) を参照。

## ToQuery DSL

`ToQuery` は View 定義専用の Fluent API です。`From<T>()` を起点に `Join<T2>()`、`Where(...)`、`Select(...)` を順に呼び出してチェーンを構築します。JOIN は2テーブルまでサポートされており、結合条件は `Join` メソッド内で指定します。必要に応じて `Where` で追加のフィルタリングを行えます。呼び出し順序が守られない場合や未サポートの JOIN 数は構文検証で例外となります。

```csharp
modelBuilder.Entity<OrderSummary>().ToQuery(q => q
    .From<Order>()
    .Join<Customer>((o, c) => o.CustomerId == c.Id)
    .Where((o, c) => c.IsActive)
    .Select((o, c) => new OrderSummary { OrderId = o.Id, CustomerName = c.Name }));
```

`.ToQuery(...)` で得られた `KsqlQueryModel` は `KsqlContext` 初期化時に `CREATE STREAM/TABLE AS SELECT` 文として利用されます。

## エラーハンドリング

| API / Enum                 | 説明                           | 実装状態 |
|----------------------------|--------------------------------|---------|
| `ErrorAction` (Skip/Retry/DLQ) | エラー時の基本アクション    | ✅      |
| `ErrorHandlingPolicy`      | リトライ回数やカスタムハンドラ設定を保持 | ✅ |
| `ErrorHandlingExtensions`  | `.OnError()` `.WithRetryWhen()` 等の拡張 | ✅ |
| `DlqProducer` / `DlqEnvelope` | DLQ 送信処理               | ✅      |
| `DlqOptions`    | DLQ トピックの保持期間等を指定 | ✅      |

## 状態監視・内部機構

| API                         | 説明                             | 実装状態 |
|-----------------------------|----------------------------------|---------|
| `ReadyStateMonitor`         | トピック同期状態の監視           | ✅      |
| `CacheBinding`         | Kafka トピックと Cache の双方向バインディング | ✅ |
| `SchemaRegistryClient`      | スキーマ管理クライアント        | ✅      |
| `ResilientAvroSerializerManager` | Avro操作のリトライ管理     | ✅      |

| `WindowFinalizationManager` | Window最終化処理のタイマー管理  | ✅      |

## 各 API の備考

- `IEventSet<T>.WithRetry()` の実装例は `EventSet.cs` にあります【F:src/EventSet.cs†L238-L258】。
- `OnError` の拡張は `EventSetErrorHandlingExtensions.cs` で提供されています【F:src/EventSetErrorHandlingExtensions.cs†L8-L20】。
- 手動コミットの利用例は [manual_commit.md](old/manual_commit.md) を参照してください。
- `StartErrorHandling()` → `.Map()` → `.WithRetry()` の流れで細かいエラー処理を構築できます。
- `AvroOperationRetrySettings` で SchemaRegistry 操作のリトライ方針を制御します【F:src/Configuration/Options/AvroOperationRetrySettings.cs†L8-L33】。
### tool — 2025-08-10T08:58:56.674113+00:00

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.
### assistant — 2025-08-10T08:58:56.720005+00:00

_(empty)_
### assistant — 2025-08-10T08:59:07.061493+00:00

_(empty)_
### user — 2025-08-10T09:01:12.718000+00:00

ksqlContextのAPIの一部になるイメージとして作って
### system — 2025-08-10T09:01:13.892763+00:00

_(empty)_
### assistant — 2025-08-10T09:01:13.926154+00:00

_(empty)_
### assistant — 2025-08-10T09:01:19.450677+00:00

了解。`ksqlContext` 配下に “DLQ専用” を抱き込むイメージで、**既存の文脈を壊さず**に最小APIを足します。  
（`IKsqlContext.Dlq` エントリポイントを生やし、読む・覗く・再投函をまとめる）

# 提案API（C#）

```csharp
public interface IKsqlContext
{
    // 既存API ...

    /// <summary>DLQ専用の読み取り／再投函API。</summary>
    IDlqClient Dlq { get; }
}

public interface IDlqClient
{
    /// <summary>逐次読み取り（監視・常駐向け）。RawBytesを可読化して返す。</summary>
    IAsyncEnumerable<DlqRecord> ReadAsync(DlqReadOptions? options = null, CancellationToken ct = default);

    /// <summary>先頭からN件だけプル（UI/HTTP向け）。</summary>
    Task<IReadOnlyList<DlqRecord>> PeekAsync(int maxCount = 100, DlqReadOptions? options = null, CancellationToken ct = default);

    /// <summary>許可条件のもとで再投函。DLQ→DLQは禁止。</summary>
    Task<DlqReplayResult> ReplayAsync(DlqRecord record, DlqReplayOptions? options = null, CancellationToken ct = default);
}

// DTO/オプション（公開）
public sealed record DlqRecord(
    string MessageId, string Topic, int Partition, long Offset, DateTime TimestampUtc,
    byte[] RawBytes, string? RawText, string PayloadFormat, int? SchemaId,
    string? SchemaSubject, int? SchemaVersion, IReadOnlyDictionary<string,string> Headers,
    string? ErrorType, string? ErrorMessage, string? StackTrace);

public sealed class DlqReadOptions
{
    public string TopicName { get; init; } = "dead-letter-queue";
    public string ValueFormat { get; init; } = "AVRO"; // 将来 "JSON" へ切替可
    public bool FromBeginning { get; init; } = false;
    public int MaxBytesForRawText { get; init; } = 64 * 1024;
}

public sealed record DlqReplayOptions(string? TargetTopic = null);
public sealed record DlqReplayResult(bool Succeeded, string? Reason = null);
```

## ふるまい（要点）
- **可読化**：`RawBytes → RawText` は内部で実施（JSON整形/AVRO→JSON化/不可はBase64先頭＋truncated）。  
- **re-DLQ禁止**：`options.TopicName` がDLQなら `ReplayAsync` は常に `Succeeded=false`。  
- **将来のJSON移行**：`DlqReadOptions.ValueFormat` を `"JSON"` に変えるだけで対応。  
- **DeserializationError連携**：通常パイプライン側の `DeserializationError` はそのまま使用可（DLQ側は別配線）。

## 既定実装の差し込みイメージ
```csharp
public sealed class KsqlContext : IKsqlContext
{
    public IDlqClient Dlq { get; }

    public KsqlContext(/* 既存DI群 */, IDlqClient dlqClient /* inject */)
    {
        // 既存初期化...
        Dlq = dlqClient;
    }
}
```

```csharp
public sealed class DlqClient : IDlqClient
{
    // 依存: Kafka IConsumer<byte[],byte[]>, ISchemaRegistryClient?, ILogger, Producer(再投函用) 等
    public async IAsyncEnumerable<DlqRecord> ReadAsync(DlqReadOptions? opt = null, [EnumeratorCancellation] CancellationToken ct = default)
    {
        // byte[] 受信 → 可読化フォーマッタで RawText/Format/SchemaId を付与 → DlqRecord で返す
        // 成功時のみコミット。処理失敗はログして前進（ループ防止）
        // （実装詳細は既出スケルトンに準拠）
        yield break;
    }

    public Task<IReadOnlyList<DlqRecord>> PeekAsync(int maxCount = 100, DlqReadOptions? opt = null, CancellationToken ct = default)
    { /* ReadAsync を内部で使って N 件だけ集めて返す */ throw new NotImplementedException(); }

    public Task<DlqReplayResult> ReplayAsync(DlqRecord record, DlqReplayOptions? options = null, CancellationToken ct = default)
    { /* ポリシ判定の上で元Topic等へProduce。DLQ→DLQは禁止 */ throw new NotImplementedException(); }
}
```

## 使い方（利用側コード）
```csharp
await foreach (var r in ksqlContext.Dlq.ReadAsync(
    new DlqReadOptions { TopicName = "dead-letter-queue", ValueFormat = "AVRO" }, ct))
{
    logger.LogInformation("[DLQ] {topic}@{off} {fmt} sid={sid}\n{raw}",
        r.Topic, r.Offset, r.PayloadFormat, r.SchemaId, r.RawText);

    // 条件付きで再投函
    var replay = await ksqlContext.Dlq.ReplayAsync(r);
    if (!replay.Succeeded) logger.LogWarning("Replay skipped: {reason}", replay.Reason);
}

// 画面用
var head = await ksqlContext.Dlq.PeekAsync(50, new DlqReadOptions { FromBeginning = true });
```

## 設定例（`KsqlDslOptions` 拡張）
```json
{
  "KsqlDsl": {
    "DlqTopicName": "dead-letter-queue",
    "Dlq": {
      "ValueFormat": "AVRO",              // or "JSON"
      "Reader": { "GroupId": "dlq-worker", "MaxBytesForRawText": 65536 }
    }
  }
}
```

この形なら、**既存の `KsqlContext` 利用者は `.Dlq` を呼ぶだけ**で専用処理が使え、将来フォーマットをJSONに切り替えても契約は不変です。必要なら、このAPIを `api_reference.md` 体裁で追記までやります。  

（[[REDACTED_PHONE]]:01 JST）
### assistant — 2025-08-10T09:01:19.877342+00:00

_(empty)_
### assistant — 2025-08-10T09:02:55.058648+00:00

了解、**`ReadAsync` だけ**に絞った API 案、ksqlContext 直下の “DLQクライアント” として定義します。実装は最小・将来拡張しやすい形です。

# 提案API（Read専用）

```csharp
public interface IKsqlContext
{
    IDlqClient Dlq { get; }  // 既存に追加
}

public interface IDlqClient
{
    /// <summary>
    /// DLQ を逐次読み取り、人間可読な RawText を付与して返します。
    /// エラー時はフォールバック（Base64 先頭のみ等）で必ず 1 レコードにまとめます。
    /// </summary>
    IAsyncEnumerable<DlqRecord> ReadAsync(
        DlqReadOptions? options = null,
        CancellationToken ct = default);
}
```

## 付随型（最小）

```csharp
public sealed record DlqRecord(
    string MessageId,                 // Guid.ToString("D")
    string Topic,
    int Partition,
    long Offset,
    DateTime TimestampUtc,
    byte[] RawBytes,                  // 真実のソース（そのまま）
    string? RawText,                  // 可読化済み（整形JSON/テキスト/一部Base64など）
    string PayloadFormat,             // "avro" | "json" | "protobuf" | "binary" | "empty"
    int? SchemaId,                    // Confluent wire-format なら抽出
    string? SchemaSubject,
    int? SchemaVersion,
    IReadOnlyDictionary<string,string> Headers,
    string? ErrorType,
    string? ErrorMessage,
    string? StackTrace);
```

```csharp
public sealed class DlqReadOptions
{
    public string TopicName { get; init; } = "dead-letter-queue";
    public string ValueFormat { get; init; } = "AVRO"; // 将来 "JSON" に変更可
    public bool FromBeginning { get; init; } = false;  // true で earliest にシーク
    public int MaxBytesForRawText { get; init; } = 64 * 1024; // RawText の最大生成サイズ
    public bool CommitOnRead { get; init; } = true;    // true: 1件ごとにコミットして前進
}
```

## ふるまい（仕様要点）
- **受信は常に byte[]** として行い、内部で可読化（JSON整形 / AVRO→JSON 化 / 失敗時は Base64 先頭＋`(truncated)`）。
- **SchemaId 抽出**：Confluent ワイヤフォーマット（先頭 0 + 4byte ID）の場合は `SchemaId` を設定。
- **コミット**：`CommitOnRead=true` のとき、可読化が成功したレコードは **yield 前に commit**。可読化に失敗してもフォールバック記録を生成して **commit**（無限リトライ防止）。
- **キャンセル**：`CancellationToken` で安全に中断（`OperationCanceledException` は外へ投げず完了として終了）。
- **フォーマット切替**：DLQ を JSON に移行しても、`ValueFormat="JSON"` に切り替えるだけで利用側はそのまま。
- **re-DLQ 禁止**：本 API は **読むだけ**。再投函は別 API に後日追加（契約を汚さない）。

## 使い方（最小例）

```csharp
await foreach (var r in ksqlContext.Dlq.ReadAsync(
    new DlqReadOptions {
        TopicName = "dead-letter-queue",
        ValueFormat = "AVRO",          // 将来 JSON へ切替可
        FromBeginning = false,
        MaxBytesForRawText = 64 * 1024,
        CommitOnRead = true
    }, ct))
{
    _log.LogInformation("[DLQ] {topic}[{part}]@{off} fmt={fmt} sid={sid}\n{raw}",
        r.Topic, r.Partition, r.Offset, r.PayloadFormat, r.SchemaId, r.RawText);
    // 読むだけ。再投函は別レイヤで。
}
```

## 実装メモ（内側の期待値）
- **可読化**は 1 メソッドに集約（例：`DlqPayloadFormatter.PopulateAsync`）。  
  - JSON: UTF-8 で整形  
  - AVRO: Schema Registry があれば Generic で JSON 化、なければ Base64 部分表示  
  - Protobuf: 可能なら JSON 化  
- **ヘッダ**は `Dictionary<string,string>` に UTF-8 で取り込み（読めないものは Base64 で可）。  
- **MessageId**：DLQ に入っているものを優先。なければ `Guid.NewGuid().ToString("D")` で付与。

必要なら、この API を `api_reference.md` に追記するパッチもすぐ出します。  

（[[REDACTED_PHONE]]:03 JST）
### user — 2025-08-10T09:02:55.203000+00:00

ReadAsyncのAPIだけとします
### system — 2025-08-10T09:02:55.550077+00:00

_(empty)_
### assistant — 2025-08-10T09:02:55.558213+00:00

_(empty)_
### assistant — 2025-08-10T09:03:07.607721+00:00

_(empty)_
### system — 2025-08-10T09:05:30.610599+00:00

_(empty)_
### assistant — 2025-08-10T09:05:30.705563+00:00

了解です。  
`DlqReadOptions` から **`TopicName`** と **`ValueFormat`** を削除し、Avro専用前提にしたバージョンを出します。  

---

## 更新版 API（Read専用 / Avro固定）

```csharp
public interface IKsqlContext
{
    IDlqClient Dlq { get; }  // 既存に追加
}

public interface IDlqClient
{
    /// <summary>
    /// DLQ を逐次読み取り、人間可読な RawText を付与して返します。
    /// エラー時はフォールバック（Base64 先頭のみ等）で必ず 1 レコードにまとめます。
    /// </summary>
    IAsyncEnumerable<DlqRecord> ReadAsync(
        DlqReadOptions? options = null,
        CancellationToken ct = default);
}
```
### 付随型（変更なし）

```csharp
public sealed record DlqRecord(
    string MessageId,
    string Topic,
    int Partition,
    long Offset,
    DateTime TimestampUtc,
    byte[] RawBytes,
    string? RawText,
    string PayloadFormat,             // 固定値 "avro"
    int? SchemaId,
    string? SchemaSubject,
    int? SchemaVersion,
    IReadOnlyDictionary<string,string> Headers,
    string? ErrorType,
    string? ErrorMessage,
    string? StackTrace);
```
### 簡略化された `DlqReadOptions`

```csharp
public sealed class DlqReadOptions
{
    public bool FromBeginning { get; init; } = false;  // true: earliest から読む
    public int MaxBytesForRawText { get; init; } = 64 * 1024; 
    public bool CommitOnRead { get; init; } = true;    // true: 1件ごとにコミット
}
```

---
### 使い方例

```csharp
await foreach (var r in ksqlContext.Dlq.ReadAsync(
    new DlqReadOptions {
        FromBeginning = false,
        MaxBytesForRawText = 64 * 1024,
        CommitOnRead = true
    }, ct))
{
    _log.LogInformation("[DLQ] {topic}[{part}]@{off} sid={sid}\n{raw}",
        r.Topic, r.Partition, r.Offset, r.SchemaId, r.RawText);
}
```

---

この形にしておくと、  
- **トピック名固定**（コンテキスト全体で統一管理）  
- **Avro専用**（Schema Registry前提でJSON化）  
- API呼び出しは簡潔  

という構造になります。  

このあと、APIリファレンスに追記する形式で書き出すことも可能ですが、そうしますか？  
（[[REDACTED_PHONE]]:06 JST）
### user — 2025-08-10T09:05:31.798000+00:00

DlqReadOptionsからTopicName は外す。これは全体で決まってるため
ValueFormat は外す。avroのみのため
### user — 2025-08-10T09:06:07.761000+00:00

これでcodexに依頼します。api仕様書等のドキュメント更新を含め指示文をください
### assistant — 2025-08-10T09:06:08.371779+00:00

司令、Codex向けの**そのまま貼れる依頼文**を用意したよ。  
（API実装＋テスト＋ドキュメント更新まで含めた発注書）

---

# Codex依頼文：DLQ Read専用APIの追加（Avro固定）＋ドキュメント更新

## 目的
- `ksqlContext` 配下に **DLQ用 Read 専用API** を追加し、DLQ（Avro固定）を“可読化して”逐次取得できるようにする。  
- **Topic名はグローバル設定で固定**（`KsqlDslOptions.DlqTopicName` を想定）。  
- **ValueFormat は Avro 固定**。  
- 仕様に基づく **API仕様書・リファレンス更新** も同時に行う。

---

## スコープ / 要件（必須）
1) **公開インタフェースの追加**
   - `IKsqlContext` に `IDlqClient Dlq { get; }` を追加
   - `IDlqClient`（Read専用）を新規追加
   - 付随 DTO/Options を追加（仕様は下記）

   ```csharp
   public interface IKsqlContext
   {
       IDlqClient Dlq { get; } // 新規
   }

   public interface IDlqClient
   {
       /// <summary>
       /// DLQ を逐次読み取り、人間可読な RawText を付与して返します。
       /// エラー時もフォールバック（Base64 先頭のみ等）で 1 レコードにまとめます。
       /// </summary>
       IAsyncEnumerable<DlqRecord> ReadAsync(
           DlqReadOptions? options = null,
           CancellationToken ct = default);
   }

   public sealed record DlqRecord(
       string MessageId,
       string Topic,
       int Partition,
       long Offset,
       DateTime TimestampUtc,
       byte[] RawBytes,
       string? RawText,
       string PayloadFormat,   // 固定値 "avro"
       int? SchemaId,          // Confluent wire-formatから抽出できた場合のみ
       string? SchemaSubject,
       int? SchemaVersion,
       IReadOnlyDictionary<string,string> Headers,
       string? ErrorType,
       string? ErrorMessage,
       string? StackTrace
   );

   public sealed class DlqReadOptions
   {
       public bool FromBeginning { get; init; } = false;     // trueなら earliest へシーク
       public int MaxBytesForRawText { get; init; } = 64 * 1024;
       public bool CommitOnRead { get; init; } = true;       // true: 1件ごとにコミット
   }
   ```

2) **実装方針（最小・堅牢）**
   - **トピック名は設定から取得**：`KsqlDslOptions.DlqTopicName` を使用（存在しない場合は例外）
   - **受信は byte[]**（Key/Value）で行い、アプリ側で可読化
   - **Confluent Avro wire-format** を前提に、先頭5バイトから `SchemaId` を抽出  
     - `bytes[0] == 0`、`bytes[1..4]` が Big-Endian の SchemaId
   - **可読化（RawText）ロジック**：
     - 可能なら Schema Registry 経由の Generic 反序列化 → JSON 整形（インデント2）
     - 失敗時は `UTF-8 JSON として整形` 試行 → さらに失敗したら **Base64 先頭 N バイト＋`(truncated)`**（Nは `MaxBytesForRawText`）
     - `PayloadFormat` は常に `"avro"` をセット
   - **Headers**：UTF-8 として文字列化（不可能な場合は Base64）
   - **コミット戦略**：
     - `CommitOnRead=true`：レコード処理（可読化含む）が完了したら **yield 前に commit**
     - 可読化に失敗してもフォールバック `DlqRecord` を生成して **commit**（無限リトライ防止）
   - **FromBeginning**：`true` の場合、初回起動時に earliest へシーク
   - `CancellationToken` を適切に伝播し、`OperationCanceledException` は外へ投げず **正常終了**扱い

3) **既存構成との統合**
   - `KsqlContext`（または既存の `KsqlContext` 実装）に `IDlqClient` を **DI注入**して `Dlq` プロパティを返す
   - 既存の Consumer 構築ユーティリティ（`KafkaConsumerManager` 等）があれば再利用可。なければ最小限の `ConsumerBuilder<byte[],byte[]>` 実装でOK
   - Schema Registry クライアント（`ISchemaRegistryClient`）は既存のファクトリ/設定を流用

4) **テスト**
   - ユニットテスト（モックのみでOK）
     - **正常系**：Confluent wire-format + SR あり → `SchemaId` が設定され、`RawText` が JSON 整形される
     - **SR 不可/失敗系**：UTF-8 JSON に見える bytes → JSON 整形される
     - **完全バイナリ系**：`RawText` が Base64 先頭＋`(truncated)`、`SchemaId` は存在すればセット
     - **FromBeginning**：`FromBeginning=true` で earliest シークの呼び出しが行われる（モックで検証）
     - **CommitOnRead**：`true/false` 各設定で commit 呼び出しの有無を検証
     - **キャンセル動作**：`CancellationToken` キャンセルでイテレーションが終了する
   - （任意）インテグレーションテスト：Testcontainers で Kafka+SR を立て、実データを投入→`ReadAsync` で取得・検証（時間が合えば）

5) **ドキュメント更新**
   - `api_reference.md` に **「DLQ Read API（Avro固定）」** 節を新規追加  
     - 公開シグネチャ（上記のまま）  
     - 使い方サンプル（以下テンプレを記載）  
     - 仕様（FromBeginning/CommitOnRead/可読化の振る舞い）  
     - 既知の制約（Avro固定、再投函は非対応、本APIは“読むだけ”）
   - 既存の「エラーハンドリング/DeserializationError」節に **関連リンク** を追加（DLQ読取は `.Dlq.ReadAsync(...)` を参照）
   - `CHANGELOG.md` 追加（エントリ例）
     ```
     feat(dlq): Add IKsqlContext.Dlq.ReadAsync for Avro-based DLQ reading with human-readable RawText.
     ```

6) **非スコープ（やらない）**
   - DLQ への **書き込み経路の変更**  
   - **JSON 格納**への切替・CSAS 等のkSQL操作  
   - **Replay API** の実装（今回は Read のみ）

---

## 参考コード（可読化ユーティリティの雛形）

```csharp
static int? TryExtractSchemaId(byte[] bytes)
{
    if (bytes is { Length: >= 5 } && bytes[0] == 0)
        return (bytes[1] << 24) | (bytes[2] << 16) | (bytes[3] << 8) | bytes[4];
    return null;
}

static string? TryUtf8JsonPretty(byte[] bytes)
{
    try {
        var txt = System.Text.Encoding.UTF8.GetString(bytes);
        using var doc = System.Text.Json.JsonDocument.Parse(txt);
        return System.Text.Json.JsonSerializer.Serialize(doc,
            new System.Text.Json.JsonSerializerOptions { WriteIndented = true });
    } catch { return null; }
}

static string FallbackBase64(byte[] bytes, int max)
{
    var take = Math.Min(bytes.Length, max);
    var head = Convert.ToBase64String(bytes.AsSpan(0, take));
    return take < bytes.Length ? $"{head} (truncated)" : head;
}
```

> SR 経由の Avro→JSON 変換は、プロジェクトの既存ユーティリティ（GenericRecord 化→JSON化）があればそれを利用。なければ TODO としてスタブし、SR未利用パス（`TryUtf8JsonPretty` → `FallbackBase64`）でテストを先に通す。

---

## 受け入れ基準（AC）
- `IKsqlContext.Dlq.ReadAsync` が公開され、**Avro固定**の仕様どおりに動作する
- **Topic名はグローバル設定から取得**し、`DlqReadOptions` に `TopicName` は存在しない
- `DlqRecord.PayloadFormat == "avro"` が常にセットされる
- **可読化**：SRありのAvroで JSON 整形、SRなし/失敗時に UTF-8 JSON整形または Base64 フォールバック
- **FromBeginning/CommitOnRead** の挙動がテストで確認できる
- `api_reference.md` が更新され、使用例が掲載されている
- 既存の公開APIとの互換性に影響がない（破壊的変更なし）

---

## 提案コミット粒度
1. `feat(api): add Dlq read-only interfaces (IKsqlContext.Dlq, IDlqClient, DlqRecord, DlqReadOptions)`  
2. `feat(impl): implement DlqClient.ReadAsync with Avro wire-format parsing and human-readable RawText`  
3. `test(dlq): add unit tests for ReadAsync (sr success/fallback/json/base64, fromBeginning, commit)`  
4. `docs(api): update api_reference.md with DLQ Read API (Avro only)`  
5. `chore: update CHANGELOG.md`

---

以上。疑問点があれば `TODO:` コメントで残し、**テストを先にグリーン**にしてからSR経由のAvro→JSON変換の中身を詰めてください。  

（[[REDACTED_PHONE]]:10 JST）
### system — 2025-08-10T09:06:08.870387+00:00

_(empty)_
### assistant — 2025-08-10T09:06:08.880048+00:00

_(empty)_
### assistant — 2025-08-10T09:06:11.800352+00:00

_(empty)_
### user — 2025-08-10T10:06:33.005000+00:00

addasync/foreachasync/toListAsync/DLQまで物理試験終わった
今度はクエリの部分になる
KSQLDB側での処理となるため、クエリ登録＋AddAsync 受信側はKSQLDBで登録したメッセージ格納先トピックからのconsumeのパターンをさくせいしたい
### system — 2025-08-10T10:06:33.180220+00:00

_(empty)_
### assistant — 2025-08-10T10:06:33.260257+00:00

いいね、ここからは **KSQLDB でクエリを登録（永続化）→ AddAsync で投入 → KSQLDB が吐く “格納先トピック（シンク）” をアプリが consume** の定型パターンを作ればスムーズです。最小構成の“型”を置きます。

# 全体パターン（流れ）
1) **ソース定義**：アプリが `AddAsync` で書くトピックに対する `CREATE STREAM`（AVRO）
2) **クエリ登録**：`CSAS/CTAS/INSERT INTO` で**永続クエリ**を作成（シンク先とフォーマットを決める）
3) **アプリ側**  
   - 送信：`AddAsync<SourcePoco>(…)` → KSQLDB が処理  
   - 受信：**シンクトピック**用の POCO を作り（または byte[]）、通常のコンシューマで subscribe

---

# 1) ソース定義（`CREATE STREAM`）
- ストリーム名は **英数字 + `_`**（`-` や `.` は不可）
- **トピック名は自由**（`-` OK）。AVRO/TopicNameStrategy 前提

```sql
-- アプリから AddAsync で書く先（例：orders-in）
CREATE STREAM orders_in (
  OrderId       STRING KEY,
  CustomerId    STRING,
  Amount        DECIMAL(10,2),
  CreatedAt     TIMESTAMP
) WITH (
  KAFKA_TOPIC='orders-in',
  KEY_FORMAT='AVRO',
  VALUE_FORMAT='AVRO',
  TIMESTAMP='CreatedAt',
  PARTITIONS=3,
  REPLICAS=1
);
```

> すでにトピック＋スキーマがある場合、**列定義はスキーマ準拠**で。未知なら `PRINT 'orders-in' FROM BEGINNING LIMIT 5;` で形確認→定義。

---

# 2) クエリ登録（永続化）
## (A) 単純変換：`CSAS`
```sql
-- 例：必要な列だけ残して正規化
CREATE STREAM IF NOT EXISTS orders_out
WITH (
  KAFKA_TOPIC='orders-out',      -- ← シンク先トピック（アプリが consume する先）
  VALUE_FORMAT='AVRO',
  PARTITIONS=3, REPLICAS=1
) AS
SELECT
  OrderId,
  UCASE(CustomerId) AS CustomerId,
  Amount,
  CreatedAt
FROM orders_in
EMIT CHANGES;
```

## (B) 集計ビュー：`CTAS`（TABLE）
```sql
-- 例：顧客別の合計金額を集計
CREATE TABLE IF NOT EXISTS orders_by_customer
WITH (
  KAFKA_TOPIC='orders-by-customer',   -- シンク先（TABLEのchangelog）
  VALUE_FORMAT='AVRO',
  PARTITIONS=3, REPLICAS=1
) AS
SELECT CustomerId, SUM(Amount) AS TotalAmount
FROM orders_in
GROUP BY CustomerId
EMIT CHANGES;
```

> ポイント  
> - **永続クエリ**は KSQLDB がバックグラウンドで走り続け、指定トピックに**常に書き出す**  
> - **シンクトピック名**を明示しておくと、アプリが subscribe しやすい  
> - **集計（TABLE）**は*更新*が来るので、**changelog** として読むか再整形してストリーム化するかを選ぶ

---

# 3) 登録コマンド（PowerShell / REST）
```powershell
# 例：ソース作成
curl -Method POST "http://localhost:8088/ksql" `
  -Headers @{ "Content-Type"="application/vnd.ksql.v1+json; charset=utf-8" } `
  -Body '{ "ksql": "CREATE STREAM orders_in (OrderId STRING KEY, CustomerId STRING, Amount DECIMAL(10,2), CreatedAt TIMESTAMP) WITH (KAFKA_TOPIC=''orders-in'', KEY_FORMAT=''AVRO'', VALUE_FORMAT=''AVRO'', TIMESTAMP=''CreatedAt'', PARTITIONS=3, REPLICAS=1);" }'

# 例：CSAS（シンク登録）
curl -Method POST "http://localhost:8088/ksql" `
  -Headers @{ "Content-Type"="application/vnd.ksql.v1+json; charset=utf-8" } `
  -Body '{ "ksql": "CREATE STREAM IF NOT EXISTS orders_out WITH (KAFKA_TOPIC=''orders-out'', VALUE_FORMAT=''AVRO'', PARTITIONS=3, REPLICAS=1) AS SELECT OrderId, UCASE(CustomerId) AS CustomerId, Amount, CreatedAt FROM orders_in EMIT CHANGES;" }'
```

---

# 4) アプリ側：送信 → 受信
## 送信（`AddAsync`）
```csharp
// POCO（ソース）
[KsqlStream]
[KsqlTopic("orders-in")]
public class OrderIn {
    [KsqlKey] public string OrderId { get; set; } = default!;
    public string CustomerId { get; set; } = default!;
    public decimal Amount { get; set; }
    [KsqlTimestamp] public DateTime CreatedAt { get; set; }
}

// 送信
await ctx.Set<OrderIn>().AddAsync(new OrderIn {
    OrderId = Guid.NewGuid().ToString("D"),
    CustomerId = "c-001",
    Amount = 123.45m,
    CreatedAt = DateTime.UtcNow
});
```

## 受信（シンクトピックから）
```csharp
// POCO（シンク：orders-out に合わせる）
[KsqlStream]
[KsqlTopic("orders-out")]
public class OrderOut {
    [KsqlKey] public string OrderId { get; set; } = default!;
    public string CustomerId { get; set; } = default!;
    public decimal Amount { get; set; }
    public DateTime CreatedAt { get; set; }
}

// 逐次処理
await foreach (var (msg, headers) in consumer.ConsumeAsync<OrderOut>(ct))
{
    // ここで通常ロジック
}
```

> TABLE系（`orders-by-customer`）を読む場合  
> - **更新（同一キーの上書き）**が来るため、集計結果を**最新値として扱う**実装にする  
> - ストリームとして扱いたい場合は、別途 `EMIT CHANGES` な SELECT を噛ませてストリーム化（`CREATE STREAM ... AS SELECT * FROM orders_by_customer EMIT CHANGES;`）

---

# テストの型（インテグレーション最小）
1) `CREATE STREAM orders_in` → `CREATE STREAM orders_out AS SELECT ...` を REST で登録  
2) `AddAsync<OrderIn>` で数件投入  
3) `orders-out` を 1 コンシューマで subscribe → 期待形で届くかアサート  
4) （TABLEを使う場合は）同一 `CustomerId` で複数件投入 → `orders-by-customer` で合計が更新されるのを検証

---

# 運用のコツ
- **ストリーム名**：`snake_case`、**トピック名**：`kebab-case` と決めておくとクォート事故を避けられる  
- **フォーマット**：まずは **AVRO**（TopicNameStrategy既定）で統一。DLQだけ別運用でもOK  
- **保守**：`SHOW QUERIES;` で永続クエリを棚卸し、`TERMINATE <query_id>;` で停止可  
- **クリーンアップ**：不要になったシンクトピックは明示削除（誤 subscribe 防止）

---

必要なら、このパターンを**あなたの具体スキーマ**に落として `curl` スクリプトセット（登録/削除/確認）まで作ります。トピック名と列定義を渡してくれれば、そのまま回る形に仕上げるよ。

（[[REDACTED_PHONE]]:15 JST）
### assistant — 2025-08-10T10:06:36.829980+00:00

_(empty)_
### assistant — 2025-08-10T10:06:41.461692+00:00

_(empty)_
### user — 2025-08-10T10:07:29.139000+00:00

CREATE STREAM orders_in → CREATE STREAM orders_out AS SELECT ... を REST で登録
ここはOnModelCreatingでおこなう
### assistant — 2025-08-10T10:07:29.389166+00:00

了解。**永続クエリの登録を `OnModelCreating` でやる前提**の、実装“型”を置きます。  
ポイントは「①ソースSTREAM定義」「②CSAS/CTASを“モデル定義として”登録」「③起動時に一度だけ反映（IF NOT EXISTS/ハッシュで冪等化）」です。

# 最小パターン（C#・イメージ）

```csharp
// 例: あなたの DSL が DbContext 風なら
public sealed class OrderContext : KsqlContext
{
    public OrderContext(KsqlDslOptions opt, ILoggerFactory lf) : base(opt, lf) { }

    protected override void OnModelCreating(IKsqlModelBuilder model)
    {
        // 1) ソース: アプリが AddAsync で書く先（orders-in）
        model.Stream<OrderIn>(s => s
            .WithTopic("orders-in")           // Kafkaトピック名（kebab-case推奨）
            .WithKeyFormat("AVRO")
            .WithValueFormat("AVRO")
            .WithTimestamp(x => x.CreatedAt)
            .WithPartitions(3)
            .WithReplicas(1)
        );

        // 2) 永続クエリ: CSAS（orders_out を常時吐く）
        model.CreateStreamAsSelect("orders_out", q => q
            .WithTopic("orders-out")          // シンク先トピック
            .WithValueFormat("AVRO")
            .WithPartitions(3)
            .WithReplicas(1)
            .Select(@"
                SELECT
                  OrderId,
                  UCASE(CustomerId) AS CustomerId,
                  Amount,
                  CreatedAt
                FROM orders_in
                EMIT CHANGES")
            .IfNotExists()                    // 冪等化
        );

        // 3) 例：集計TABLE（必要なら）
        model.CreateTableAsSelect("orders_by_customer", q => q
            .WithTopic("orders-by-customer")
            .WithValueFormat("AVRO")
            .Select(@"
                SELECT CustomerId, SUM(Amount) AS TotalAmount
                FROM orders_in
                GROUP BY CustomerId
                EMIT CHANGES")
            .IfNotExists()
        );
    }
}
```

> DSLに `CreateStreamAsSelect`/`CreateTableAsSelect` が無ければ、**生KSQL をモデルに積む**メソッドを1つ用意してください：
> ```csharp
> model.Sql(@"
>   CREATE STREAM IF NOT EXISTS orders_out WITH (KAFKA_TOPIC='orders-out', VALUE_FORMAT='AVRO', PARTITIONS=3, REPLICAS=1)
>   AS SELECT OrderId, UCASE(CustomerId) AS CustomerId, Amount, CreatedAt FROM orders_in EMIT CHANGES;");
> ```
> 名前は `orders_out` のように**識別子は `_`**、トピック名は **`orders-out`** にしてクォート事故を防ぎます。

# 起動時適用（冪等・差分）
- 既にお持ちの `InitializeWithSchemaRegistration()`／`RegisterSchemasAndMaterializeAsync()` のフローに、  
  **モデル→DDL/クエリの生成→/ksql へ反映**を入れてください。
- 冪等戦略：
  - **`IF NOT EXISTS` を付ける**（STREAM/TABLE/CSAS/CTAS）
  - もしくは **クエリ本文のハッシュ**を `_ksql_migrations`（Kafkaトピック or ksqlDB TABLE）に保存し、**未登録のみ実行**  
    （変更検知が要る場合は `TERMINATE`→再作成の“手動マイグレーション”設計を）

# エンティティ例（送信/受信）

```csharp
// ソース側（AddAsync で投げる）
[KsqlStream]
[KsqlTopic("orders-in")]
public class OrderIn {
    [KsqlKey] public string OrderId { get; set; } = default!;
    public string CustomerId { get; set; } = default!;
    public decimal Amount { get; set; }
    [KsqlTimestamp] public DateTime CreatedAt { get; set; }
}

// シンク側（orders-out を consume するPOCO）
[KsqlStream]
[KsqlTopic("orders-out")]
public class OrderOut {
    [KsqlKey] public string OrderId { get; set; } = default!;
    public string CustomerId { get; set; } = default!;
    public decimal Amount { get; set; }
    public DateTime CreatedAt { get; set; }
}
```

# 動作確認の流れ（最短）
1) アプリ起動 → `OnModelCreating` の定義が `/ksql` に反映（STREAM/CSAS/CTAS作成）  
2) `await ctx.Set<OrderIn>().AddAsync(new OrderIn { ... });` で投入  
3) `orders-out` を通常の consume（既に物理試験済みの `ConsumeAsync<OrderOut>()`）で受信 → 期待形を確認

# よくある落とし穴
- **ハイフン/ドット**：ストリーム名は `snake_case`、トピック名は `kebab-case`。`WITH (KAFKA_TOPIC='…')` 指定を忘れない  
- **フォーマット不一致**：AddAsync 側の AVRO スキーマと `orders_in` 定義を必ず一致させる  
- **CTASの読み方**：TABLEは**上書き更新**が来る（changelog）。必要ならストリームに再投影してから読む

---  

この“型”であれば、「**クエリ登録は `OnModelCreating` に集約**」「**アプリは AddAsync/Consume に集中**」の分業ができます。  
必要なら、あなたのDSL実装名に合わせたコード（`model.Sql` or `model.CreateStreamAsSelect` の具体）に書き換えます。  

（[[REDACTED_PHONE]]:18 JST）
### system — 2025-08-10T10:07:29.702973+00:00

_(empty)_
### assistant — 2025-08-10T10:07:33.085688+00:00

_(empty)_
### assistant — 2025-08-10T10:07:40.300364+00:00

_(empty)_
### user — 2025-08-10T10:08:24.415000+00:00

APIのセットは添付した内容をみて
### assistant — 2025-08-10T10:08:25.772511+00:00

了解、司令。`OnModelCreating` で **クエリ登録→シンク先トピックをアプリで consume** までを一気通貫できるよう、`api_reference.md` にそのまま追記できる **APIセット（ドラフト）** を用意したよ。  
（既存ドキュメントのトーンに合わせて、インタフェース表＋使用例＋注意点で構成）

---

# 追記案：OnModelCreating でのクエリ登録とシンク消費（CSAS/CTAS）

## モデル定義 API（クエリ登録）

| API | 説明 | 対象 | 実装状態 |
|---|---|---|---|
| `model.Stream<T>(cfg => …)` | ソースSTREAM定義（`AddAsync<T>` の投入口を宣言） | Model | 🚧 |
| `model.CreateStreamAsSelect(name, cfg => …)` | **CSAS**: `CREATE STREAM … AS SELECT …` を登録（永続クエリ） | Model | 🚧 |
| `model.CreateTableAsSelect(name, cfg => …)` | **CTAS**: `CREATE TABLE … AS SELECT …` を登録（永続クエリ） | Model | 🚧 |
| `model.Sql(@"…")` | 任意の KSQL 文をモデルへ登録（IF NOT EXISTS 併用推奨） | Model | 🚧 |
### 典型パターン（最小）

```csharp
protected override void OnModelCreating(IKsqlModelBuilder model)
{
    // 1) アプリが AddAsync で書く「ソース」宣言（orders-in）
    model.Stream<OrderIn>(s => s
        .WithTopic("orders-in")
        .WithKeyFormat("AVRO")
        .WithValueFormat("AVRO")
        .WithTimestamp(x => x.CreatedAt)
        .WithPartitions(3)
        .WithReplicas(1)
    );

    // 2) KSQLDB 側で常時変換して「シンク」に吐く（orders-out）
    model.CreateStreamAsSelect("orders_out", q => q
        .WithTopic("orders-out")           // ← アプリはこのトピックを consume する
        .WithValueFormat("AVRO")
        .WithPartitions(3).WithReplicas(1)
        .Select(@"
            SELECT
              OrderId,
              UCASE(CustomerId) AS CustomerId,
              Amount,
              CreatedAt
            FROM orders_in
            EMIT CHANGES")
        .IfNotExists()                      // 冪等化
    );

    // 3) 集計が必要なら TABLE も同様に（任意）
    model.CreateTableAsSelect("orders_by_customer", q => q
        .WithTopic("orders-by-customer")
        .WithValueFormat("AVRO")
        .Select(@"
            SELECT CustomerId, SUM(Amount) AS TotalAmount
            FROM orders_in
            GROUP BY CustomerId
            EMIT CHANGES")
        .IfNotExists()
    );
}
```

> メモ  
> - **ストリーム/テーブル名**は `snake_case` を推奨（`-`/`.` は不可）。  
> - **トピック名**は `kebab-case` を推奨し、`WITH (KAFKA_TOPIC='…')` 相当を `.WithTopic(...)` で必ず明示。  
> - 永続クエリは **IF NOT EXISTS** で冪等化。仕様変更時はマイグレーション方針（`TERMINATE`→再作成）を運用で管理。

## 送受信の定型
### 送信（ソースへ `AddAsync`）
```csharp
[KsqlStream]
[KsqlTopic("orders-in")]
public class OrderIn {
    [KsqlKey] public string OrderId { get; set; } = default!;
    public string CustomerId { get; set; } = default!;
    public decimal Amount { get; set; }
    [KsqlTimestamp] public DateTime CreatedAt { get; set; }
}

await ctx.Set<OrderIn>().AddAsync(new OrderIn {
    OrderId = Guid.NewGuid().ToString("D"),
    CustomerId = "c-001",
    Amount = 123.45m,
    CreatedAt = DateTime.UtcNow
});
```
### 受信（シンクを consume）
```csharp
[KsqlStream]
[KsqlTopic("orders-out")]
public class OrderOut {
    [KsqlKey] public string OrderId { get; set; } = default!;
    public string CustomerId { get; set; } = default!;
    public decimal Amount { get; set; }
    public DateTime CreatedAt { get; set; }
}

// 逐次処理（既存の物理試験スタイルに合わせる）
await foreach (var (msg, headers) in consumer.ConsumeAsync<OrderOut>(ct))
{
    // 本処理…
}
```

> TABLE（`orders-by-customer`）を直接読む場合は **changelog（上書き）** になる点に注意。ストリームとして扱いたい場合は、別途 CSAS で再投影してから consume するのが安全。

## 運用・注意点

- **スキーマ整合**：`OrderIn` の AVRO スキーマと `orders_in` の列定義を一致させる。  
- **互換性**：シンクのスキーマ進化は **BACKWARD** を推奨（必須追加には `default` を付与）。  
- **可観測性**：クエリは `SHOW QUERIES;`、停止は `TERMINATE <query_id>;`。不要トピックは削除。  
- **命名衝突回避**：アプリ側 `Class` 名と KSQL `STREAM` 名は一致させず（`OrderIn` ↔ `orders_in` のように区別）。

---

必要ならこの文面を `api_reference.md` の「Fluent API ガイドライン」直後に新節として差し込めるよう整形するよ。加筆箇所の位置指定があれば、その体裁に合わせて出し直します。

（[[REDACTED_PHONE]]:22 JST）
### system — 2025-08-10T10:08:26.456759+00:00

_(empty)_
### tool — 2025-08-10T10:08:26.492694+00:00

_(empty)_
### tool — 2025-08-10T10:08:26.523878+00:00

fileciteturn3file0

# API Reference (Draft)

この文書は `Kafka.Ksql.Linq` OSS における公開 DSL/API と主要コンポーネントの概要を整理したものです。今後の設計ドキュメントや実装コード、テストコードへの参照基盤として利用します。

## 既定値の参照

- 既定値一覧は [docs_configuration_reference.md](docs_configuration_reference.md) を参照してください。
### テスト設計上の注意
- Kafka メッセージ送信は `Chr.Avro.Confluent` を利用した POCO 型の自動スキーマ連携を推奨します。
- `GROUP BY` を指定した場合は Push Query として `EMIT CHANGES` が自動付与されます。Pull Query や TABLE クエリで `GROUP BY` を利用するとエラーとなります。
- `WINDOW` 句は `GROUP BY` の直後に配置してください。
- `CASE` 式では `THEN`/`ELSE` の型を必ず一致させる必要があります。
- `MIN`/`MAX` などのストリーム専用集計関数を含む場合、クエリは自動的に `CREATE STREAM` として生成されます。`CREATE TABLE` を明示するとエラーになります。

## Context クラスとベースインタフェース

| API                   | 説明                                   | 対象レイヤ | 実装状態 |
|------------------------|----------------------------------------|------------|---------|
| `IKsqlContext`         | コンテキスト操作の抽象インタフェース   | Context    | ✅      |
| `KafkaContextCore`     | `IKsqlContext` 実装の基底クラス        | Context    | ✅      |
| `KsqlContext`          | Kafka連携を統合した抽象コンテキスト    | Context    | ✅      |
| `KsqlContextBuilder`   | `KsqlContextOptions` 構築用ビルダー   | Application| ✅      |
| `KsqlContextOptions`   | スキーマレジストリ等の設定保持        | Application| ✅      |
| `IEventSet<T>`         | LINQ/Streaming操作の共通インタフェース| Stream/Table| ✅     |
| `IManualCommitMessage<T>` | 手動コミットメッセージ             | Subscription| ✅     |

## LINQ 風 DSL 一覧

| DSL メソッド                   | 説明                          | 戻り値型                          | 対象レイヤ    | 実装状態 |
|--------------------------------|-------------------------------|-----------------------------------|---------------|---------|
| `.Where(predicate)`            | 条件フィルタ                  | `IEventSet<T>`                    | Stream/Table  | ✅      |
| `.Window(WindowDef \| TimeSpan)` | タイムウィンドウ指定       | `IQueryable<T>`                   | Stream        | ✅      |
| `.Window(int minutes)`          | `WindowMinutes`によるフィルタ  | `IEntitySet<T>`                  | Stream/Table  | ✅      |
| `.Window().BasedOn<TSchedule>(keySelector, ?openProp, ?closeProp)` | `[ScheduleRange]` 属性、または `openProp`/`closeProp` パラメータで開始/終了を示すスケジュールPOCOに基づきウィンドウを生成 | `IQueryable<T>` | Stream | ✅ |
| `.GroupBy(...)`                | グループ化および集約          | `IEventSet<IGrouping<TKey, T>>`   | Stream/Table  | ✅      |
| `.OnError(ErrorAction)`        | エラー処理方針指定            | `EventSet<T>`                     | Stream        | ✅      |
| `.WithRetry(int)`              | リトライ設定                  | `EventSet<T>`                     | Stream        | ✅      |
| `.StartErrorHandling()`        | エラーチェーン開始            | `IErrorHandlingChain<T>`          | Stream        | ✅      |
| `.WithManualCommit()`          | 手動コミットモード切替        | `IEntityBuilder<T>`               | Subscription  | ✅      |
| `.Limit(int)`                  | **保持件数制限。Table型(Set<T>)でのみ利用可。OnModelCreatingで定義し、超過分は自動削除される。** | `IEntitySet<T>`                  | Table  | ✅      |

- `ToList`/`ToListAsync` は Pull Query として実行されます【F:src/Query/Pipeline/DMLQueryGenerator.cs†L27-L34】。
- `WithManualCommit()` を指定しない `ForEachAsync()` は自動コミット動作となります【F:docs/old/manual_commit.md†L1-L23】。
- `OnError(ErrorAction.DLQ)` を指定すると DLQ トピックへ送信されます【F:docs/old/defaults.md†L52-L52】。
- `ctx.Set<DlqEnvelope>()` を指定すると DLQ ストリームを取得できます。DLQ は無限ログのため `Take()` や `ToListAsync()` などの一括取得 API は利用できず、`ForEachAsync()` のみサポートします。また DLQ ストリームで `.OnError(ErrorAction.DLQ)` を指定すると無限ループになるため禁止されています。
- Messaging クラス自体は DLQ 送信処理を持たず、`ErrorOccurred`/`DeserializationError`/`ProduceError` などのイベントを通じて外部で DLQ 送信を行います。
- DLQ の内容確認は `ctx.Dlq.ReadAsync(...)` を利用してください。
- `Set<T>().Limit(n)` は Table 型の保持件数を制限する DSL です。`OnModelCreating` 内で指定し、超過分のレコードは自動削除されます。Stream 型や実行時クエリでは利用できません。
- バーエンティティでは `WithWindow().Select<TBar>()` で `BarTime` に代入した式が自動的に記録され、`Limit` の並び替えに使用されます。
- `RemoveAsync(key)` は値 `null` のトムストーンを送り、KTable やキャッシュから該当キーのデータを削除します。
- `.Window().BasedOn<TSchedule>` を用いる場合、バーは `[ScheduleRange]` 属性、または `openProp`/`closeProp` パラメータで示された `Open` ～ `Close` の範囲に含まれるデータのみで構成されます。日足生成で `Close` が 6:30 のときは、6:30 未満のデータが当日の終値として扱われます。
- バーやウィンドウ定義は必ず `KafkaKsqlContext.OnModelCreating` 内で宣言してください。アプリケーション側では定義済みの `Set<T>` を参照するだけです。
- `WithWindow<Rate, MarketSchedule>()` に続けて `.Select<RateCandle>()` を呼び出すことで、レートからバーエンティティを構成できます。
- `WithWindow<TEntity, TSchedule>(windows, timeSelector, rateKey, scheduleKey)` として `timeSelector` 引数でウィンドウを区切る時刻プロパティを明示します。

これらの戻り値型を把握することで、DSLチェーンにおける次の操作を判断しやすくなります。特に `OnError()` や `WithRetry()` は `EventSet<T>` を返すため、続けて `IEventSet` 系メソッドを利用できます。
### ToQuery チェーン

| メソッド | 説明 | 注意点 |
|---|---|---|
| `.From<T>()` | ビュー定義の開始 | - |
| `.Join<T2>(condition)` | 2テーブルまでの内部結合 | 後続に `.Where` 必須 |
| `.Where(predicate)` | 結合条件やフィルタ | `.Join` 使用時は必須 |
| `.Select(selector)` | 投影 | 呼び出し順序は `From`→`Join?`→`Where`→`Select` |

これらは `modelBuilder.Entity<T>().ToQuery(q => q ... )` 内で使用し、`KsqlQueryModel` に変換されます。

## 属性 (Attribute) 定義

| 属性                       | 役割                           | 実装状態 |
|----------------------------|--------------------------------|---------|
| `MaxLengthAttribute`       | 文字列長制限                   | ✅      |
| `ScheduleRangeAttribute`   | 取引開始・終了をまとめて指定する属性 | 🚧 |

`WithDeadLetterQueue()` は過去の設計で提案されましたが、現在は `OnError(ErrorAction.DLQ)` に置き換えられています。

## 構成オプションとビルダー

| API                        | 説明                             | 実装状態 |
|----------------------------|----------------------------------|---------|
| `KsqlDslOptions`           | DLQ 設定や ValidationMode など DSL 全体の構成を保持 | ✅ |
| `ModelBuilder`             | POCO から `EntityModel` を構築するビルダー | ✅ |
| `KafkaAdminService`        | DLQ トピック作成などの管理操作  | ✅      |
| `AvroOperationRetrySettings`| Avro操作ごとのリトライ設定     | ✅      |
| `AvroRetryPolicy`          | リトライ回数や遅延などの詳細ポリシー | ✅  |

`KsqlDslOptions.DlqTopicName` は既定で `"dead-letter-queue"` です【F:src/Core/Dlq/DlqProducer.cs†L248-L256】。

<a id="fluent-api-list"></a>### Fluent API 一覧

| メソッド | 説明 |
|----------|------|
| `Entity<T>(readOnly = false, writeOnly = false)` | エンティティ登録とアクセスモード指定 |
| `.AsStream()` | ストリーム型として登録 |
| `.AsTable(topicName = null, useCache = true)` | テーブル型として登録 |
| `.WithManualCommit()` | 手動コミットモード有効化 |
| `.ToQuery(build)` | 新DSLでのクエリ定義 |

<a id="fluent-api-guide"></a>
## Fluent API ガイドライン

POCO モデルを Fluent API で構成する際の設計指針と移行フローをまとめます。属性ベースからの移行後は
`IEntityBuilder<T>` を用いて宣言的に設定を行います。
### 1. 基本方針
 - `[KsqlTopic]` や `[KsqlKey]` などの属性でトピック名やキー順序を宣言します。
 - Fluent API はクエリ構築やモード指定のみを担い、スキーマ情報は属性に集約します。
 - エンティティ登録時は `readonly` `writeonly` `readwrite` の 3 種類で役割を指定し、未指定時は `readwrite` とみなします。
### 2. 推奨記述例
```csharp
[KsqlTopic("orders")]
public class Order
{
    [KsqlKey(Order = 0)]
    public int Id { get; set; }
    public decimal Amount { get; set; }
}

void OnModelCreating(ModelBuilder builder)
{
    builder.Entity<Order>(writeOnly: true);
}
```
`[KsqlTopic]` や `[KsqlDecimal]` 属性でトピックや精度を宣言できます。
### 3. 既存 POCO → Fluent API 移行フロー
1. POCO へ `[KsqlTopic]` と `[KsqlKey]` を付与してスキーマ情報を記述する。
2. `OnModelCreating` では `Entity<T>()` の登録のみ行い、その他は属性に委ねる。
3. テストを実行してキー順序やトピック設定が正しいか確認する。
   旧属性に関する詳細は `docs/namespaces/core_namespace_doc.md` を参照してください。
### 4. MappingManager との連携
`MappingManager` を利用して key/value を抽出する例です。詳細は `docs/architecture/key_value_flow.md` を参照してください。
```csharp
var ctx = new MyKsqlContext(options);
var mapping = ctx.MappingManager;
var entity = new Order { Id = 1, Amount = 100 };
var (key, value) = mapping.ExtractKeyValue(entity);
await ctx.AddAsync(entity, headers: new Dictionary<string, string> { ["is_dummy"] = "true" });
```
#### ベストプラクティス
- エンティティ登録は `OnModelCreating` 内で一括定義する。
- `MappingManager` を毎回 `new` しない。DI コンテナで共有し、モデル登録漏れを防ぐ。
### 5. 追加検討事項
- `[KsqlTopic]` 属性で指定できない詳細設定の扱いを検討中。
- MappingManager のキャッシュ戦略（スレッドセーフな実装範囲）を確定する必要あり。
### 6. サンプル実装での気づき
- `AddSampleModels` 拡張で `MappingManager` への登録をまとめると漏れ防止になる。
- 複合キーは `Dictionary<string, object>` として抽出されるため、型安全ラッパーの検討余地あり。
- `Dictionary<string,string>` 型のプロパティは Avro の `map` (`{"type": "map", "values": "string"}`) として扱われる。
  - キー・値ともに文字列のみサポート。その他の型やネスト構造は未対応。
  - `null` は許容されないため、プロパティは空ディクショナリで初期化する。
- `decimal` プロパティは `DecimalPrecisionConfig` で設定された `precision`/`scale` を持つ Avro `bytes` (`logicalType: decimal`) として生成される。
- 複数エンティティを登録するヘルパーがあると `OnModelCreating` の記述量を抑えられる。
### 7. AddAsync 統一に伴うポイント
- メッセージ送信 API は `AddAsync` に一本化した。旧 `ProduceAsync` は廃止予定。
- LINQ クエリ解析から `MappingManager.ExtractKeyValue()` を経由し `AddAsync` を呼び出す流れをサンプル化。
- 詳細なコード例は [architecture/query_to_addasync_sample.md](architecture/query_to_addasync_sample.md) を参照。

## ToQuery DSL

`ToQuery` は View 定義専用の Fluent API です。`From<T>()` を起点に `Join<T2>()`、`Where(...)`、`Select(...)` を順に呼び出してチェーンを構築します。JOIN は2テーブルまでサポートされており、結合条件は `Join` メソッド内で指定します。必要に応じて `Where` で追加のフィルタリングを行えます。呼び出し順序が守られない場合や未サポートの JOIN 数は構文検証で例外となります。

```csharp
modelBuilder.Entity<OrderSummary>().ToQuery(q => q
    .From<Order>()
    .Join<Customer>((o, c) => o.CustomerId == c.Id)
    .Where((o, c) => c.IsActive)
    .Select((o, c) => new OrderSummary { OrderId = o.Id, CustomerName = c.Name }));
```

`.ToQuery(...)` で得られた `KsqlQueryModel` は `KsqlContext` 初期化時に `CREATE STREAM/TABLE AS SELECT` 文として利用されます。

## エラーハンドリング

| API / Enum                 | 説明                           | 実装状態 |
|----------------------------|--------------------------------|---------|
| `ErrorAction` (Skip/Retry/DLQ) | エラー時の基本アクション    | ✅      |
| `ErrorHandlingPolicy`      | リトライ回数やカスタムハンドラ設定を保持 | ✅ |
| `ErrorHandlingExtensions`  | `.OnError()` `.WithRetryWhen()` 等の拡張 | ✅ |
| `DlqProducer` / `DlqEnvelope` | DLQ 送信処理               | ✅      |
| `DlqOptions`    | DLQ トピックの保持期間等を指定 | ✅      |
### DLQ Read API（Avro固定）

```csharp
public interface IKsqlContext
{
    IDlqClient Dlq { get; }
}

public interface IDlqClient
{
    IAsyncEnumerable<DlqRecord> ReadAsync(
        DlqReadOptions? options = null,
        CancellationToken ct = default);
}
```

**使い方サンプル**

```csharp
await foreach (var rec in ctx.Dlq.ReadAsync())
{
    Console.WriteLine(rec.RawText);
}
```

**仕様**

- `FromBeginning=true` で earliest へシーク
- `CommitOnRead=true` で 1 件ごとにコミット
- Avro ワイヤフォーマットから RawText を可読化

**既知の制約**

- PayloadFormat は常に `"avro"`
- 再投函は非対応（読むだけ）

## 状態監視・内部機構

| API                         | 説明                             | 実装状態 |
|-----------------------------|----------------------------------|---------|
| `ReadyStateMonitor`         | トピック同期状態の監視           | ✅      |
| `CacheBinding`         | Kafka トピックと Cache の双方向バインディング | ✅ |
| `SchemaRegistryClient`      | スキーマ管理クライアント        | ✅      |
| `ResilientAvroSerializerManager` | Avro操作のリトライ管理     | ✅      |

| `WindowFinalizationManager` | Window最終化処理のタイマー管理  | ✅      |

## 各 API の備考

- `IEventSet<T>.WithRetry()` の実装例は `EventSet.cs` にあります【F:src/EventSet.cs†L238-L258】。
- `OnError` の拡張は `EventSetErrorHandlingExtensions.cs` で提供されています【F:src/EventSetErrorHandlingExtensions.cs†L8-L20】。
- 手動コミットの利用例は [manual_commit.md](old/manual_commit.md) を参照してください。
- `StartErrorHandling()` → `.Map()` → `.WithRetry()` の流れで細かいエラー処理を構築できます。
- `AvroOperationRetrySettings` で SchemaRegistry 操作のリトライ方針を制御します【F:src/Configuration/Options/AvroOperationRetrySettings.cs†L8-L33】。
### tool — 2025-08-10T10:08:26.607152+00:00

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.
### assistant — 2025-08-10T10:08:29.365413+00:00

_(empty)_
### assistant — 2025-08-10T10:08:37.714516+00:00

_(empty)_
### user — 2025-08-10T10:09:37.714000+00:00

.Select(@"
            SELECT CustomerId, SUM(Amount) AS TotalAmount
            FROM orders_in
            GROUP BY CustomerId
            EMIT CHANGES")
これがおかしい
Linq式で構成する。api仕様書から判断できなければドキュメントをまず更新する
### system — 2025-08-10T10:09:38.225099+00:00

_(empty)_
### assistant — 2025-08-10T10:09:38.368939+00:00

司令、了解。**SQL文字列ではなく LINQ でクエリを宣言**できるように、まずドキュメントを更新する案を出します。これをそのまま `api_reference.md` に追記すれば、実装側も迷いません。

---

# 追記案：LINQ ベースの KSQL クエリ宣言（CSAS/CTAS）

## 概要
`OnModelCreating` では、**生SQLは使わず LINQ でクエリを構成**します。  
これにより C# 型安全・リファクタリング耐性・IntelliSense を確保しつつ、KSQL の CSAS/CTAS を定義できます。

---

## API サマリ

```csharp
// CSAS: CREATE STREAM ... AS SELECT ...
model.CreateStream<TOut>("stream_name", q => q
    .From<TSource>()                         // 入力STREAM
    .Where(x => /* 条件 */)                 // 任意
    .Select(x => new TOut { /* 射影 */ })   // 必須（匿名型不可、TOut へ）
    .EmitChanges()                           // 必須（STREAM）
    .WithTopic("kebab-case-topic")           // 必須
    .WithValueFormat("AVRO")                 // 既定: AVRO
    .WithPartitions(3).WithReplicas(1)
    .IfNotExists()                           // 任意（冪等化）
);

// CTAS: CREATE TABLE ... AS SELECT ...
model.CreateTable<TOut>("table_name", q => q
    .From<TSource>()                         // 入力STREAM
    .GroupBy(x => x.CustomerId)              // 集計キー（単一キー）
    .Select(g => new TOut {                  // g はグループ
        CustomerId  = g.Key,                 // キーは g.Key
        TotalAmount = g.Sum(x => x.Amount)   // 集計関数（Sum/Count/Min/Max/Avg）
    })
    .WithTopic("kebab-case-topic")
    .WithValueFormat("AVRO")
    .WithPartitions(3).WithReplicas(1)
    .IfNotExists()
);
```
### サポート演算
- `Where`, `Select`, `SelectMany`（必要に応じ）、`GroupBy`（単一キー）
- 集計：`g.Sum(...)`, `g.Count()`, `g.Min(...)`, `g.Max(...)`, `g.Avg(...)`
- 別名：**`TOut` のプロパティ名**が列名になります
- `EmitChanges()`：**STREAM のとき必須**。TABLE（CTAS）では不要
- フォーマット/トピック/パーティション/レプリカは `With*` で指定
### 未対応/注意
- 複合キー `GroupBy(x => new { x.A, x.B })` は当面非対応（必要なら拡張予定）
- JOIN/ウィンドウは次期拡張（本節のスコープ外）

---

## 例：CSAS（正規化ストリーム）

**目的**：`orders_in` から `orders_out` へ、CustomerId を大文字化してミラーする。

```csharp
model.CreateStream<OrderOut>("orders_out", q => q
    .From<OrderIn>()
    .Select(x => new OrderOut {
        OrderId     = x.OrderId,                 // KEY 列を必ず引き継ぐ
        CustomerId  = x.CustomerId.ToUpper(),
        Amount      = x.Amount,
        CreatedAt   = x.CreatedAt
    })
    .EmitChanges()
    .WithTopic("orders-out")
    .WithValueFormat("AVRO")
    .WithPartitions(3).WithReplicas(1)
    .IfNotExists()
);
```

> 置き換え前（NG例・SQL直書き）  
> ```sql
> CREATE STREAM orders_out AS
> SELECT OrderId, UCASE(CustomerId) AS CustomerId, Amount, CreatedAt FROM orders_in EMIT CHANGES;
> ```

---

## 例：CTAS（集計テーブル）

**目的**：顧客別の合計金額を集計し `orders-by-customer` に永続化する。

```csharp
model.CreateTable<OrdersByCustomer>("orders_by_customer", q => q
    .From<OrderIn>()
    .GroupBy(x => x.CustomerId)
    .Select(g => new OrdersByCustomer {
        CustomerId  = g.Key,                  // ← GroupBy のキー
        TotalAmount = g.Sum(x => x.Amount)    // ← 集計関数
    })
    .WithTopic("orders-by-customer")
    .WithValueFormat("AVRO")
    .WithPartitions(3).WithReplicas(1)
    .IfNotExists()
);
```
### 対応する POCO
```csharp
[KsqlTable] // または KsqlStream（消費側の用途に合わせる）
[KsqlTopic("orders-by-customer")]
public class OrdersByCustomer
{
    [KsqlKey] public string  CustomerId   { get; set; } = default!;
    public decimal           TotalAmount  { get; set; }
}
```

---

## 型マッピングとルール

- **キー列**：`TOut` の `[KsqlKey]` プロパティで決まる  
  - CSAS では **入力のキー**を必ず引き継ぐ（別の列をキーにしたい場合は再パーティションが必要なため将来拡張）
  - CTAS では `GroupBy` のキーが `[KsqlKey]` に対応
- **時間列**：`[KsqlTimestamp]` があれば `TIMESTAMP='...'` を自動付与
- **数値/小数**：`decimal` は `DECIMAL(precision, scale)`（未指定は実装既定に従う）
- **文字列**：`string` → `STRING`
- **バイト列**：`byte[]` → `BYTES`

---

## 冪等化とマイグレーション
- `IfNotExists()` を付与すると初回のみ作成し、2回目以降はスキップ  
- クエリ内容を変更する場合は、**`TERMINATE <query_id>` → 再作成**のマイグレーション運用を推奨  
  - 変更検知（ハッシュ）や `_ksql_migrations` の設計は別節参照

---

## 送受信（再掲）

**送信**：`OrderIn` を `AddAsync` で投入 → KSQLDB が処理 → `orders-out`/`orders-by-customer` に出力  
**受信**：シンク側 POCO（`OrderOut`/`OrdersByCustomer`）で `ConsumeAsync<T>()` を使う

---

## FAQ
- **Q: 複数列で GroupBy したい**  
  A: 現行 LINQ DSL は単一キーのみ。複合キーは次期拡張。必要なら一時的に SQL 直書きの `model.Sql()` を使用。
- **Q: `EmitChanges()` を付けるか迷う**  
  A: **STREAM（CSAS）は必須**。TABLE（CTAS）は不要。
- **Q: フォーマットは AVRO 以外を使える？**  
  A: 現行は AVRO 推奨。`WithValueFormat("JSON")` などは将来拡張。

---

このドキュメント差分を入れれば、**「Select に SQL を直書きしない」**方針が明確になります。  
この方針でOKなら、Codex向けに **実装タスク化（LINQ→KSQL の式木トランスレータ・対応演算）** の依頼文もすぐ用意します。

（[[REDACTED_PHONE]]:26 JST）
### assistant — 2025-08-10T10:09:42.028141+00:00

_(empty)_
### assistant — 2025-08-10T10:09:46.946841+00:00

_(empty)_
