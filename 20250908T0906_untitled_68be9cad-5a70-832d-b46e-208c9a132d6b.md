---
conversation_id: "68be9cad-5a70-832d-b46e-208c9a132d6b"
title: "多段ロールアップ設計"
create_time: 1757322415.90477
update_time: 1757323010.5179
language_guess: "en"
num_messages: 30
part_index: 1
part_count: 1
---

### system — 2025-09-08T09:06:55.904770+00:00

_(empty)_

### system — 2025-09-08T09:06:55.904770+00:00

_(empty)_

### user — 2025-09-08T09:06:55.904770+00:00

_(empty)_

### system — 2025-09-08T09:06:55.904770+00:00

_(empty)_

### user — 2025-09-08T09:06:56.260000+00:00

DerivationPlanner.Plan は 1 分・週次以外のすべてのタイムフレームに対し、ライブ入力を *_1m_live に固定しています
そのため .Tumbling() で 5m/15m/30m だけを指定すると、計画内に存在しない *_1m_live を参照してしまいロールアップが組めません。
「1m が含まれる」「すべて 1m の倍数」という前提でしか成立しない設計なので、実際には多段ライブロールアップは機能しません。

テスト
⚠️ No tests were executed (read-only review)

Notes
Kafka.Ksql.Linq.Tests.Integration を含むテストは実行していません。

常に_1m_liveを作る構成とすればよいのではないか
コピーする
_1m_live を常に生成しても、上位のライブ足（5m、15m など）がすべて 1m から集約されてしまい、多段ロールアップの意味がなくなります。
本来は「直下の live」を順に参照することで、データ量・遅延・重複処理を抑えた段階的ロールアップを行う設計です。
したがって、問題解決には各粒度が直前の粒度からライブを派生するように計画を見直す必要があり、単に _1m_live を常設するだけでは対応できません。

### system — 2025-09-08T09:06:58.083462+00:00

_(empty)_

### system — 2025-09-08T09:06:58.113983+00:00

_(empty)_

### assistant — 2025-09-08T09:06:58.117449+00:00

_(empty)_

### assistant — 2025-09-08T09:06:58.837159+00:00

_(empty)_

### assistant — 2025-09-08T09:07:23.950734+00:00

やあ、司令。指摘どおり、現状の DerivationPlanner.Plan は「1m（と週次を除く）」に固定して live 入力を *_1m_live に寄せているため、5m/15m/30m だけを選ぶと親が不在になりロールアップ DAG が崩れます。恒常的に *_1m_live を作っても「すべてが 1m からの集約」になって多段の旨味（遅延・重複処理・コスト最適化）が消えるので根本解決にはなりません。

以下、「直下粒度から順に live を派生させる」前提での設計差し替え案です。

# 目標
- 入力: 要求されたタイムフレーム集合 S（例: {5m,15m,30m}）
- 出力: 「各 live_X が“直下の live_Y”から派生する」DAG と DDL 生成用メタ（親子関係／WINDOW サイズ／ロール）
- 制約: f が g の整数倍（f % g == 0）で、かつ“同じアンカー体系”に属すること  
  （minutes 系, hours 系, days 系, schedule 週次系は混在させない。週次はスケジュールアンカー前提で別系統）

---

# アルゴリズム（Planner の置換）

## 1) フレーム定義
- Frame = {Duration: TimeSpan, UnitGroup: Minutes|Hours|Days|Schedule, NameSuffix: "1m"/"5m"/"15m"/"1h"/"1d"/"1w" …}
- 「週次」は `UnitGroup=Schedule`（マーケットカレンダー等のアンカー）。minutes/hours/days とは別グループ扱い。

## 2) 親選択規則（直下倍数チェーン）
- `frames = S` を UnitGroup ごとに分離し、各グループ内で昇順ソート。
- グループごとに:
  - 最小の f_min は「ベース live」。親は“物理入力ストリーム”（例: `deduprates`）。
  - それ以外の f について、`parent = max{ g in frames | g < f && f % g == 0 }` を選ぶ。  
    見つからなければ **エラー** とする（同グループ内に妥当な直下粒度なし → 多段成立しないため）。
- minutes と hours の“相互倍数”は **許可しない**（1h=60m は数学的には倍数でも、実運用では minutes 系 vs hours 系の builder/命名・WITH/GRACE の既定が異なるため、明示的にどちらかへ統一指定を要求する）

## 3) 命名・ロール
- live 名: `<base>_<suffix>_live`（例: `bar_5m_live`）  
  final/aggfinal は既存規則に従う（ここでは live チェーンの話に限定）。
- 各 f の DDL 入力は:
  - 親が“物理”なら FROM `<source>` WINDOW TUMBLING (SIZE f) EMIT CHANGES
  - 親が“live_g”なら FROM `<name_of_live_g>` WINDOW TUMBLING (SIZE f/g * g) …ではなく **SIZE=f**（“再ウィンドウ”ではなく「f 粒度のタムリング集約」を親ストリームに対して行う）。  
    ※ つまり 15m は 5m_live を **入力**に取りつつ、**15m の WINDOW**で再集約する。
- キーは `GROUP BY` に必要な key 列（BROKER, SYMBOL 等）＋ `WINDOWSTART->BucketStart` の取り扱いは既存の WindowedQueryBuilder の投影ロジックに準拠（live で WINDOWSTART を投影するかは現在のポリシーに合わせる。最終的な集約が欲しい AggFinal だけに限定しているならそのままでOK。live で BucketStart が必要ならフラグで切替）。

---

# 擬似コード（C#, Planner 核心イメージ）

```csharp
public IReadOnlyList<DerivationNode> Plan(
    IEnumerable<TimeFrame> requestedFrames,
    string baseStream // e.g., "deduprates"
)
{
    var groups = requestedFrames.GroupBy(f => f.UnitGroup)
                                .ToDictionary(g => g.Key, g => g.OrderBy(x => x.Duration).ToList());

    var nodes = new List<DerivationNode>();

    foreach (var (group, frames) in groups)
    {
        // 最小粒度をベースとして物理入力から派生
        var f0 = frames.First();
        nodes.Add(NodeForLive(f0, parent: Source(baseStream)));

        for (int i = 1; i < frames.Count; i++)
        {
            var f = frames[i];
            var parent = frames.Take(i).Where(g => f.Duration % g.Duration == TimeSpan.Zero)
                                .DefaultIfEmpty(null).MaxBy(g => g?.Duration);
            if (parent == null)
            {
                throw new InvalidOperationException(
                  $"No valid parent for {f}. Provide an immediate divisor in same UnitGroup.");
            }

            nodes.Add(NodeForLive(f, parent: Live(parent)));
        }
    }

    return nodes;
}

private DerivationNode NodeForLive(TimeFrame f, DerivationInput parent)
{
    // 名前: <base>_<suffix>_live は上位層で決定してもよい
    var name = $"{_baseName}_{f.Suffix}_live";

    return new DerivationNode
    {
        Name = name,
        Role = QueryRole.Live,
        Window = Window.Tumbling(f.Duration),
        From = parent, // Source(baseStream) か Live(<lower>)
        // 他: WITH 設定、KEY_FORMAT/VALUE_FORMAT、EMIT、GRACE 既定など
    };
}
```

> 補足  
> - UnitGroup を跨ぐ倍数関係（例: 1h ← 5m）は Planner で禁止（混在は明示追加が必要）。  
> - 週次（Schedule）は Minutes/Hours/Days と完全分離。`MarketSchedule` 前提のアンカーでのみ成立。週次 live は「日次 live のロールアップ」ではなく、**スケジュール・バケット**をキーに native に集約する想定に寄せるのが安全。

---

# 実装ポイント（差分）

1) **DerivationPlanner.Plan の 1m 固定撤廃**  
   - 上記の直下倍数チェーンを実装。1m を特別扱いしない。  
   - minutes/hours/days/schedule の **グループ別 DAG** を複数作れるように。

2) **Validation を強化**  
   - 非倍数（例: {7m, 15m}）や“親不在”（{15m,30m} で 5m が無い）を明示的にエラー化。  
   - グループ混在（minutes と hours を同系列に載せる等）を警告 or エラー。

3) **WindowedQueryBuilder / QueryAdapter の入力元切替**  
   - 現在は「すべて *_1m_live を FROM」にしている箇所を、Planner の親参照に従って **From を差し替え**。  
   - Final/AggFinal の COMPOSE や EMIT FINAL まわりのバイパスは、既存の「Final は COMPOSE しない」修正方針に合わせて呼び出し側の Role ハンドリングで吸収。

4) **命名規約の一貫性**  
   - `bar_5m_live` → `bar_15m_live` → `bar_30m_live` と、親が“直下 live”になるように From を解決。  
   - tests から「親テーブル（ストリーム）名」をアサートできるよう StatementBuilder にメタ（ParentName）を保持。

---

# テスト戦略（最小で効くやつ）

- 単体（Planner）
  - 入力 `{5m, 15m, 30m}` → edges: 5m←source, 15m←5m, 30m←15m
  - 入力 `{15m, 30m}` → edges: 15m←source, 30m←15m（OK）
  - 入力 `{30m}` → edge: 30m←source（OK）
  - 入力 `{7m, 14m}` → edges: 7m←source, 14m←7m（OK）
  - 入力 `{7m, 15m}` → 15m の親なし ⇒ 例外
  - 入力 `{5m, 1h}`（minutes & hours 混在）⇒ 警告 or 例外（ポリシーで固定）

- 結合（StatementBuilder）
  - 15m_live の FROM が `bar_5m_live` になっていること  
  - 30m_live の FROM が `bar_15m_live` になっていること  
  - WINDOW 句がそれぞれ SIZE=15 MINUTES / 30 MINUTES であること

- 物理（最短 E2E）
  - 5m_live にだけ produce しても、15m_live/30m_live が **親チェーン**に従って埋まること
  - 1m を作らなくても 5m→15m→30m が動くこと

---

# 週次の扱い（参考）
- 週次は schedule アンカー（例: `MarketSchedule`）での TUMBLING をネイティブに切る。  
- 「日次 live → 週次 live」のロールアップは、営業日欠損・祝日織り込みの再集約設計が煩雑なので **非推奨**。  
- 週次は minutes/hours/days 系と **別系統**でプランニングする（混在禁止）。

---

# 移行手順（小さく入れる）
1. Planner の新ロジックを feature flag（例: `Plan.UseChainedLiveParents=true`）で導入。  
2. 最小セット `{5m,15m,30m}` の Integration を先にグリーン化。  
3. 既存の *_1m_live 固定コードを段階的に削除。  
4. Final/AggFinal の COMPOSE 抜きの修正が既にあるなら、それに追随して WindowedQueryBuilder の呼び出し経路だけ調整。

---

この差し替えで、「1m が含まれる」「1m の倍数」という前提に依存せず、指定された粒度だけで **多段ライブロールアップ**が機能します。必要なら、この仕様で PR 用の差分（Planner + Builder 呼び出し部 + 単体/結合テスト雛形）まで私がまとめます。

送信日時（JST）: [[REDACTED_PHONE]]:07

### assistant — 2025-09-08T09:07:25.771869+00:00

_(empty)_

### user — 2025-09-08T09:13:19.944000+00:00

rollupはなりたたない？

### assistant — 2025-09-08T09:13:20.169260+00:00

結論：**“今の実装のまま”だと多段ロールアップは成立しません**（常に `_1m_live` を親に固定しているため）。  
でも、**Planner を「直下粒度チェーン」に変えれば成立します**。つまり `5m ← 15m ← 30m` のように、各 live が**ひとつ下の live**を FROM で参照して再集約すればOK。

# 成立条件（最小セット）
- 同じ系（minutes なら minutes だけ）で、**上位が下位の整数倍**。例: `{5m, 15m, 30m}` は可。
- Planner が **親選択＝“最大の直下倍数”** を使う（`15m` は `5m_live` を、`30m` は `15m_live` を参照）。
- 各段で **WINDOW TUMBLING(SIZE=その段の粒度)** を掛け直す（親の“5m窓”をそのまま使わない）。
- `GROUP BY` のキーが妥当（BROKER/SYMBOL 等）。`BucketStart` が必要なら投影ポリシーで付与。

# いま必要な最小改修
1) DerivationPlanner.Plan の「常に `_1m_live`」を廃止し、**直下倍数チェーン**に置換  
2) StatementBuilder が **FROM 親 live** と **WINDOW サイズ**を正しく出すかをテストで検証  
3) minutes/hours/days/schedule を**混在禁止**（週次は別系）

# 動作確認の超スリムテスト
- 入力 `{15m,30m}` でも通る（`15m←source`, `30m←15m_live`）。  
- 生成SQLで `FROM bar_5m_live`／`FROM bar_15m_live` が期待どおりになっている。  
- 実データを `5m_live` だけに流しても、**15m/30m が連鎖で埋まる**。

要するに、「設計の前提」を直せばロールアップは**十分に成立**します。PR用の最小差分（Planner差し替え＋テスト3本）までまとめようか？

送信日時（JST）: [[REDACTED_PHONE]]:20

### system — 2025-09-08T09:13:20.654832+00:00

_(empty)_

### system — 2025-09-08T09:13:20.658809+00:00

_(empty)_

### assistant — 2025-09-08T09:13:21.801686+00:00

_(empty)_

### assistant — 2025-09-08T09:13:27.877533+00:00

_(empty)_

### user — 2025-09-08T09:14:10.924000+00:00

_1m_liveを親固定だと問題になる？

### system — 2025-09-08T09:14:13.232842+00:00

_(empty)_

### system — 2025-09-08T09:14:13.240001+00:00

_(empty)_

### assistant — 2025-09-08T09:14:14.148497+00:00

_(empty)_

### assistant — 2025-09-08T09:14:21.742008+00:00

なるなる。**`*_1m_live` 親“固定”は実運用で痛点だらけ**です。要点だけ並べます。

- **多段の旨味が消える**：常に 1m から再集約すると、5m→15m→30m の段階処理にならず、毎段が同じ 1m を読み直す＝重複計算＆重複I/O。  
- **レイテンシ増大**：上位足（15m/30m）が1mの完成待ち＋広い再集約を毎回実施。直下（5m→15m）参照なら中間結果を再利用できて早い。  
- **負荷・コスト悪化**：Kafka/ksqlDB の state store/再分散トピックが各段で肥大。1m起点の**扇形ファンアウト**はネットワーク＆ストレージ負担が跳ねる。  
- **バックフィルに弱い**：後から 15m を増やすと、過去ぶんを全部 1m から再計算。直下チェーンなら 5m の再利用で軽い。  
- **障害局所化できない**：1m が遅延／停止した瞬間、上位の全足が影響。直下チェーンなら障害が**一段に局在**しやすい。  
- **ウィンドウ整合の罠**：親が1m固定だと、`WINDOW TUMBLING` の境界・`GRACE/EMIT` を段ごとに最適化しづらい。直下チェーンなら段ごとに運用ポリシーを変えやすい。  
- **DAG柔軟性が死ぬ**：{15m,30m} だけ欲しいケースや minutes/hours を分けたいケースで“親不在”になりがち（今回の本件）。

---

## 例外的に“親=1m固定”でも許容できる場面
- 取り扱い粒度が**少数（1～2段）**で**流量が小さい** PoC/一時運用。  
- **厳密な同一ロジック保証**（「全部 1m 定義に揃えたい」）が最優先で、リソース増を許容するとき。

---

## 維持するなら最低限の“延命”策
- 1m を **専用トピック＋専用クラスター/タスク**で隔離（扇形負荷の吸収）。  
- 上位段ごとに **Repartition/STATESTORE サイズ**と **GRACE/EMIT** を最適化。  
- バックフィル用に **コールドパス（オフライン再集約）**を別経路で用意。  
…とはいえ、根治ではなく“コストで殴る”方向です。

---

## 推奨：**直下倍数チェーン**に移行
- 親選択＝「同一系（minutes/hours/days）内で**最大の直下倍数**」。  
  例：`15m ← 5m_live`, `30m ← 15m_live`。  
- 週次は **Schedule 系**として minutes/hours/days と分離（混在禁止）。  
- テスト最小セット：  
  - `{15m,30m}` でも動く（`30m FROM bar_15m_live` をアサート）。  
  - 生成SQLの `FROM` と `WINDOW SIZE` が段ごとに正しいこと。  
  - 5m だけに投入しても 15m/30m が連鎖で埋まること。

---

## まとめ
- **はい、`*_1m_live` 親固定は問題になります。**  
- 根治は **Planner を「直下粒度チェーン」に変更**すること。  
- 当面の延命は可能だが、負荷・遅延・運用コストの面でスケールしません。

必要なら、この方針で **Planner差分＋結合テスト3本**の雛形をそのまま出します。呼んで👍

送信日時（JST）: [[REDACTED_PHONE]]:14

### assistant — 2025-09-08T09:14:22.688706+00:00

_(empty)_

### user — 2025-09-08T09:16:01.843000+00:00

倍数で使用しない場合はコストは上がるけどできなくはない？

### assistant — 2025-09-08T09:16:02.001206+00:00

結論：**「できなくはないが、“多段ロールアップ”としては非推奨」**です。倍数関係がないと、正確な再集約を“下位ライブの再利用”だけで保証できません。やるなら次のいずれかの割り切りが必要です。

## 現実的な選択肢
1) **各タイムフレームをソースから独立集約（扇形ファンアウト）**  
   - 例：7m と 15m をどちらも `deduprates` から直接 TUMBLING。  
   - ✅ 正確（窓境界がズレない）／❌ コスト・レイテンシ増、再分散＆state storeが倍増。

2) **“多段”は倍数チェーンのみ、非倍数はチェーンから除外**  
   - 例：{5m,15m,30m,7m} の場合、`5←15←30` はチェーン、`7m` はソース直集約。  
   - ✅ 直下の再利用による効率と正確性の両立／❌ 実装が少し複雑（フレームごとに親の種類が混在）。

3) **準ミクロ基準（例：1m）で“再ビン詰め”を許容する**  
   - 1m を**明示的に“基準バケット”**として作り、7m/13m などは 1m の合成で作る。  
   - ✅ 任意長対応が簡単／❌ 端の境界ズレによる統計の偏りや、I/O重複（以前の課題が再発）。  
   - ※ Max/Min など可換・結合的な指標は再合成できても、**OHLC** は窓境界差で**厳密一致しません**。

4) **HOPPING/SLIDING 窓で“設計自体を変える”**  
   - 非倍数を滑らかに扱えるが、もはや“ロールアップの階層”というより**別アルゴリズム**。  
   - ✅ 柔軟／❌ 要件が変わる（TUMBLING 前提の設計・検証が使えない）。

## 技術的な注意点
- **OHLC の厳密性**：非倍数チェーンで下位ライブから再集約すると、窓端の組み合わせがズレて**完全一致しません**（特に Open/Close）。  
- **集約の再合成性**：Sum/Count/Min/Max は再合成しやすいが、平均や分位、ユニーク数は追加の十分統計（Sum, Count, HLL など）が要る。  
- **運用コスト**：ソース直集約（扇形）は Repartition/Changelog/State がタイムフレーム数に比例的に増大。  
- **テスト**：非倍数は「一致保証」ではなく**許容差分**（境界差）を前提にした検証に切替が必要。

## おすすめ方針（実装ガイド）
- **Planner**  
  - 倍数チェーンのみ「直下ライブ親」を許容。  
  - 非倍数フレームは **parent=source** に自動切替（チェーンに載せない）。  
  - 同一系（minutes/hours/days/schedule）を厳密に分離。  
- **ポリシー**  
  - 既定：倍数以外は扇形（正確性優先）。  
  - オプトイン：`AllowMicroBaseRebinning=true` のときのみ 1m 合成を許可（※精度注意の警告をログに出す）。  
- **メトリクス**  
  - 目的が「概形」なら 1m 合成容認、**厳密検証や価格系 OHLC**は**扇形直集約**。

> まとめ：**「コストは上がるが“できる”」のは、ソース直集約（扇形）か、1m 合成を明示容認した場合のみ。**  
> 多段ロールアップとしての“再利用効率＋厳密性”を両立したいなら、**倍数チェーン**を基本にし、非倍数はチェーン外に逃がすのが堅実です。

送信日時（JST）: [[REDACTED_PHONE]]:31

### system — 2025-09-08T09:16:02.539358+00:00

_(empty)_

### system — 2025-09-08T09:16:02.544775+00:00

_(empty)_

### assistant — 2025-09-08T09:16:04.088798+00:00

_(empty)_

### assistant — 2025-09-08T09:16:23.983438+00:00

_(empty)_
